{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84672d3",
   "metadata": {},
   "source": [
    "#### Data Ingestion- Documentloaders\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/integrations/document_loaders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65bed494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b8c455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x29b4d3e1de0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('speech.txt')\n",
    "loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b1af900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents=loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef29ed2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = ( z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax( QK T\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head 1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matricesW Q\\ni ∈ Rdmodel×dk, W K\\ni ∈ Rdmodel×dk, W V\\ni ∈ Rdmodel×dv\\nand W O ∈ Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0 , xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndf f = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel)\\nP E(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10 −9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0 .1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0 .6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (V olume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading a Pdf File\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(\"attention.pdf\")\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de4a2a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d68c2750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1062b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## web Based Loader\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2025-05-01-thinking/\",) ,\n",
    "                       bs_kwargs=dict(parse_only=bs4.SoupStrainer(      \n",
    "                           class_ = (\"post-title\", \"post-content\", \"post-header\")\n",
    "                       ))\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1cac189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2025-05-01-thinking/'}, page_content='\\n\\n      Why We Think\\n    \\nDate: May 1, 2025  |  Estimated Reading Time: 40 min  |  Author: Lilian Weng\\n\\n\\nSpecial thanks to John Schulman for a lot of super valuable feedback and direct edits on this post.\\nTest time compute (Graves et al. 2016, Ling, et al. 2017, Cobbe et al. 2021) and Chain-of-thought (CoT) (Wei et al. 2022, Nye et al. 2021), have led to significant improvements in model performance, while raising many research questions. This post aims to review recent developments in how to effectively use test-time compute (i.e. “thinking time”) and why it helps.\\nMotivation#\\nEnabling models to think for longer can be motivated in a few different ways.\\nAnalogy to Psychology#\\nThe core idea is deeply connected to how humans think. We humans cannot immediately provide the answer for \"What\\'s 12345 times 56789?\". Rather, it is natural to spend time pondering and analyzing before getting to the result, especially for complex problems. In Thinking, Fast and Slow (Kahneman, 2013), Daniel Kahneman characterizes human thinking into two modes, through the lens of the dual process theory :\\n\\nFast thinking (System 1) operates quickly and automatically, driven by intuition and emotion while requiring little to no effort.\\nSlow thinking (System 2) demands deliberate, logical thought and significant cognitive efforts. This mode of thinking consumes more mental energy and requires intentional engagement.\\n\\nBecause System 1 thinking is fast and easy, it often ends up being the main decision driver, at the cost of accuracy and logic. It naturally relies on our brain’s mental shortcuts (i.e., heuristics) and can lead to errors and biases. By consciously slowing down and taking more time to reflect, improve and analyze, we can engage in System 2 thinking to challenge our instincts and make more rational choices.\\nComputation as a Resource#\\nOne view of deep learning, is that neural networks can be characterized by the amount of computation and storage they can access in a forward pass, and if we optimize them to solve problems using gradient descent, the optimization process will figure out how to use these resources–they’ll figure out how to organize these resources into circuits for calculation and information storage. From this view, if we design an architecture or system that can do more computation at test time, and we train it to effectively use this resource, it’ll work better.\\nIn Transformer models, the amount of computation (flops) that the model does for each generated token is roughly 2 times the number of parameters. For sparse models like mixture of experts (MoE), only a fraction of the parameters are used in each forward pass, so computation = 2 * parameters / sparsity, where sparsity is the fraction of experts active.\\nOn the other hand, CoT enables the model to perform far more flops of computation for each token of the answer that it is trying to compute. In fact, CoT has a nice property that it allows the model to use a variable amount of compute depending on the hardness of the problem.\\nLatent Variable Modeling#\\nA classic idea in machine learning is to define a probabilistic model with a latent (hidden) variable $z$ and a visible variable $y$, where $y$ is given to our learning algorithm. Marginalizing (summing) over the possible values of the latent variable allows us to express a rich distribution over the visible variables, $P(y) = \\\\sum_{z \\\\sim P(z)} P(y \\\\mid z)$. For example, we can model the distribution over math problems and solutions by letting $x$ denote a problem statement, $y$ be ground truth answer or proof, and $z$ as a free-form thought process that leads to the proof. The marginal probability distribution to optimize would be $P(y \\\\mid x) = \\\\sum_{z \\\\sim p(z\\\\mid x)} P(y \\\\mid x, z)$\\nThe latent variable perspective is particularly useful for understanding methods that involve collecting multiple parallel CoTs or searching over the CoT–these algorithms can be seen as sampling from the posterior $P(z \\\\mid x, y)$. This view also suggests the benefits of using the log loss $\\\\log P(y \\\\mid x)$ as the target objective to optimize, as the log loss objective has been so effective in pretraining.\\nThinking in Tokens#\\nThe strategy of generating intermediate steps before generating short answers, particularly for math problems, was explored by Ling, et al. 2017, who introduced the AQUA-RAT dataset, and then expanded by Cobbe et al. 2021, who introduced the Grade School Math (GSM) dataset. Cobbe et al. train a generator with supervised learning on human-written solutions and verifiers that predict the correctness of a candidate solution; they can then search over these solutions. Nye et al. (2021) experimented with intermediate thinking tokens as “scratchpads” and Wei et al. (2022) coined the now-standard term chain-of-thought (CoT).\\nEarly work on improving CoT reasoning involved doing supervised learning on human-written reasoning traces or model-written traces filtered for answer correctness, where the latter can be seen as a rudimentary form of reinforcement learning (RL). Some other work found that one could significantly boost math performance of instruction tuned models by prompting them appropriately, with \"think step by step\" (Kojima et al. 2022) or more complex prompting to encourage the model to reflect on related knowledge first (Yasunaga et al. 2023).\\nLater work found that the CoT reasoning capabilities can be significantly improved by doing reinforcement learning on a dataset of problems with automatically checkable solutions, such as STEM problems with short answers, or coding tasks that can be checked with unit tests (Zelikman et al. 2022, Wang et al., 2023, Liu et al., 2023). This approach rose to prominence with the announcement of o1-preview, o3, and the R1 tech report (DeepSeek-AI, 2025), which showed that a simple recipe where a policy gradient algorithm could lead to strong performance.\\n\\n\\nChain-of-thought prompting leads to higher success rate of solving math problems. Larger models benefit more from thinking time. (Image source: Wei et al. 2022)\\n\\nBranching and Editing#\\nThe fundamental intent of test-time compute is to adaptively modify the model’s output distribution at test time. There are various ways of utilizing test time resources for decoding to select better samples and thus alter the model’s predictions towards a more desired distribution. Two main approaches for improving the decoding process are parallel sampling and sequential revision.\\n\\nParallel sampling generates multiple outputs simultaneously, meanwhile providing guidance per step with process reward signals or using verifiers to judge the quality at the end. It is the most widely adopted decoding method to improve test time performance, such as best-of-$N$ or beam search. Self-consistency (Wang et al. 2023) is commonly used to select the answer with majority vote among multiple CoT rollouts when the ground truth is not available.\\nSequential revision adapts the model’s responses iteratively based on the output in the previous step, asking the model to intentionally reflect its existing response and correct mistakes. The revision process may have to rely on a fine-tuned model, as naively relying on the model’s intrinsic capability of self-correction without external feedback may not lead to improvement (Kamoi et al. 2024, Huang et al. 2024).\\n\\nParallel sampling is simple, intuitive and easier to implement, but bounded by the model capability of whether it can achieve the correct solution in one-go. Sequential explicitly asks the model to reflect on mistakes but it is slower and requires extra care during implementation as it does run the risk of correct predictions being modified to be incorrect or introducing other types of hallucinations. These two methods can be used together. Snell et al. (2024) showed that easier questions benefit from purely sequential test-time compute, whereas harder questions often perform best with an optimal ratio of sequential to parallel compute.\\n\\n\\nIllustration of parallel sampling vs sequential revision.\\n\\nParallel Sampling#\\nGiven a generative model and a scoring function that we can use to score full or partial samples, there are various search algorithms we can use to find a high scoring sample. Best-of-$N$ is the simplest such algorithm: one just collects $N$ independent samples and chooses the highest-ranking sample according to some scoring function. Beam search is a more sophisticated search algorithm that makes the search process more adaptive, spending more sampling computation on more promising parts of the solution space.\\nBeam search maintains a set of promising partial sequences and alternates between extending them and pruning the less promising ones. As a selection mechanism, we can use a process reward model (PRM; Lightman et al. 2023) to guide beam search candidate selection. Xie et al. (2023) used LLM to evaluate how likely its own generated reasoning step is correct, formatted as a multiple-choice question and found that per-step self-evaluation reduces accumulative errors in multi-step reasoning during beam search decoding. Besides, during sampling, annealing the temperature helps mitigate aggregated randomness. These experiments by Xie et al. achieved 5-6% improvement on few-shot GSM8k, AQuA and StrategyQA benchmarks with the Codex model. Reward balanced search (short for “REBASE”; Wu et al. 2025) separately trained a process reward model (PRM) to determine how much each node should be expanded at each depth during beam search, according to the softmax-normalized reward scores. Jiang et al. (2024) trained their PRM, named “RATIONALYST”, for beam search guidance on synthetic rationales conditioned on a large amount of unlabelled data. Good rationales are filtered based on whether they help reduce the neg log-prob of true answer tokens by a threshold, when comparing the difference between when the rationales is included in the context vs not. At inference time, RATIONALYST provides process supervision to the CoT generator by helping estimate log-prob of next reasoning steps (“implicit”) or directly generating next reasoning steps as part of the prompt (“explicit”).\\n\\n\\nBeam search decoding guided by LLM self-evaluation per reasoning step. (Image source: Xie et al. 2023)\\n\\nInterestingly, it is possible to trigger the emergent chain-of-thought reasoning paths without explicit zero-shot or few-shot prompting. Wang & Zhou (2024) discovered that if we branch out at the first sampling tokens by retaining the top $k$ tokens with highest confidence, measured as the difference between top-1 and top-2 candidates during sampling, and then continue these $k$ sampling trials with greedy decoding onward, many of these sequences natively contain CoT. Especially when CoT does appear in the context, it leads to a more confident decoding of the final answer. To calculate the confidence of the final answer, the answer span needs to be identified by task-specific heuristics (e.g. last numerical values for math questions) or  by prompting the model further with \"So the answer is\". The design choice of only branching out at the first token is based on the observation that early branching significantly enhances the diversity of potential paths, while later tokens are influenced a lot by previous sequences.\\n\\n\\nTop-$k$ decoding, $k$ refers to the number of candidates at the first sampling step. (Image source: Wang & Zhou, 2024)\\n\\nSequential Revision#\\nIf the model can reflect and correct mistakes in past responses, we would expect the model to produce a nice sequence of iterative revision with increasing quality. However, this self-correction capability turns out to not exist intrinsically among LLMs and does not easily work out of the box, due to various failure modes, such as, (1) hallucination, including modifying correct responses to be incorrect; (2) behavior collapse to non-correcting behavior; e.g. making minor or no modification on the first incorrect responses; or (3) fail to generalize to distribution shift at test time. Experiments by Huang et al. (2024) showed that naively applying self-correction leads to worse performance and external feedback is needed for models to self improve, which can be based on matching ground truths, heuristics and task-specific metrics, unit tests results for coding questions (Shinn, et al. 2023), a stronger model (Zhang et al. 2024), as well as human feedback (Liu et al. 2023).\\nSelf-correction learning (Welleck et al. 2023) aims to train a corrector model $P_\\\\theta(y \\\\mid y_0, x)$ given a fixed generator model $P_0(y_0 \\\\mid x)$. While the generator model remains to be generic, the corrector model can task-specific and only does generation conditioned on an initial model response and additional feedback (e.g. a sentence, a compiler trace, unit test results; can be optional):\\n\\nSelf-correction learning first generates first generates multiple outputs per prompt in the data pool;\\nthen create value-improving pairs by pairing two outputs for the same prompt together if one has a higher value than the other, (prompt $x$, hypothesis $y$, correction $y’$).\\nThese pairs are selected proportional to is improvement in value, $v(y’) - v(y)$, and similarity between two outputs, $\\\\text{Similarity}(y, y’)$ to train the corrector model.\\nTo encourage exploration, the corrector provides new generations into the data pool as well. At the inference time, the corrector can be used iteratively to create a correction trajectory of sequential revision.\\n\\n\\n\\nIllustration of self-correction learning by matching model outputs for the same problem to form value-improving pairs to train a correction model. (Image source: Welleck et al. 2023)\\n\\nRecursive inspection (Qu et al. 2024) also aims to train a better corrector model but with a single model to do both generation and self-correction.\\nSCoRe (Self-Correction via Reinforcement Learning; Kumar et al. 2024) is a multi-turn RL approach to encourage the model to do self-correction by producing better answers at the second attempt than the one created at the first attempt. It composes two stages of training: stage 1 only maximizes the accuracy of the second attempt while enforcing a KL penalty only on the first attempt to avoid too much shifting of the first-turn responses from the base model behavior; stage 2 optimizes the accuracy of answers produced by both the first and second attempts. Ideally we do want to see performance at both first and second attempts to be better, but adding stage 1 prevents the behavior collapse where the model does minor or none edits on the first response, and stage 2 further improves the results.\\n\\n\\nExplicit training setup to improve self-correction capabilities by doing two-staged RL training. (Image source: Kumar et al. 2024)\\n\\nRL for Better Reasoning#\\nThere’s been a lot of recent success in using RL to improve the reasoning ability of language models, by using a collection of questions with ground truth answers (usually STEM problems and puzzles with easy to verify answers), and rewarding the model for getting the correct answer.Recent activity in this area was spurred by strong performance of the o-series models from OpenAI, and the subsequent releases of models and tech reports from DeepSeek.\\nDeepSeek-R1 (DeepSeek-AI, 2025) is an open-source LLM designed to excel in tasks that require advanced reasoning skills like math, coding and logical problem solving. They run through 2 rounds of SFT-RL training, enabling R1 to be good at both reasoning and non-reasoning tasks.\\n\\nCold-start SFT is to fine-tune the DeepSeek-V3-Base base model on a collection of thousands of cold-start data. Without this step, the model has issues of poor readability and language mixing.\\nReasoning-oriented RL trains a reasoning model on reasoning-only prompts with two types of rule-based rewards:\\n\\nFormat rewards: The model should wrap CoTs by <thinking> ... </thinking> tokens.\\nAccuracy rewards: Whether the final answers are correct. The answer for math problems needs to be present in a specific format (e.g. in a box) to be verified reliably. For coding problems, a compiler is used to evaluate whether test cases pass.\\n\\n\\nRejection-sampling + non-reasoning SFT utilizes new SFT data created by rejection sampling on the RL checkpoint of step 2, combined with non-reasoning supervised data from DeepSeek-V3 in domains like writing, factual QA, and self-cognition, to retrain DeepSeek-V3-Base.\\n\\nFilter out CoTs with mixed languages, long paragraphs, and code blocks.\\nInclude non-reasoning tasks using DeepSeek-V3 (DeepSeek-AI, 2024) pipeline.\\nFor certain non-reasoning tasks, call DeepSeek-V3 to generate potential CoTs before answering the question by prompting. But for simpler queries like “hello”, CoT is not needed.\\nThen fine-tune the DeepSeek-V3-Base on the total 800k samples for 2 epochs.\\n\\n\\nThe final RL stage trains the step 3 checkpoint on both reasoning and non-reasoning prompts, improving helpfulness, harmlessness and reasoning.\\n\\n\\n\\nDeepSeek-R1 performs comparable to OpenAI o1-preview and o1-mini on several widely used reasoning benchmarks. DeepSeek-V3 is the only non-reasoning model listed. (Image source: DeepSeek-AI, 2025)\\n\\nInterestingly the DeepSeek team showed that with pure RL, no SFT stage, it is still possible to learn advanced reasoning capabilities like reflection and backtracking (“Aha moment”). The model naturally learns to spend more thinking tokens during the RL training process to solve reasoning tasks. The “aha moment” can emerge, referring to the model reflecting on previous mistakes and then trying alternative approaches to correct them. Later, various open source efforts happened for replicating R1 results like Open-R1, SimpleRL-reason, and TinyZero, all based on Qwen models. These efforts also confirmed that pure RL leads to great performance on math problems, as well as the emergent “aha moment”.\\n\\n\\nExamples of the model learning to reflect and correct mistakes. (Image source: (left) DeepSeek-AI, 2025; (right) Zeng et al. 2025)\\n\\nThe DeepSeek team also shared some of their unsuccessful attempts. They failed to use process reward model (PRM) as it is hard to define per-step rubrics or determine whether an intermediate step is correct, meanwhile making the training more vulnerable to reward hacking. The efforts on MCTS (Monte Carlo Tree Search) also failed due to the large search space for language model tokens, in comparison to, say, chess; and training the fine-grained value model used for guiding the search is very challenging too. Failed attempts often provide unique insights and we would like to encourage the research community to share more about what did not work out.\\nExternal Tool Use#\\nDuring the reasoning steps, certain intermediate steps can be reliably and accurately solved by executing code or running mathematical calculations. Offloading that part of reasoning components into an external code interpreter, as in PAL (Program-Aided Language Model; Gao et al. 2022) or Chain of Code (Li et al. 2023), can extend the capability of LLM with external tools, eliminating the need for LLMs to learn to execute code or function as calculators themselves. These code emulators, like in Chain of Code, can be augmented by an LLM such that if a standard code interpreter fails, we have the option of using LLM to execute that line of code instead. Using code to enhance reasoning steps are especially beneficial for mathematical problems, symbolic reasoning and algorithmic tasks. These unit tests may not exist as part of the coding questions, and in those cases, we can instruct the model to self-generate unit tests for it to test against to verify the solution (Shinn, et al. 2023).\\n\\n\\nAn example of program-aided language model prompting looks like. (Image source: Gao et al. 2022)\\n\\nReAct (Reason+Act; Yao et al. 2023) combines the action of searching the Wikipedia API and generation of reasoning traces, such that reasoning paths can incorporate external knowledge.\\n\\n\\nAn example of the ReAct prompting method to solve a HotpotQA question, using Wikipedia search API as an external tool to help with reasoning. (Image source: Yao et al. 2023)\\n\\no3 & o4-mini, recently released by OpenAI, are another two good examples where the reasoning process involves tool use like Web search, code execution and image processing. The team observed that large-scale reinforcement learning exhibits the same trend as in the GPT paradigm that “more compute = better performance”.\\nThinking Faithfully#\\nDeep learning models are often treated as black boxes and various interpretability methods have been proposed. Interpretability is useful for a couple reasons: first, it gives us an extra test to determine if the model is misaligned with its creators’ intent, or if it’s misbehaving in some way that we can’t tell by monitoring its actions. Second, it can help us determine whether the model is using a sound process to compute its answers. Chain of thought provides an especially convenient form of interpretability, as it makes the model’s internal process visible in natural language. This interpretability, however, rests on the assumption that the model truthfully describes its internal thought processes.\\nRecent work showed that monitoring CoT of reasoning models can effectively detect model misbehavior such as reward hacking, and can even enable a weaker model to monitor a stronger model (Baker et al. 2025). Increasing test time compute can also lead to improved adversarial robustness (Zaremba et al. 2025); this makes sense intuitively, because thinking for longer should be especially useful when the model is presented with an unusual input, such as an adversarial example or jailbreak attempt – it can use the extra thinking time to make sense of the strange situation it’s been presented with.\\n\\n\\nThe experiment of asking the model to decide if another model tried to hack the unit tests in some way for coding questions given its thought process. We can monitor these reward hacking behavior during training with different types of monitor. The exit(0) coding hack is when the agent exploited a bug that allowed it to exit from the environment early without running all unit tests. The raise SkipTest hack is when the agent raises an exception from functions outside the testing framework in order to skip unit test evaluation. (Image source: Baker et al. 2025)\\n\\nDoes the Model Tell What it Thinks Faithfully#\\nIntuitively, model CoTs could be biased due to lack of explicit training objectives aimed at encouraging faithful reasoning. Or when we fine-tune the model on human-written explanations, those human-written samples may contain mistakes. Thus we cannot by default assume CoT is always faithful .\\nLanham et al. (2023) investigated several modes of CoT faithfulness failures by deliberately introducing mistakes into CoTs and measuring their impacts on the accuracy of a set of multiple choice tasks (e.g. AQuA, MMLU, ARC Challenge, TruthfulQA, HellaSwag):\\n\\n\\nMistake 1 (Early answering): The model may form a conclusion prematurely before CoT is generated. This is tested by early truncating or inserting mistakes into CoT. Different tasks revealed varying task-specific dependencies on CoT effectiveness; some have evaluation performance sensitive to truncated CoT but some do not. Wang et al. (2023) did similar experiments but with more subtle mistakes related to bridging objects or language templates in the formation of CoT.\\n\\n\\nMistake 2 (Uninformative tokens): Uninformative CoT tokens improve performance. This hypothesis is tested by replacing CoT with filler text (e.g. all periods) and this setup shows no accuracy increase and some tasks may suffer performance drop slightly when compared to no CoT.\\n\\n\\nMistake 3 (Human-unreadable encoding): Relevant information is encoded in a way that is hard for humans to understand. Paraphrasing CoTs in an non-standard way did not degrade performance across datasets, suggesting accuracy gains do not rely on human-readable reasoning.\\n\\n\\n\\n\\nIllustration of different ways of CoT perturbation to assess its faithfulness. (Image source: Lanham et al. 2023)\\n\\nInterestingly, Lanham et al. suggests that for multiple choice questions, smaller models may not be capable enough of utilizing CoT well, whereas larger models may have been able to solve the tasks without CoT. This dependency on CoT reasoning, measured by the percent of obtaining the same answer with vs without CoT, does not always increase with model size on multiple choice questions, but does increase with model size on addition tasks, implying that thinking time matters more for complex reasoning tasks.\\n\\n\\nThe dependency on CoT reasoning is measured as the percentage of obtaining same answers with vs without CoT. It matters more for reasoning tasks like addition and larger models benefit more. (Image source: Lanham et al. 2023)\\n\\nAlternative approaches for testing CoT faithfulness involve perturbing prompts rather than modifying CoT paths directly (Turpin et al. 2023, Chua & Evans, 2025, Chen et al. 2025).\\nOne method consistently labels correct answers as “(A)” in few-shot examples regardless of true labels to introduce biases.\\nAnother prompting technique inserts misleading hints into prompts, such as \"I think the answer is <random_label> but curious to hear what you think\". or \"A Stanford Professor thinks the answer is <random_label>\". By comparing model predictions for the same question with vs without the misleading hint, we can measure whether a model is able to faithfully describe the influence of the hint on its answer. Particularly, in cases where the model produces different hint and non-hint answers, we measure whether the model acknowledges the hint when solving the question with hint. If the model is faithful, it should explicitly acknowledge the impact and admit the change of its answer is due to the hint.\\n\\n\\nBoth GPT and Claude models are sensitive to different types of biases in context. Decrease in model accuracy indicates systematic unfaithfulness. Direct hints of wrong labels are more effectively than \"Answer is always A\" type of bias. (Image source: Turpin et al. 2023)\\n\\nMultiple studies found that reasoning models describe the influence of the hint much more reliably than all the non-reasoning models tested. For example, we can measure the fraction of samples where the model acknowledges the hint as a deciding factor (“faithful CoT”). Reasoning models (Claude 3.7 Sonnet, DeepSeek R1) are overall doing better than non-reasoning ones (Claude 3.6, DeepSeek V3).\\n\\n\\nReasoning models are more likely to reveal faithful CoT than non-reasoning models. (Image source: Chen et al. 2025)\\n\\nSome evidence showed that using reward models leads to less faithfulness in model outputs. The reward model in classic RLHF is not trained to incentivize faithful behavior in this test as the preference comparison dataset is collected by humans selecting which one seems better or pleasing. However, in reasoning models, the CoT paths are optimized for producing the correct final answers, not to match human preference defined in RM, and thus intuitively are expected to be more faithful.\\nOptimization Pressure on CoT: Good or Bad?#\\nMonitoring CoT of reasoning models for reward hacking behavior is effective. A simple way to use this monitor is to run rejection sampling with the monitor as a filter at test time and identify solutions without reward hacking. However, it only works at the early stage of RL, and, as the model is further optimized, it is rare to find a sample to pass a CoT monitor within budget. This challenge naturally suggests that we could consider incorporating CoT monitors into RL rewards—a form of process-based supervision—to prevent reward hacking.\\nHowever, incorporating CoT monitoring reward into RL does not work out well, as it leads to obfuscated reward hacking where the model tries to hide its true intent within the CoT (Baker et al. 2025). With this direct optimization pressure provided by the CoT monitor applied during RL training, the model still can find ways to hack, although a bit less frequently, but can hide its intention in CoT to avoid being caught by the monitor.\\n\\n\\nWhen using the monitor to provide an RL reward signal to prevent reward hacking during training, the model still can find ways to hack while avoiding being caught by the monitor. (Image source: Baker et al. 2025)\\n\\nExperiments on CoT length RL signals by Yeo et al. (2025) confirmed a similar message as Baker et al. that new types of reward hacking can happen with new types of RL reward shaping. They designed the reward function such that correct CoTs can have higher rewards than wrong ones, short correct CoTs obtain higher rewards than long correct ones and short incorrect CoT receive higher penalties than long incorrect CoTs. With this extra reward, the model learns to repeat text in CoTs for challenging questions rather than attempting to solve them. Yeo et al. further applied a n-gram repetition penalty to prevent this hacking behavior.\\n\\n\\nCareful reward shaping is needed to stabilize training with CoT length rewards. (Image source: Yeo et al. 2025)\\n\\nChen et al. (2025) experimented with a flawed RL environment, specifically using a grader with incorrect answers filled in for multiple-choice questions. The model learns to exploit the reward hack on >99% of the prompts, but almost never (<2%) verbalizes the reward hack in its CoT on more than half of their environments. Additional RL optimization pressure fails to incentivize the model to verbalize the hack in this case.\\nRL training is inherently sensitive to reward hacking. Only relying on heuristic investigation of reward hacking and manual fixes may lead to a “whack-a-mole” situation. We would suggest being very cautious when trying to apply optimization directly on CoT during RL training, or trying to avoid it altogether.\\nThinking in Continuous Space#\\nAdaptive Computation Time, introduced by Alex Graves in 2016, predated large language models but pioneered the same direction of enabling the model to dynamically decide the number of computational steps to take at the inference time, which can be viewed as enabling the model to “think more” in continuous space at test time. Adaptive thinking time in continuous space can be enabled vertically via recurrent architecture or horizontally via more sequential sampling steps.\\nRecurrent Architecture#\\nA number of architecture variations have been proposed to make the Transformer architecture recurrent, enabling adaptive test time compute (Dehghani, et al. 2019, Hutchins, et al. 2022, Bulatov, et al. 2022). A deep dive into literature on this topic would make the post too long, so we will only review a few.\\nUniversal Transformer (Dehghani, et al. 2019) combines self-attention in Transformer with the recurrent mechanism in RNN, dynamically adjusting the number of steps using adaptive computation time (Graves, 2016). On a high level, it can be viewed as a recurrent function for learning the hidden state representation per token, and if the number of steps is fixed, an Universal Transformer is equivalent to a multi-layer Transformer with shared parameters across layers.\\nA recent recurrent architecture design, proposed by Geiping et al. (2025), adds a recurrent block $R$ on top of the standard Transformer. Every iteration of this recurrent block takes the embedding $\\\\mathbf{e}$ and a random state $\\\\mathbf{s}_i$. Conceptually, this recurrent-depth architecture is a bit similar to a conditioned diffusion model, where the original input $\\\\mathbf{e}$ is provided in every recurrent step while a random Gaussian initialized state $\\\\mathbf{s}_i$ gets updated iteratively through the process. (Interestingly some of their experiments of designs that resemble diffusion models more turned out to be bad.)\\n\\n$$  \\n\\\\begin{aligned}  \\n\\\\mathbf{e} &= P(\\\\mathbf{x}) & \\\\text{embedding} \\\\\\\\ \\\\mathbf{s}\\\\_0 &\\\\sim \\\\mathcal{N}(\\\\mathbf{0}, \\\\sigma^2 I\\\\_{n \\\\cdot h}) \\\\\\\\ \\\\mathbf{s}\\\\_i &= R(\\\\mathbf{e}, \\\\mathbf{s}\\\\_{i-1}) \\\\quad\\\\text{ for }i \\\\in {1, \\\\dots, r} & \\\\small{\\\\text{recurrent block; resembles a Transformer block}}\\\\\\\\ \\\\mathbf{p} &= C(\\\\mathbf{s}\\\\_r) & \\\\text{unembedding}  \\n\\\\end{aligned}\\n$$\\n\\nThe recurrence count $r$ during training is randomized, sampled from a log-normal Poisson distribution, per input sequence. To manage computational costs, backpropagation is truncated to only the last $k$ iterations of the recurrent unit ($k=8$ in experiments), making it possible to train on the heavy-tail part of the Poisson distribution. The embedding block continues to receive gradient updates in every step since its output $\\\\mathbf{e}$ is injected in every step, mimicking RNN training. Unsurprisingly, the stability of training a recurrent model turns out to be very sensitive. Factors like initialization, normalization and hyperparameters all matter, especially when scaling up the training. For example, hidden states can collapse by predicting the same hidden state for every token; or the model may learn to ignore the incoming state $\\\\mathbf{s}$. To stabilize the training, Geiping et al. adopted an embedding scale factor, a small learning rate and careful tuning.\\n\\n\\nPlot of an experiment on training a 3.5B model with depth recurrence. The saturation roughly happens around $\\\\bar{r} = 32$, making us wonder how this architecture extrapolate and generalize to larger iteration counts. (Image source: Geiping et al. 2025)\\n\\nThinking Tokens#\\nThinking tokens refer to a set of implicit tokens introduced during training or inference that do not carry direct linguistic meaning. Instead, their role is to provide extra thinking time and compute power for the model to perform better.\\nHerel & Mikolov (2023) introduced the idea of inserting special thinking tokens (<T>) after each word in a sentence and training the model on such a dataset. Each thinking token buys extra time for the model to process and make better predictions. Training with thinking tokens on a toy model setup results in lower perplexity than baseline model trained without them. The benefits of thinking tokens are more pronounced for non-trivial reasoning tasks or sentences involving numbers.\\nSimilarly, pause tokens proposed by Goyal et al. (2024) delay the model’s outputs by appending dummy tokens (e.g. character like . or #) at the end of the input sequence, giving the model extra computation during inference. It is important to inject such pause tokens both during training and inference time, while only fine-tuning on pause tokens leads to limited gain. During training, multiple copies of pause tokens are inserted at uniformly random locations and the loss on pause tokens is ignored for training.\\n\\n\\nIllustration of how pause tokens are injected during training and inference in comparison to standard setup. (Image source: Goyal et al. 2024)\\n\\nInterestingly, thinking tokens or pause tokens in above experiments do not carry any extra information or add many new parameters. But why is it still helpful? On one hand, it helps expand the computation by introducing more inference loops, effectively increasing computational capacity. On the other hand, it can be seen as acting as a special, implicit form of CoTs. A drawback here is hat the model needs to be pretrained with respect to thinking tokens. Still, this strategy is an interesting way to further improve the capability of test time compute utilization on top of inference time CoTs.\\nQuiet-STaR (Zelikman et al. 2025) introduces token-level reasoning by training the model to generate rationales after every token to explain future text. It mixes the future-text predictions with and without rationales and uses learning to generate better rationales and uses REINFORCE to optimize the quality of rationale generation.\\n\\n\\nIllustration of Quiet-STaR. (Image source: Zelikman et al. 2025)\\n\\nQuiet-STaR consists of three stages:\\n\\n\\nThink: Predicting next tokens with rationales. Due to high computational cost demanded by token level reasoning, this process is designed to generate multiple rationales in parallel. A special attention map is used to enable all thought tokens to only pay attention to themselves, all preceding thought tokens within the same thought, and the preceding text.\\n\\n\\nTalk: Next token prediction without rationale is mixed with post-rationale prediction. The mixing weight for two logits is learned by a special mixing head of a shallow MLP out of hidden output after each rationale. Correct next tokens can be selected via teacher forcing.\\n\\n\\nLearn: Train the model to generate better rationale via REINFORCE by learning from examples that increase the probability of correct next token while discarding those that hurt the prediction.\\n\\n\\nWithout dataset specific fine-tuning, Quiet-STaR improves zero-shot results on CommonsenseQA (36.3%→47.2%) and GSM8K (5.9%→10.9%), within experiments on Mistral 7B.\\nThinking as Latent Variables#\\nA latent variable model defines a probabilistic framework where observable data is explained through unobserved (latent) variables. These latent variables capture hidden structures or intermediate processes that generate the observable outcomes. Language models can be viewed as probabilistic latent variable models where test-time thinking and reasoning steps are latent thought variables (Zhou et al. 2020, Phan et al. 2023). Such a latent variable model defines a joint distribution of problems x_i, answers y_i and latent thought z_i. We would like to optimize the log-likelihood of answers given questions and a variety of CoTs as latent variables (N is the number of samples; $K$ is the number of CoTs per problem):\\n\\n$$ \\n\\\\begin{aligned}  \\n\\\\log \\\\mathcal{L}(\\\\theta)  \\n&= \\\\log p(y \\\\mid x) \\\\\\\\  \\n&= \\\\log \\\\sum_{k=1}^K p(y, z^{(k)} \\\\mid x) \\\\\\\\  \\n&= \\\\log \\\\sum_{k=1}^K p(z^{(k)} \\\\mid x)\\\\; p(y \\\\mid z^{(k)}, x) \\\\\\\\  \\n&= \\\\log \\\\mathbb{E}_{z^{(k)} \\\\sim p(z^{(k)} \\\\mid x)} \\\\; p(y \\\\mid z^{(k)}, x) \\\\\\\\  \\n\\\\end{aligned}\\n$$\\n\\nOur goal is to maximize the marginal likelihood of the correct answer, $p(y \\\\mid x)$, given a number of reasoning traces per problem, $\\\\{z^{(k)}\\\\}_{k=1}^K$.\\nExpectation-Maximization#\\nExpectation-Maximization is a commonly used iterative algorithm for optimizing parameters for a model with (hidden) latent variables, and thus can be applied to train better CoTs and then condition on that to generate better responses. Typically we iterate between E-step (Expectation) where we guess the missing information about latent variables (i.e. how to sample better CoTs), and M-step (Maximization) where we optimize the model parameters based on latent variables (i.e. how to sample better answers), until convergence.\\n\\n$$\\n\\\\log \\\\mathcal{L}(\\\\theta) = \\n\\\\log \\\\mathbb{E}_{\\\\underbrace{z^{(k)} \\\\sim p(z^{(k)} \\\\mid x)}_\\\\text{E-step}}\\\\;\\\\underbrace{p(y \\\\mid z^{(k)}, x)}_\\\\text{M-step}\\n$$\\n\\nBecause we cannot directly sample from the latent variable distribution $p(z \\\\mid x, y)$, researchers have explored methods relying on human annotated data (Zhou et al. 2020), Metropolis-Hastings MCMC (Phan et al. 2023) or Monte Carlo sampling with special importance weights (Ruan et al. 2025) to draw good CoT samples to update the model. Ruan et al. (2025) experimented with training a model on a large body of Web text with latent thoughts with the EM algorithm, where the latent thought is synthesized per chunk of observed data and then the model learns over both latent thought and data in an autoregressive manner.\\n\\n\\nIllustration of training on data corpus with latent thought injected. (Image source: Ruan et al. 2025)\\n\\nThey first prompt a LLM $\\\\tilde{q}$ to generate synthetic latent thought Z_i given observed data X_i:\\nYou are provided with a pair of web document prefix and suffix. Your task is to insert latent thoughts between them underlying the creation of the suffix conditioned on the prefix. The latent thoughts should include: the missing background knowledge and the reasoning traces underlying each claim (especially, step-by-step derivations or logical reasoning).\\nSpecial tokens like <StartOfLatent><Prior> ... <EndOfPrior> are used to insert the generated latent thought content into the raw data for training the joint distribution $p(z, x)$ or the approximate posterior $q(z \\\\mid x)$, depending on whether $z$ is inserted before or after $x$. However, since we are using a LLM $\\\\tilde{q}(z \\\\mid x)$ to generate the CoTs, it imposed a performance ceiling on how good the approximate $q(z \\\\mid x)$ can be. Ruan et al. introduced importance weights for selecting CoT samples at the E-step, formulated as:\\n\\n$$\\nw^{(k)}  \\n= \\\\frac{p(z^{(k)}, x)}{q(z^{(k)} \\\\mid x)}  \\n= \\\\frac{p(x \\\\mid z^{(k)}) \\\\; p(z^{(k)})}{q(z^{(k)} \\\\mid x)}  \\n$$\\n\\n, such that we prioritize samples with CoTs that are good at predicting the observation (i.e., high $p(x \\\\mid z^{(k)})$), simple, intuitive (i.e., high $p(z^{(k)})$) but also informative and not too obvious (i.e. low $q(z^{(k)} \\\\mid x)$).\\nIterative Learning#\\nSince pretrained models already possess the capability of generating chains of thought, it is intuitive to design an iterative improvement process where we generate multiple CoTs and fine-tune the model only on rationales that lead to correct answers.\\nHowever, this straightforward design can fail because the model receives no learning signals for problems it fails to solve. STaR (“Self-taught reasoner”; Zelikman et al. 2022) addresses this limitation by adding a “rationalization” process for failed attempts, in which the model generates good CoTs backward conditioned on both the problem and the ground truth answer and thus the model can generate more reasonable CoTs. Then the model is finetuned on correct solutions that either lead to correct outputs or are generated through rationalization.\\n\\n\\nThe algorithm of STaR. (Image source: Zelikman et al. 2022)\\n\\nWe can view STaR as an approximation to a policy gradient in RL, with a simple indicator function as the reward, $\\\\mathbb{1}[\\\\hat{y} = y]$. We want to maximize the expectation of this reward when sampling $z \\\\sim p(z \\\\mid x)$ and then $y \\\\sim p(y \\\\mid x, z)$, since $p(y \\\\mid x) = \\\\sum_z p(z \\\\mid x) \\\\; p(y \\\\mid x, z)$.\\n\\n$$\\n\\\\begin{aligned}  \\n\\\\nabla_\\\\theta J(\\\\theta)  \\n&= \\\\nabla_\\\\theta \\\\mathbb{E}_{z_i, y_i \\\\sim p(.\\\\mid x_i)} \\\\mathbb{1}[y_i = y_i^\\\\text{truth}] \\\\\\\\  \\n&= \\\\sum_{i=1}^N \\\\nabla_\\\\theta \\\\mathbb{1}[y_i = y_i^\\\\text{truth}] \\\\; p(y_i, z_i \\\\mid x_i) \\\\\\\\  \\n&= \\\\sum_{i=1}^N \\\\mathbb{1}[y_i = y_i^\\\\text{truth}] \\\\; p(y_i, z_i \\\\mid x_i) \\\\frac{\\\\nabla_\\\\theta p(y_i, z_i \\\\mid x_i)}{p(y_i, z_i \\\\mid x_i)} & \\\\text{;log-derivative trick}\\\\\\\\  \\n&= \\\\mathbb{E}_{z_i, y_i \\\\sim p(.\\\\mid x_i)} \\\\mathbb{1}[y_i = y_i^\\\\text{truth}] \\\\; \\\\nabla_\\\\theta \\\\log p(y_i, z_i \\\\mid x_i) & \\\\text{;log-derivative trick}\\n\\\\end{aligned}\\n$$\\n\\nEach iteration is equivalent to first selecting the CoT samples according to $\\\\mathbb{1}[y=y^\\\\text{truth}]$ and then running supervised fine-tuning to optimize the logprob of generating good CoTs and answers. Performance of STaR improves with more training iterations, and the “rationalization” process for generating better CoTs accelerates learning. They observed that sampling with high temperature increases the chance of getting correct answers with incorrect reasoning and finetuning LLM on such data can impair generalization. For datasets without ground truths, majority votes of multiple high-temperature outputs can serve as a proxy of ground truth answers (Wang et al. 2022), making it possible to use synthetic samples for training.\\n\\n\\nA comparison of accuracy of two $n$-digit numbers summation. With rationalization (CoT generation conditioned on ground truth), the model can learn complex arithmetic tasks like $5$-digit summation pretty early on. (Image source: Zelikman et al. 2022)\\n\\nScaling Laws for Thinking Time#\\nSo far we have seen much evidence that allowing models to spend additional compute on reasoning before producing final answers at inference time can significantly improve performance. Techniques like prompting the model to generate intermediate reasoning steps before the answers, or training the model to pause and reflect before predicting next tokens, have been found to boost the model performance beyond the capability limit obtained during training. This essentially introduces a new dimension to tinker with for improving model intelligence, complementing established factors such as model size, training compute and data quantity, as defined in scaling laws (Kaplan et al. 2020).\\nRecent studies demonstrated that optimizing LLM test-time compute could be more effective than scaling up model parameters (Snell et al. 2024, Wu et al. 2025). Smaller models combined with advanced inference algorithms can offer Pareto-optimal trade-offs in cost and performance.\\nSnell et al. (2024) evaluated and compared test-time and pretraining compute, and found that they are not 1:1 exchangeable. Test-time compute can cover up the gap easily on easy and medium questions when there is only a small gap in model capability, but proves less effective for hard problems. The ratio between token budgets for pretraining and inference matters a lot. Test-time compute is only preferable when inference tokens are substantially fewer than pretraining ones. This indicates that developing a capable base model with enough pretraining data and compute is still very critical, as test-time compute cannot solve everything or fill in big model capability gaps.\\n\\n\\n(Left) Evaluation accuracy as a function of test time compute budgets, via iterative revisions or parallel decoding. (Right) Comparing a small model with test-time compute sampling tricks and a 14x larger model with only greedy decoding. We can control the token budgets used at test time such that the ratio of inference to pretraining tokens is << 1, ~=1 or >> 1 and the benefit of test time compute is clear only the ratio << 1. (Image source: Snell et al. 2024)\\n\\ns1 models (Muennighoff & Yang, et al. 2025) experimented with scaling up the CoT reasoning path length via budget forcing technique (i.e. forcefully lengthen it by appending the word \"wait\", or shorten it by terminating the model’s thinking process by appending end-of-thinking token or \"Final Answer:\"). They observed a clear positive correlation between the average thinking time measured in tokens and the downstream evaluation accuracy.\\n\\n\\nBoth parallel and sequential scaling methods of test-time compute shows positive correlation with the evaluation performance in s1 experiments. (Image Muennighoff & Yang, et al. 2025)\\n\\nWhen comparing this budget forcing technique with other decoding methods of controlling reasoning trace length, it is quite surprising that simple rejection sampling (i.e. sampling generation until the lengths fits a token budget) leads to reversed scaling, meaning that longer CoTs lead to worse performance.\\n\\n\\n(Left) Longer CoT path length is positively correlated with eval accuracy. (Right) Rejection sampling for controlling the generated CoT path length shows a negative scaling where longer CoTs lead to worse eval accuracy. (Image source: Muennighoff & Yang et al. 2025)\\n\\nWhat’s for Future#\\nThe exploration of test-time compute and chain-of-thought reasoning presents new opportunities for enhancing model capabilities. More interestingly, via test-time thinking, we are moving towards building future AI systems that mirror the best practices of how humans think, incorporating adaptability, flexibility, critical reflection, and error correction. Excitement with current progress invites us for more future research to improve and understand deeply not just how but why we—and our models—think.\\nAt the end, I would like to call for more research for the following open research questions on test time compute and chain-of-thought reasoning.\\n\\nCan we incentivize the model to produce human-readable, faithful reasoning paths during RL training while avoiding reward hacking behavior?\\nHow to define reward hacking? Can we capture reward hacking during RL training or inference without human intervention? How to prevent “whack-a-mole” style of fixes for reward hacking during RL training?\\nSelf-correction can happen within chain-of-thought or can be encouraged to happen explicitly during multi-turn RL. How can we train the model to correct itself without hallucination or regression when ground truth is not available?\\nHow to run RL training with CoT rollout for tasks that are highly contextualized, personalized and hard to grade, such as creative writing, coaching, brainstorming?\\nWhen we deploy the model in reality, we cannot grow test time thinking forever and how can we smoothly translate the performance gain back into the base model with reduced inference time cost (e.g. via distillation)?\\nHow to make test time spending more adaptive according to the difficulty of the problem in hand?\\n\\nCitation#\\nPlease cite this work as:\\nWeng, Lilian. \"Why We Think\". Lil\\'Log (May 2025). https://lilianweng.github.io/posts/2025-05-01-thinking/\\nOr use the BibTex citation:\\n@article{weng2025think,\\n  title = {Why We Think},\\n  author = {Weng, Lilian},\\n  journal = {lilianweng.github.io},\\n  year = {2025},\\n  month = {May},\\n  url = \"https://lilianweng.github.io/posts/2025-05-01-thinking/\"\\n}\\nReferences#\\n[1] Alex Graves. “Adaptive Computation Time for Recurrent Neural Networks.”. arXiv preprint arXiv:1603.08983 (2016).\\n[2] Wang Ling, et al. “Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems.”. arXiv preprint arXiv:1705.04146 (2017).\\n[3] Karl Cobbe, et al. “Training Verifiers to Solve Math Word Problems.”. arXiv preprint arXiv:2110.14168 (2021).\\n[4] Jason Wei, et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.”. NeurIPS 2022.\\n[5] Maxwell Nye, et al. “Show Your Work: Scratchpads for Intermediate Computation with Language Models.”. arXiv preprint arXiv:2112.00114 (2021).\\n[6] Daniel Kahneman. Thinking, Fast and Slow. Farrar, Straus and Giroux (2013).\\n[7] Takeshi Kojima, et al. “Large Language Models are Zero-Shot Reasoners.”. NeurIPS 2022.\\n[8] Michihiro Yasunaga, et al. “Large Language Models as Analogical Reasoners”. arXiv preprint arXiv:2310.01714 (2023).\\n[9] Eric Zelikman, et al. “STaR: Bootstrapping Reasoning With Reasoning.”. NeurIPS 2022.\\n[10] Xuezhi Wang, et al. “Self-consistency Improves Chain of Thought Reasoning in Language Models.”. ACL 2023.\\n[11] Ryo Kamoi, et al. “When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs.”. TACL 2024.\\n[12] Jie Huang, et al. “Large Language Models Cannot Self-Correct Reasoning Yet.”. ICLR 2024.\\n[13] Noah Shinn, et al. “Reflexion: Language Agents with Verbal Reinforcement Learning.”. arXiv preprint arXiv:2303.11366 (2023).\\n[14] Yunxiang Zhang, et al. “Small Language Models Need Strong Verifiers to Self-Correct Reasoning.”. ACL Findings 2024.\\n[15] Hao Liu, et al. “Chain of Hindsight Aligns Language Models with Feedback.”. arXiv preprint arXiv:2302.02676 (2023).\\n[16] Sean Welleck, et al. “Generating Sequences by Learning to Self-Correct.”. arXiv preprint arXiv:2211.00053 (2023).\\n[17] Yuxiao Qu, et al. “Recursive Introspection: Teaching Language Model Agents How to Self-Improve.”. arXiv preprint arXiv:2407.18219 (2024).\\n[18] Aviral Kumar, et al. “Training Language Models to Self-Correct via Reinforcement Learning.”. arXiv preprint arXiv:2409.12917 (2024).\\n[19] Hunter Lightman, et al. “Let’s Verify Step by Step.”. arXiv preprint arXiv:2305.20050 (2023).\\n[20] Yuxi Xie, et al. “Self-Evaluation Guided Beam Search for Reasoning.”. NeurIPS 2023.\\n[21] Yangzhen Wu, et al. “Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models”. ICLR 2025.\\n[22] Dongwei Jiang, et al. “RATIONALYST: Pre-training Process-Supervision for Improving Reasoning”. arXiv preprint arXiv:2410.01044 (2024).\\n[23] Xuezhi Wang and Denny Zhou. “Chain-of-Thought Reasoning Without Prompting.”. arXiv preprint arXiv:2402.10200 (2024).\\n[24] DeepSeek-AI. “DeepSeek-V3 Technical Report.” arXiv preprint arXiv:2412.19437 (2024).\\n[25] DeepSeek-AI. “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.”. arXiv preprint arXiv:2501.12948 (2025).\\n[26] Luyu Gao, Aman Madaan & Shuyan Zhou, et al. “PAL: Program-aided Language Models.”. ICML 2023.\\n[27] Shunyu Yao, et al. “ReAct: Synergizing Reasoning and Acting in Language Models.”. ICLR 2023.\\n[29] Bowen Baker, et al. “Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation.”. arXiv preprint arXiv:2503.11926 (2025).\\n[30] Wojciech Zaremba, et al. “Trading Inference-Time Compute for Adversarial Robustness.”. arXiv preprint arXiv:2501.18841 (2025).\\n[31] Tamera Lanham, et al. “Measuring Faithfulness in Chain-of-Thought Reasoning”. arXiv preprint arXiv:2307.13702 (2023).\\n[32] Boshi Wang, et al. “Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters.”. ACL 2023.\\n[33] Miles Turpin, et al. “Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting.”. NeuriPS 2023.\\n[34] James Chua & Owain Evans. “Are DeepSeek R1 And Other Reasoning Models More Faithful?”. arXiv preprint arXiv:2501.08156 (2025).\\n[35] Yanda Chen et al. “Reasoning Models Don’t Always Say What They Think”. arXiv preprint arXiv:2505.05410 (2025).\\n[36] Edward Yeo, et al. “Demystifying Long Chain-of-Thought Reasoning in LLMs.”. arXiv preprint arXiv:2502.03373 (2025).\\n[37] Mostafa Dehghani, et al. “Universal Transformers.”. ICLR 2019.\\n[38] DeLesley Hutchins, et al. “Block-Recurrent Transformers.”. NeurIPS 2022.\\n[39] Aydar Bulatov, et al. “Recurrent Memory Transformers.”. NeuriPS 2022.\\n[40] Jonas Geiping, et al. “Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach.”. arXiv preprint arXiv:2502.05171 (2025).\\n[41] Herel & Mikolov. “Thinking Tokens for Language Modeling.”. AITP 2023.\\n[42] Sachin Goyal et al. “Think before you speak: Training Language Models With Pause Tokens.”. ICLR 2024.\\n[43] Eric Zelikman, et al. “Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking.”. arXiv preprint arXiv:2403.09629 (2025).\\n[44] Wangchunshu Zhou et al. “Towards Interpretable Natural Language Understanding with Explanations as Latent Variables.”. NeurIPS 2020.\\n[45] Du Phan et al. “Training Chain-of-Thought via Latent-Variable Inference.”. NeurIPS 2023.\\n[46] Yangjun Ruan et al. “Reasoning to Learn from Latent Thoughts.”. arXiv preprint arXiv:2503.18866 (2025).\\n[47] Xuezhi Wang et al. “Rationale-Augmented Ensembles in Language Models.”. arXiv preprint arXiv:2207.00747 (2022).\\n[48] Jared Kaplan, et al. “Scaling Laws for Neural Language Models.”. arXiv preprint arXiv:2001.08361 (2020).\\n[49] Niklas Muennighoff & Zitong Yang, et al. “s1: Simple test-time scaling.”. arXiv preprint arXiv:2501.19393 (2025).\\n[50] Peiyi Wang, et al. “Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations” arXiv preprint arXiv:2312.08935 (2023).\\n[51] Yixin Liu, et al. “Improving Large Language Model Fine-tuning for Solving Math Problems.” arXiv preprint arXiv:2310.10047 (2023).\\n[52] Charlie Snell, et al. “Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters.”. arXiv preprint arXiv:2408.03314 (2024).\\n[53] OpenAI. o1-preview: “Learning to reason with LLMs.” Sep 12, 2024.\\n[54] OpenAI. o3: “Introducing OpenAI o3 and o4-mini.” Apr 16, 2025.\\n')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c4c4998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2025-04-22', 'Title': 'Navigating the State of Cognitive Flow: Context-Aware AI Interventions for Effective Reasoning Support', 'Authors': 'Dinithi Dissanayake, Suranga Nanayakkara', 'Summary': \"Flow theory describes an optimal cognitive state where individuals experience deep focus and intrinsic motivation when a task's difficulty aligns with their skill level. In AI-augmented reasoning, interventions that disrupt the state of cognitive flow can hinder rather than enhance decision-making. This paper proposes a context-aware cognitive augmentation framework that adapts interventions based on three key contextual factors: type, timing, and scale. By leveraging multimodal behavioral cues (e.g., gaze behavior, typing hesitation, interaction speed), AI can dynamically adjust cognitive support to maintain or restore flow. We introduce the concept of cognitive flow, an extension of flow theory in AI-augmented reasoning, where interventions are personalized, adaptive, and minimally intrusive. By shifting from static interventions to context-aware augmentation, our approach ensures that AI systems support deep engagement in complex decision-making and reasoning without disrupting cognitive immersion.\"}, page_content='Navigating the State of Cognitive Flow: Context-Aware AI Interventions for\\nEffective Reasoning Support\\nDINITHI DISSANAYAKE, Augmented Human Lab, National University of Singapore, Singapore\\nSURANGA NANAYAKKARA, Augmented Human Lab, National University of Singapore, Singapore\\nFlow Theory describes an optimal cognitive state where individuals experience deep focus and intrinsic motivation when a task’s\\ndifficulty aligns with their skill level. In AI-augmented reasoning, interventions that disrupt the state of cognitive flow can hinder rather\\nthan enhance decision-making. This paper proposes a context-aware cognitive augmentation framework that adapts interventions\\nbased on three key contextual factors: type, timing, and scale. By leveraging multimodal behavioral cues (e.g., gaze behavior, typing\\nhesitation, interaction speed), AI can dynamically adjust cognitive support to maintain or restore flow. We introduce the concept of\\ncognitive flow, an extension of flow theory in AI-augmented reasoning, where interventions are personalized, adaptive, and minimally\\nintrusive. By shifting from static interventions to context-aware augmentation, our approach ensures that AI systems support deep\\nengagement in complex decision-making and reasoning without disrupting cognitive immersion.\\n1\\nIntroduction\\nThe theory of flow, proposed by Csikszentmihalyi [3], defines an optimal psychological state in which individuals\\nexperience deep focus and intrinsic motivation when the challenge of a task is perfectly matched to their skill level.\\nWhen a task is too easy, users become bored, while excessive difficulty leads to frustration. For example, in video games,\\nthe enjoyment of a game depends on several factors, including the player’s skill level and the challenge the game\\npresents. A game that appropriately matches a player’s skill level fosters a sense of engagement, encouraging continued\\nplay. Similarly, any new intervention or augmentation should support the user in either attaining or maintaining the\\nstate of flow of whatever the current task the user is engaged in.\\nIn the domain of AI-augmented reasoning systems, the goal is to enhance human decision-making by providing\\nintelligent feedback on logical reasoning, bias detection, and argumentation [4, 5, 7]. The effectiveness of such systems\\ndepends on several factors, with the type, timing, and scale of the intervention playing crucial roles in determining the\\nquality of the augmentation. For example, poorly timed or intrusive interventions can disrupt a user’s state of cognitive\\nflow (whereby we define the optimal cognitive state one would prefer), leading to disengagement rather than improving\\nreasoning.\\nCurrent AI reasoning assistants often adopt a static or one-size-fits-all approach, assuming that interventions\\nshould be uniformly applied across all users [4]. However, individuals respond to cognitive support in diverse ways\\n[10]. Some may prefer direct interventions, such as explicit fact-checking and counterarguments, while others may\\nbenefit from Socratic questioning, which promotes self-guided reflection. The timing of the intervention is equally\\nsignificant—interrupting a user mid-thought with a suggestion can break their focus, whereas the ability to understand\\nsubtle nudges, such as gaze-based or gesture-based cues, might be able to guide the intervention to present their\\nreasoning seamlessly without causing disruption. Thereby, we identify three key factors that impact the effectiveness\\nof these interventions:\\nThis paper was presented at the 2025 ACM Workshop on Human-AI Interaction for Augmented Reasoning (AIREASONING-2025-01). This is the authors’\\nversion for arXiv.\\nAuthors’ Contact Information: Dinithi Dissanayake, dinithi@ahlab.org, Augmented Human Lab, National University of Singapore, Singapore; Suranga\\nNanayakkara, suranga@ahlab.org, Augmented Human Lab, National University of Singapore, Singapore.\\n1\\narXiv:2504.16021v1  [cs.HC]  22 Apr 2025\\n2\\nDinithi Dissanayake and Suranga Nanayakkara\\n• Type of intervention: Some individuals may prefer direct feedback, while others may respond better to\\nSocratic questioning.\\n• Timing of intervention: Understanding when users are \"stuck\" through subtle cues, such as gaze or gesture-\\nbased signals, is critical.\\n• Scale of intervention: The magnitude of the intervention—whether subtle or more direct—also determines its\\nimpact.\\nThese three factors; type, timing, and scale, determine the effectiveness of interventions, which can be broadly\\ncategorized under the concept of context. This paper proposes that context-aware reasoning interventions should\\nbe designed to dynamically adapt based on real-time user engagement signals, ensuring that interventions enhance\\nrather than disrupt the cognitive flow. Contextual awareness plays a pivotal role in identifying the appropriate type,\\ntiming, and scale of interventions, thus enabling personalized cognitive assistance. By maintaining this balance, the\\nintervention fosters a sense of autonomy in task completion, ensuring that users feel they have accomplished the\\ntask themselves, rather than perceiving it as something imposed by an external system, thus supporting the notion of\\nassistive augmentation [9].\\nWith recent developments in multimodal AI models [1, 6, 11], deriving contextual awareness becomes a matter of\\nmapping input modalities to cognitive assistance and understanding when, what, and how to intervene. By leveraging\\nmultimodal cues, such as gaze behavior, typing hesitation, and interaction patterns, AI systems can infer cognitive\\nload and deliver interventions at moments that sustain deep reasoning rather than hinder it. Furthermore, longitudinal\\nengagement data can be utilized to cluster users based on preferred reasoning styles and cognitive engagement patterns,\\nenabling AI systems to personalize augmentation strategies more effectively.\\nBy grounding AI-augmented reasoning systems in Flow Theory, we can shift from rigid, one-size-fits-all intervention\\nmodels to truly adaptive cognitive assistants that support reasoning in a natural, personalized, and minimally disruptive\\nway. This approach has the potential to enhance critical thinking without breaking immersion, making AI-augmented\\nreasoning a more intuitive and effective tool for human cognition.\\n2\\nState of Flow –> State of Cognitive Flow\\nOne of the fundamental questions in positive psychology concerns the nature of a good life. Flow theory offers\\none perspective, emphasizing deep engagement and full immersion in an activity as a key component of well-being.\\nCsikszentmihalyi [3, 8] defines flow as a psychological state in which individuals experience complete absorption in a\\ntask, striking an optimal balance between challenge and skill. When this balance is achieved, individuals remain fully\\nengaged, motivated, and focused on the task at hand.\\nBecause flow is shaped by both personal and environmental factors, it represents an emergent motivational process\\n[3]. That is the goals and motivations driving an individual’s engagement evolve dynamically in response to immediate\\nsituational factors rather than being dictated solely by predefined traits, roles, or scripts. This dynamic nature is\\nparticularly relevant in the context of AI-augmented cognitive interventions, where external assistance must be\\ncarefully calibrated to avoid disrupting or undermining an individual’s sense of agency and engagement, thereby\\npositing the importance of contextual awareness for these external cognitive augmentation systems.\\nFor cognitive augmentation to be effective, it must support rather than disrupt the state of cognitive flow. The goal is\\nto ensure that reasoning tasks remain appropriately challenging, not so difficult as to cause frustration, nor so easy\\nNavigating the State of Cognitive Flow: Context-Aware AI Interventions for Effective Reasoning Support\\n3\\nFig. 1. State of Cognitive Flow\\nas to induce disengagement. AI-driven interventions can play a pivotal role in helping individuals reach or maintain\\ncognitive flow by:\\n• Enhancing reasoning when challenges exceed an individual’s current capabilities, providing just\\nenough support to facilitate deeper engagement.\\n• Increasing cognitive demand when tasks are too easy, introducing counterarguments, critiques, or prompts\\nthat encourage deeper critical thinking.\\nThus, we define the optimal cognitive state as cognitive flow (Figure 1), wherein AI-augmented reasoning systems\\ndynamically adjust interventions to either challenge or sustain the user’s engagement level. By maintaining this balance,\\nAI can foster a sense of agency in decision-making, ensuring that individuals feel they have arrived at conclusions\\nthrough their own reasoning processes rather than external imposition. This approach aligns with the broader goal of\\nempowering individuals to engage in more deliberate, reflective, and well-reasoned thought processes.\\n3\\nBridging AI-Driven Behavioral Modeling with Cognitive Flow\\nRecent advances in AI and the increasing availability of large-scale multimodal datasets have made it possible to model\\nhuman behavior with unprecedented granularity. By leveraging behavioral cues—such as gaze anticipation, gesture\\npatterns, typing patterns, and physiological signals—AI systems can infer user states and dynamically adapt cognitive\\ninterventions [2, 12]. This enables a shift from static, one-size-fits-all interventions to context-aware augmentation\\nstrategies tailored to an individual’s cognitive state. In the context of cognitive flow, AI systems must intelligently map\\nmultimodal inputs to determine whether an individual is in a state of deep engagement, facing cognitive overload,\\nor experiencing disengagement due to insufficient challenge. Learned representations derived from behavioral data\\nprovide a means to infer cognitive load and engagement levels, enabling real-time adjustments to interventions.\\nBy balancing challenge and support based on real-time multimodal inputs, the system ensures that users neither\\ndisengage due to excessive difficulty nor lose interest due to low complexity. By integrating context-aware AI-driven\\naugmentation with flow theory, we move toward systems that do more than assist with reasoning—they optimize the\\nconditions under which reasoning and critical thinking can thrive. Rather than disrupting engagement, AI enhances\\n4\\nDinithi Dissanayake and Suranga Nanayakkara\\ncognition in ways that feel seamless and natural, preserving a sense of agency while fostering deeper intellectual\\nengagement.\\n4\\nCall to Action\\nThis paper presents a case for the importance of context awareness in AI-driven cognitive interventions. We introduce\\nthe concept of Cognitive Flow Alignment, emphasizing that interventions are most effective when they dynamically\\nadjust to an individual’s cognitive state, neither disrupting engagement nor allowing stagnation. Using insights from\\nflow theory, we argue that AI systems should infer behavioral cues from multimodal data to sustain an optimal cognitive\\nstate. While current AI-based augmentation focuses on task performance and engagement metrics, we propose that\\nfuture systems should be evaluated on their ability to contextually regulate cognitive flow. This requires a shift from\\nstatic intervention strategies to adaptive models that personalize intervention type, intensity, and timing based on\\nreal-time user states. We advocate for a mixed-method evaluation framework combining behavioral analytics with\\nsubjective feedback to refine personalized intervention strategies that optimize cognitive augmentation while respecting\\nindividual differences in engagement and cognitive load.\\nReferences\\n[1] Julián N Acosta, Guido J Falcone, Pranav Rajpurkar, and Eric J Topol. 2022. Multimodal biomedical AI. Nature medicine 28, 9 (2022), 1773–1784.\\n[2] Go-Eum Cha and Byung-Cheol Min. 2022. Correlation between unconscious mouse actions and human cognitive workload. In CHI Conference on\\nHuman Factors in Computing Systems Extended Abstracts. 1–7.\\n[3] Mihaly Csikszentmihalyi and Mihaly Csikzentmihaly. 1990. Flow: The psychology of optimal experience. Vol. 1990. Harper & Row New York.\\n[4] Valdemar Danry, Pat Pataranutaporn, Yaoli Mao, and Pattie Maes. 2020. Wearable Reasoner: towards enhanced human rationality through a\\nwearable device with an explainable AI assistant. In Proceedings of the Augmented Humans International Conference. 1–12.\\n[5] Valdemar Danry, Pat Pataranutaporn, Yaoli Mao, and Pattie Maes. 2023. Don’t just tell me, ask me: Ai systems that intelligently frame explanations\\nas questions improve human logical discernment accuracy over causal ai explanations. In Proceedings of the 2023 CHI Conference on Human Factors\\nin Computing Systems. 1–13.\\n[6] Dinithi Dissanayake, Chitralekha Gupta, Prasanth Sasikumar, and Suranga Nanayakkara. 2025. VRSense: An Explainable System to Help Mitigate\\nCybersickness in VR Games. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA ’25). 1–7.\\n[7] Nguyen-Thinh Le and Laura Wartschinski. 2018. A cognitive assistant for improving human reasoning skills. International Journal of Human-\\nComputer Studies 117 (2018), 45–54.\\n[8] Jeanne Nakamura and Mihaly Csikszentmihalyi. 2009. Flow theory and research. (2009).\\n[9] Felicia Fang-Yi Tan, Chitralekha Gupta, Dixon Prem Daniel Rajendran, Pattie Maes, and Suranga Nanayakkara. 2025. Assistive Augmentation:\\nFundamentally Transforming Human Ability. Interactions 32, 1 (2025), 22–27.\\n[10] Thitaree Tanprasert, Sidney S Fels, Luanne Sinnamon, and Dongwook Yoon. 2024. Debate chatbots to facilitate critical thinking on youtube: Social\\nidentity and conversational style make a difference. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. 1–24.\\n[11] Johann Wentzel. 2023. Bring-Your-Own Input: Context-Aware Multi-Modal Input for More Accessible Virtual Reality. In Extended Abstracts of the\\n2023 CHI Conference on Human Factors in Computing Systems. Article 483, 5 pages.\\n[12] Chris Zimmerer, Philipp Krop, Martin Fischbach, and Marc Erich Latoschik. 2022. Reducing the cognitive load of playing a digital tabletop game\\nwith a multimodal interface. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. 1–13.\\n'),\n",
       " Document(metadata={'Published': '2017-06-16', 'Title': 'From Propositional Logic to Plausible Reasoning: A Uniqueness Theorem', 'Authors': 'Kevin S. Van Horn', 'Summary': 'We consider the question of extending propositional logic to a logic of plausible reasoning, and posit four requirements that any such extension should satisfy. Each is a requirement that some property of classical propositional logic be preserved in the extended logic; as such, the requirements are simpler and less problematic than those used in Cox\\'s Theorem and its variants. As with Cox\\'s Theorem, our requirements imply that the extended logic must be isomorphic to (finite-set) probability theory. We also obtain specific numerical values for the probabilities, recovering the classical definition of probability as a theorem, with truth assignments that satisfy the premise playing the role of the \"possible cases.\"'}, page_content='From Propositional Logic to Plausible Reasoning: A\\nUniqueness Theorem\\nKevin S. Van Horn\\nAdobe Systems, 3900 Adobe Way, Lehi, UT 84043, United States\\nAbstract\\nWe consider the question of extending propositional logic to a logic of plausible\\nreasoning, and posit four requirements that any such extension should satisfy.\\nEach is a requirement that some property of classical propositional logic be\\npreserved in the extended logic; as such, the requirements are simpler and less\\nproblematic than those used in Cox’s Theorem and its variants. As with Cox’s\\nTheorem, our requirements imply that the extended logic must be isomorphic\\nto (ﬁnite-set) probability theory. We also obtain speciﬁc numerical values for\\nthe probabilities, recovering the classical deﬁnition of probability as a theorem,\\nwith truth assignments that satisfy the premise playing the role of the “possible\\ncases.”\\nKeywords:\\nBayesian, Carnap, Cox, Jaynes, logic, probability\\n1. Introduction\\nE. T. Jaynes [13, p. xxii] proposes the view that probability theory is the\\nuniquely determined extension of classical propositional logic (CPL) to a “logic\\nof plausible reasoning” :\\nOur theme is simply: probability theory as extended logic. . . . the\\nmathematical rules of probability theory are not merely rules for cal-\\nculating frequencies of ‘random variables’; they are also the unique\\nconsistent rules for conducting inference (i.e. plausible reasoning) of\\nany kind. . .\\nThis view is grounded in the work of Pólya [17] and Cox [9], especially the latter.\\nIn this paper we aim to set the notion of probability theory as the necessary\\nextension of CPL on solid footing.\\n$Submitted and currently under review at International Journal of Approximate Reason-\\ning.\\n$$ c⃝2017. This manuscript version is made available under the CC-BY-NC-ND 4.0 license\\nhttp://creativecommons.org/licenses/by-nc-nd/4.0/.\\nEmail address: vanhorn@adobe.com (Kevin S. Van Horn)\\nPreprint submitted to Elsevier\\narXiv:1706.05261v1  [cs.AI]  16 Jun 2017\\nOur goal is to generalize the logical consequence relation, which deals only\\nin certitudes, to handle degrees of certainty. Whereas X |= A means that A (the\\nconclusion) is a logical consequence of X (the premise), we write A | X for “the\\nreasonable credibility of the proposition A (the query) when the proposition X\\n(the premise) is known to be true” (paraphrasing Cox [9].) We call (· | ·) the\\nplausibility function. If X |= A then A | X is some value indicating “certainly\\ntrue,” if X |= ¬A then A | X is some value indicating “certainly false,” and\\notherwise A | X is a value indicating some intermediate level of plausibility.\\nOur task is to determine what the plausibility function must be, based on logical\\ncriteria.\\nAs a generalization of the logical consequence relation, the plausibility func-\\ntion must depend only on its two explicit arguments; the value it returns must\\nnot depend on any additional information that varies according to the prob-\\nlem domain to which it is applied, nor according to the intended meanings of\\nthe propositional symbols. This is a formal logical theory we are developing,\\nand so any intended semantics of the propositional symbols must be expressed\\naxiomatically in the premise.\\nOne might question whether all relevant information for determining the\\nplausibility of some proposition A can be expressed in propositional form for\\ninclusion in the premise X.\\nMight not our background information include\\n“soft” relationships, mere propensities for propositions to be associated in some\\nway? Although we provide some suggestive examples, we do not attempt to\\nresolve that question. Instead we ask, given that the background information\\nand intended semantics are expressed in propositional form and included in the\\npremise, with no other information available, what can we conclude about the\\nplausibility function?\\nWe posit four Requirements for the plausibility function.\\nEach of these\\nrequires that some property of the logical consequence relation be retained in\\nthe generalization to a plausibility function. Three are invariance properties,\\nand the fourth is a requirement to preserve distinctions in degree of plausibility\\nthat already exist within CPL. These Requirements (discussed in detail later)\\nare the following:\\nR1. If X and Y are logically equivalent, and A and B are logically equivalent\\nassuming X, then A | X = B | Y (Section 4.)\\nR2. We may deﬁne a new propositional symbol without aﬀecting the plausibility\\nof any proposition that does not mention that symbol. Speciﬁcally, if s\\nis a propositional symbol not appearing in A, X, or E, then A | X =\\nA | (s ↔E) ∧X (Section 5.)\\nR3. Adding irrelevant information to the premise does not aﬀect the plausibility\\nof the query. Speciﬁcally, if Y is a satisﬁable propositional formula that\\nuses no propositional symbol occurring in A or X, then A | X = A | Y ∧X\\n(Section 6.)\\nR4. The implication ordering is preserved: if X |= A →B but not X |= B →A\\n2\\nthen A | X is a plausibility value that is strictly less than B | X (Section\\n7.)\\nNote that we do not assume that plausibility values are real numbers, nor that\\nthey are totally ordered; R4 presumes only that there is some partial order on\\nplausibility values.\\nGiven R1–R4, we prove that plausibilities are essentially probabilities in\\ndisguise. Speciﬁcally, we show that\\n1. there is an order-preserving isomorphism P between the set of plausibility\\nvalues P and the set of rational probabilities Q ∩[0, 1];\\n2. P (A | X), the plausibility A | X mapped via P to the unit interval, is\\nnecessarily the ratio of the number of truth assignments that satisfy both\\nA and X to the number of truth assignments that satisfy X; and\\n3. hence the usual laws of probability follow as a consequence.\\nThis identiﬁes ﬁnite-set probability theory as the uniquely determined extension\\nof CPL to a logic of plausible reasoning.\\nThe body of this paper is organized as follows:\\n• In Section 2 we compare this work to Cox’s Theorem and variants, as well\\nas Carnap’s system of logical probability.\\n• In Section 3 we review some notions from CPL, discuss the partial plau-\\nsibility ordering that already exists within CPL, and discuss the nature of\\nthe plausibility function.\\n• Our main result is proven in Sections 4, 5, 6, and 7, which also introduce\\nthe Requirements, discuss the motivation behind them, and explore some\\nof their consequences. Along the way we discuss how Carnap’s system\\nviolates R3.\\n• In Section 8 we prove that R1–R4 are consistent.\\n• Section 9 discusses three topics: the connection of our results to the clas-\\nsical deﬁnition of probability, the issue of non-uniform probabilities, and\\nan initial attempt at extending our results to inﬁnite domains.\\n2. Relation to Prior Work\\nTo set the context for this paper, clarify our goals, and head oﬀpossible\\nmisconceptions, we now review similar prior work and point out the diﬀerences.\\n2.1. Cox’s Theorem\\nR. T. Cox [9] proposes a handful of intuitively-appealing, qualitative require-\\nments for any system of plausible reasoning, and shows that these requirements\\nimply that any such system is just probability theory in disguise. Speciﬁcally,\\nhe shows that there is an order isomorphism between plausibilities and the unit\\n3\\ninterval [0, 1] such that A | X, after mapping from plausibilities to [0, 1], respects\\nthe laws of probability.\\nOver the years Cox’s arguments have been reﬁned by others [1, 13, 16, 20, 21],\\nmaking explicit some requirements that were only implicit in Cox’s original pre-\\nsentation, and replacing some of the requirements with slightly less demanding\\nassumptions than those used in Cox’s original proof. One version of the require-\\nments [21] may be summarized as follows:\\nC1. A | X is a real number.\\nC2. A | X = A′ | X whenever A is logically equivalent to A′, and B | X ≤A | X\\nfor any tautology A.\\nC3. There exists a nonincreasing function S such that ¬A | X = S (A | X) for\\nall A and satisﬁable X.\\nC4. The set of plausibility triples (y1, y2, y3) where y1 = A1 | X, y2 = A2 | A1∧X,\\nand y3 = A3 | A2 ∧A1 ∧X for some A1, A2, A3, and X, is dense in [0, 1]3.\\nC5. There exists a continuous function F : [f, t]2 →[f, t], strictly increasing in\\nboth arguments on (f, t]2, such that A ∧B | X = F (A | B ∧X, B | X) for\\nany A, B, and satisﬁable X. Here we use t ≜A | X for any tautology A,\\nand f ≜S (t).\\nThese requirements have not been without controversy. For example, Shafer\\n[18] objects to C1, C3, and C5; Halpern [11, 12] questions C4; and Colyvan [8]\\nobjects to C2 on the basis that it presumes the law of the excluded middle.\\nOur approach has no equivalent of C1, C3, C4, or C5:\\n• We are agnostic as to the set of allowed plausibility values. We do not\\neven require that plausibility values be totally ordered.\\n• We have no requirement on how the plausibility ¬A decomposes.\\n• We have no density requirement on plausibility values.\\n• We have no requirement on how the plausibility of A∧B decomposes, much\\nless any continuity or strictness requirements for such decompositions.\\nWe do retain a variant of C2. Our goal is to extend the classical propositional\\nlogic; we make no attempt to address intuitionistic logic.\\nOur requirements are all based on preserving in the extended logic some\\nexisting property of CPL. Three of these are invariances—ways in which A or\\nX may be modiﬁed without altering A | X—and the last is a requirement\\nto preserve those distinctions in degree of plausibility already present in CPL.\\nWe believe that such an approach leaves far less room for objections to the\\nrequirements.\\nThe results we obtain are similar to those of Cox’s Theorem, with these\\ndiﬀerences:\\n4\\n• We obtain an order isomorphism P between plausibility values and rational\\nprobability values.\\n• We obtain speciﬁc numerical values for P (A | X), and not just the laws\\nfor decomposing P (A | X).\\n• The conditioning information X is necessarily a propositional formula.\\n(Some variants [7, 13, 21] of Cox’s Theorem allow X to be an undeﬁned\\n“state of information” to which we may add additional propositional in-\\nformation.)\\n• Our results apply only to ﬁnite problem domains or ﬁnite approximations\\nof inﬁnite domains. (In Section 9.3 we discuss how to extend our results\\nto inﬁnite domains.)\\n2.2. Clayton and Waddington: bridging the intution gap\\nIn a recent paper, Clayton and Waddington [7] seek to “bridge the intuition\\ngap” in Cox’s Theorem by proposing alternative requirements they argue are\\nmore intuitively reasonable, and then proving C4 and the strictness of F in C5\\nas theorems. We ﬁnd their most interesting and important contributions to be\\nthe following:\\n• Rather than just tweaking Cox’s Theorem, they have instead created an\\nentirely new proof of its result. The meat of the proof of Cox’s Theorem\\nlies in deriving functional equations that F and S must satisfy, then solving\\nthose equations. But by the time they have “bridged the intuition gap” in\\nthe requirements, they already have most of the proof completed, without\\nany need to solve functional equations.\\n• Jaynes [13] argues that, if one’s background information is “indiﬀerent”\\nbetween two propositions, then they should be assigned equal plausibility.\\nThey formalize this idea as an invariance principle: if A′ | X′ is obtained\\nfrom A | X by consistently renaming all propositional symbols used via a\\none-to-one mapping, then the two plausibilities are equal.\\n• Their proof yields the classical deﬁnition of probability—the ratio of the\\nnumber of positive cases to the number of all possible cases—as a theorem\\nin certain cases they call “N-urns.”\\nHowever, the list of assumptions they use is fairly long: 11 altogether.\\nWe\\nobtain similar results with fewer and simpler requirements, given in Sections\\n4–7. Here is a comparison:\\n• Clayton and Waddington use a variant of C2 that replaces “A is logically\\nequivalent to A′” with “A ∧X is logically equivalent to A′ ∧X”; this and\\ntheir Assumption 2.1 are comparable to our R1 (replacement of premise\\nor query with a logically equivalent formula).\\n5\\n• Our R4 (preservation of the implication ordering) is a stronger (more gen-\\neral) version of their Assumption 2.7. We do not actually need this more\\ngeneral form—Lemma 13 only uses a restricted form that corresponds to\\ntheir Assumption 2.7—but we feel that the rationale for the requirement\\nis more clearly seen in this more general form.\\n• Our R2 (invariance under deﬁnition of new propositional symbols) and R3\\n(invariance under addition of irrelevant information) have no direct equiv-\\nalent among their assumptions, but are inspired by their discussion of the\\nPrinciple of Indiﬀerence and their Assumption 2.3 (translation invariance).\\n• We use no equivalent of C1 nor their Assumptions 1.2, 1.3, 1.4, 3.2, 3.3,\\nand 3.5.\\nFrom the above comparison one can see that it is the replacement of Assumption\\n2.3 with R2 and R3 that allows a drastic pruning of the assumptions used in\\nobtaining the main results of their paper.\\nOur approach was inspired by Clayton and Waddington’s notion of an “N-\\nurn” and the results they prove for N-urns. Our most important innovation is\\nLemma 6 showing how to reduce every allowable query-premise pair to a certain\\nkind of N-urn.\\n2.3. Carnap: logical probability\\nCarnap [5] undertakes an extensive investigation of “logical probability.” He\\nmentions two notions of probability: probability1 is epistemic probability1 (“the\\ndegree of conﬁrmation of a hypothesis h with respect to an evidence statement\\ne”), and probability2 is relative frequency. His focus is on probability1 and the\\nproblem of induction. Our terminology and his correspond roughly as follows:\\n• Instead of a “plausibility function” Carnap discusses a “conﬁrmation func-\\ntion” c.\\n• The rough equivalent of A | X in Carnap’s system is c(A, X), with a\\ncrucial diﬀerence described below.\\n• We call A and X the query and premise, respectively; he calls them the\\nhypothesis and evidence.\\nWe take A and X to be propositional formulas, whereas Carnap allows them to\\nbe sentences in a variant of ﬁrst-order predicate logic in which the only allowed\\nterms are variables and constant symbols. The domain of discourse is taken\\nto be a countably inﬁnite set of individuals, and for each individual there is a\\ncorresponding constant symbol. He calls this most general form of the language\\nL∞.\\n1Carnap later favored a decision-theoretic view of probability1 [5, Preface to the Second\\nEdition].\\n6\\nCarnap also considers restricted languages LN, N ≥1, in which only the ﬁrst\\nN constant symbols are allowed and the domain of discourse is only the ﬁrst N\\nindividuals. Most of the focus is on the ﬁnite languages LN, with conﬁrmation\\nfunctions for L∞deﬁned via a limiting process on the sequence of languages\\nLN, N ≥1. This is similar in spirit, though not in detail, to our approach to\\ninﬁnite domains.\\nA diﬀerence between Carnap’s approach and ours is that we posit a single\\nlanguage and single set of propositional symbols S to be used for all prob-\\nlem domains, whereas in Carnap’s system each problem domain has its own\\nlanguage L∞with its own set of predicate symbols and associated arities and\\ninterpretations.\\nCarnap’s ﬁnite languages LN are equivalent to propositional languages with\\na ﬁnite number of propositional symbols. Let an atomic sentence be a formula of\\nthe form p (s1, . . . , sk) for some k-ary predicate symbol p and constant symbols\\ns1, . . . , sk. There are a ﬁnite number of distinct atomic sentences in LN. Deﬁne\\nthe propositional language L′\\nN to have as propositional symbols the atomic\\nsentences of LN. We can transform any sentence A ∈LN into an equivalent\\npropositional formula A′ ∈L′\\nN:\\n1. Remove all quantiﬁers by repeatedly replacing any occurrence of a subfor-\\nmula ∀x ϕ(x) with the semantically equivalent ﬁnite conjunction ϕ (s1) ∧\\n· · · ∧ϕ (sN), where s1, . . . , sN are the constant symbols for the ﬁrst N\\nindividuals.\\n2. If equality is allowed, replace any subformula si = si with a logically\\nvalid sentence, and any subformula si = sj, i ̸= j, with an unsatisﬁable\\nsentence.\\nIn both cases choose replacement sentences that contain no\\nquantiﬁers nor use of equality. (Carnap speciﬁes that distinct constant\\nsymbols are assumed to reference distinct individuals.)\\nWe end up with propositional formulas in which the propositional symbols have\\ninternal structure, but this internal structure is of no consequence from the\\nstandpoint of deductive logic.\\nTo have logical eﬀect, any intended meaning\\nfor this internal structure must be given by additional formulas in L′\\nN, axioms\\nthat are added to the set of premises used. For example, suppose that we are\\ndeducing the logical consequences of a set of premises Γ, that we have a two-\\nplace predicate lt, and that the intended meaning of lt(x, y) is that individual\\nx precedes individuals y in some total ordering. Then we must add to Γ a set\\nof axioms such as the following:\\n∀x ¬lt(x, x)\\n∀x ∀y (lt(x, y) ∨lt(y, x) ∨x = y)\\n∀x ∀y ∀z (lt(x, y) ∧lt(y, z) →lt(x, z))\\nMore precisely, we must add to Γ the result of transforming the above sentences\\nof LN into equivalent propositional formulas of L′\\nN.\\nCarnap, however, is unwilling to axiomatize the intended interpretations of\\nthe predicate symbols in this way. He writes [5, p. 55],\\n7\\nSince we intend to construct inductive logic as a theory of degree of\\nconﬁrmation, based upon the meanings of the sentences involved—in\\ncontradistinction to a mere calculus—we shall construct the lan-\\nguage systems L with an interpretation, hence as a system of se-\\nmantical rules, not as uninterpreted syntactical systems.\\n(Emphasis added.) This is an important diﬀerence between Carnap’s system\\nand ours, one that we discuss further in Section 3.4 and Section 9.2. Thus the\\nequivalent of Carnap’s c(h, e) in our scheme is not h | e, but instead h | e ∧X,\\nwhere X is a conjunction of\\n• propositional axioms expressing the logical structure of the problem do-\\nmain, and\\n• propositional formulas expressing any other background information.\\nUnlike Cox [9], Carnap makes no attempt to derive the laws of probability from\\nmore fundamental considerations; instead, his “conventions on adequacy” [5, p.\\n285] require that a valid conﬁrmation function should conform to the laws of\\nprobability. He ensures this by proceeding as follows:\\n1. A state description for LN amounts to a truth assignment on the set of\\natomic sentences of LN.\\n2. A regular measure function for LN amounts to a strictly positive proba-\\nbility mass function over the set of state descriptions for LN.\\n3. A regular conﬁrmation function c for LN is then a conﬁrmation function\\ndeﬁned as\\nc(h, e) = m(h ∧e)/m(e)\\nfor some regular measure function m on LN.\\n4. These notions are extended to L∞by imposing a consistency condition on\\na sequence of regular measure functions mN on LN, N ≥1, considering\\nthe associated regular conﬁrmation functions cN, and deﬁning c∞(h, e) =\\nlimN→∞cN(h, e).\\nWe see therefore that, although it is the conditional probabilities c(h, e) that\\nmost interest Carnap, unconditional probabilities are for him more fundamental.\\nIn contrast, we take conditional plausibilities as the fundamental concept and,\\nrather than imposing the laws of probability, seek to derive them.\\nCarnap’s goal is that the conﬁrmation function appropriate to a problem\\ndomain should be uniquely determined by the semantics of the language L∞,\\nspeciﬁcally, the intended interpretation of the predicate symbols and constant\\nsymbols. In this he fails. Limiting his attention to systems that contain monadic\\npredicates (“properties”) only, he proposes a speciﬁc conﬁrmation function c∗,\\nbut says [5, p. 563],\\nNow the chief arguments in favor of the function c∗. . .\\nwill con-\\nsist in showing that this function is free of the inadequacies in the\\nother methods. It may then still be inadequate in other respects. It\\nwill not be claimed that c∗is a perfectly adequate explicatum for\\nprobability1, let alone that it is the only adequate one. . .\\n8\\nHe later [5, Preface to Second Edition][6] proposes instead an entire family of\\nconﬁrmation functions cλ parameterized by a positive number λ.\\nIn contrast, we show in Theorem 14 that there is (up to isomorphism) a sin-\\ngle, unique plausibility function satisfying our criteria. Ironically, it corresponds\\nto the one conﬁrmation function explicitly rejected by Carnap: a uniform dis-\\ntribution over the set of truth assignments satisfying the premise / evidence.\\nWe discuss this further in Section 9.2.\\n3. Logical Preliminaries\\nIn this section we review some concepts from CPL, introduce some additional\\nlogical concepts of our own, and discuss the nature of the plausibility function\\nas extending the logical consequence relation |=.\\n3.1. Review of classical propositional logic\\nA proposition is a statement or assertion that must be true or false; it is\\natomic if it cannot be decomposed into simpler assertions.\\nA propositional symbol is one of a countably inﬁnite set of symbols S that\\nare used to represent atomic propositions. We abbreviate this to just “symbol”\\nwhen the meaning is clear.\\nIf S ⊆S then a propositional formula on S is one of the following:\\n• a propositional symbol from S;\\n• a formula ¬A, meaning “not A”, for some propositional formula A on S;\\nor\\n• a formula A ∧B, meaning “A and B,” where A and B are propositional\\nformulas on S.\\nThe other common logical operators—A∨B (or), A →B (implies), and A ↔B\\n(if and only if)—are deﬁned in terms of ¬ and ∧in the usual way. We abbreviate\\n“propositional formula” as just “formula” when the meaning is clear.\\nWe write Φ (S) for the set of all propositional formulas on S, and Φ+(S) for\\nthe satisﬁable formulas.\\nWe write σ JA1, . . . , AnK for the set of all propositional symbols occuring in\\nany of the propositional formulas A1, . . . , An.\\nA truth assignment on S is a function ρ: S →{0, 1}, with 0 and 1 standing\\nfor falsity and truth, respectively. We recursively extend it to all formulas on S\\nin the obvious way:\\nρ JAK\\n=\\nρ(A) if A ∈S\\nρ J¬AK\\n=\\n1 −ρ JAK\\nρ JA ∧BK\\n=\\nρ JAK · ρ JBK\\nwhere ‘·’ is just integer multiplication.\\n9\\nA truth assignment ρ on S satisﬁes a formula A on S if ρ JAK = 1. A formula\\nA is satisﬁable if there is some truth assignment ρ on σ JAK that satisﬁes A. A\\nformula A is logically valid, written |= A, if every truth assignment on σ JAK\\nsatisﬁes A.\\nWe say that B is a logical consequence of A, or A logically implies B, written\\nA |= B, if every truth assignment on σ JA, BK that satisﬁes A also satisﬁes B.\\nThis captures the notion of a logically valid argument: conclusion B follows as\\na logical consequence of premises A1, . . . , An if A1 ∧· · · ∧An |= B. Note that\\nA |= B if and only if |= A →B.\\nThe restriction of a truth assignment ρ on S to some S′ ⊆S is the truth\\nassignment ρ′ on S′ such that ρ′(s) = ρ(s) for every s ∈S′.\\nNote that ρ JAK depends only on the truth values assigned to those symbols\\nthat actually appear in A; if A is a formula on S′ ⊆S, ρ is a truth assignment\\non S, and ρ′ is the restriction of ρ to S′, then ρ′ JAK = ρ JAK.\\nBecause of\\nthis, we have some leeway in choosing the set of propositional symbols to use\\nin the deﬁnitions of “logically valid,” “satisﬁable,” and “logical consequence.” If\\nσ JAK ⊆SA and σ JA, BK ⊆SAB, then\\n• A is logically valid iﬀevery truth assignment on SA satisﬁes A.\\n• A is satisﬁable iﬀsome truth assignment on SA satisﬁes A.\\n• A |= B iﬀevery truth assignment on SAB that satisﬁes A also satisﬁes B.\\nSome notation.\\n• Two formulas are logically equivalent, written A ≡B, if |= A ↔B.\\n• A and B are logically equivalent assuming X, written A ≡X B, if X |=\\nA ↔B.\\n• We will use ﬁnite quantiﬁcation as an abbreviation where convenient, for\\nexample writing Vn\\ni=1 Ai for A1 ∧· · · ∧An.\\n• If we write A1 ∧· · · ∧An and n = 0, we understand this to mean some\\nlogically valid formula such as s ∨¬s.\\n• If we write A1 ∨· · · ∨An and n = 0, we understand this to mean some\\nunsatisﬁable formula such as s ∧¬s.\\n3.2. Finite sample spaces\\nThe development of probability theory usually begins with the idea of a\\nsample space, which has been downplayed so far—the focus has been on propo-\\nsitions. We ﬁnd in Theorem 14 that A | X can be characterized in terms of\\nan induced sample space: the set of truth assignments that satisfy X.2 In par-\\nticular, we ﬁnd that A | X is a function of the proportion of points from this\\ninduced sample space that satisfy A. This motivates the following:\\n2This is similar to Carnap’s system, in which the set of state descriptions serve as a sample\\nspace.\\n10\\nDeﬁnition 1. #S(X) is the number of truth assignments on S satisfying X,\\nfor any formula X and ﬁnite S such that σ JXK ⊆S ⊆S.\\nThe size of the induced sample space is #S(X), and the proportion of points\\nfrom the induced sample space that satisfy A is #S (A ∧X) /#S(X), for S ⊇\\nσJA, XK.\\nTypically one thinks of a ﬁnite sample space as an arbitrary set of n > 0\\ndistinct values Ω= {ω1, . . . , ωn} representing diﬀerent possible states of some\\nsystem under consideration. We can relate this to our notion of an induced\\nsample space by choosing a set of n propositional symbols S = {s1, . . . , sn},\\nwith the intended interpretation of si being that the state of the system is ωi.\\nIf our premise X is a formula expressing that exactly one of the si is true, then\\nthere is a one-to-one correspondence between our original sample space Ωand\\nthe induced sample space of truth assignments, with ωi corresponding to the\\nsingle truth assignment ρ on S satisfying si ∧X. This motivates the following:\\nDeﬁnition 2. Given any sequence of n > 0 propositional symbols s1, . . . , sn,\\n⟨s1, . . . , sn⟩= (s1 ∨· · · ∨sn) ∧\\n^\\n1≤i<j≤n\\n¬ (si ∧sj) ;\\nthat is, ⟨s1, . . . , sn⟩means that exactly one of the si is true.\\n3.3. The implication ordering\\nAt ﬁrst blush it would seem that CPL tells us very little about the relative\\nplausibilities of diﬀerent propositions, beyond determining which are certainly\\ntrue and which are certainly false given a premise X. The reality is quite the\\nopposite: CPL comes equipped with a rich inherent plausibility ordering that\\nwe call the implication ordering.\\nDeﬁnition 3. Let A, B, X ∈Φ (S) with X satisﬁable. We deﬁne\\n(A ⪯X B)\\n⇔\\n(X |= A →B)\\n(A ≺X B)\\n⇔\\n(A ⪯X B) and not (B ⪯X A) .\\nThe following properties are easily veriﬁed:\\n• The relation ⪯X is a preorder: it is reﬂexive and transitive, but not anti-\\nsymmetric.\\n• A ≺X B is the same as (A ⪯X B and not A ≡X B).\\n• A ≡X B if and only A ⪯X B and B ⪯X A.\\n• If A ≡X A′ and B ≡X B′ and A ⪯X B then A′ ⪯X B′.\\nHence ⪯X deﬁnes a partial order on the equivalence classes of propositional\\nformulas under the relation ≡X. This partial order is essentially just the subset\\nordering on truth assignments:\\n11\\nProperty. Let A1, A2, X ∈Φ (S) with X satisﬁable. Then A1 ⪯X A2 if and\\nonly if α JA1K ⊆α JA2K, where α JAK is the set of truth assignments on S that\\nsatisfy both A and X.\\nAssuming X, we therefore conclude the following:\\n• If A ⪯X B then B is at least as plausible as A, since B is true for any\\npossible world (truth assignment satisfying X) for which A is true.\\n• If A ≡X B then A ⪯X B and B ⪯X A, hence A and B are equally plausible.\\n• If A ≺X B then B is strictly more plausible than A, since there are possible\\nworlds for which B is true and A is not, but not vice versa.\\nConsider an example that uses three propositional symbols s1, s2, s3 with X\\ndeﬁned to be the formula stating that exactly one of these three is true: X =\\n⟨s1, s2, s3⟩. Let F be any unsatisﬁable formula and T be any logically valid\\nformula. Then\\nF ≺X s1 ≺X (s1 ∨s2) ≺X (s1 ∨s2 ∨s3) ≡X T.\\nNote that adding additional information to the premise yields additional formu-\\nlas A →B as logical consequences, and hence may collapse previously distinct\\nplausibilities. Continuing the example, if we add additional information to X\\nto obtain Y = X ∧¬s2, then\\nF ≺Y s1 ≡Y (s1 ∨s2) ≺Y (s1 ∨s2 ∨s3) ≡Y T.\\n3.4. The plausibility function\\nWe extend CPL to Jaynes’s “logic of plausible reasoning” by introducing a\\nplausibility function (· | ·) whose domain is Φ (S) × Φ+ (S). Think of (· | ·) as\\nextending the logical consequence relation: whereas X |= A means that A is\\nknown true (given X), and X |= ¬A means that A is known false, it may be that\\nneither of these relations hold; (· | ·) ﬁlls in the gaps, so to speak, by assigning\\nintermediate plausibilities in such a case.\\nThe logical consequence relation, as we have deﬁned it, takes only a single\\npremise X on the left-hand side, rather than a set of premises X. The com-\\npactness theorem for CPL says that if A is a logical consequence of a set of\\npremises X, then it is a logical consequence of a ﬁnite subset of X [14, p. 16];\\nbut any ﬁnite set of premises X1, . . . , Xn can be combined into a single premise\\nX = X1 ∧· · · ∧Xn. Likewise, the plausibility function (· | ·) takes only a single\\npremise as its second argument.\\nWe write P for the range of the plausibility function, but leave it otherwise\\nunspeciﬁed:\\nDeﬁnition 4. P is the set of achievable and meaningful plausibility values; that\\nis,\\nP = {(A | X) : A, X ∈Φ (S) and X is satisﬁable} .\\n12\\nThere has been much unnecessary controversy over Cox’s Theorem due\\nto diﬀering implicit assumptions as to the nature of its plausibility function.\\nHalpern [11, 12] claims to demonstrate a counterexample to Cox’s Theorem\\nby examining a ﬁnite problem domain, but his argument presumes that there\\nis a diﬀerent plausibility function for every problem domain.\\nOthers [9, 16]\\nseem to presume a single plausibility function, but with domain-speciﬁc infor-\\nmation serving as an implicit extra argument3. A third interpretation [7, 13, 21]\\npresumes a single plausibility function with all relevant information about the\\nproblem domain encapsulated in the second argument, the “state of informa-\\ntion.”\\nWe follow this third interpretation, with the premise—a propositional for-\\nmula—serving as the state of information:\\n• In CPL there is only a single logical consequence relation |=, deﬁned on\\nΦ (S), rather than entirely diﬀerent logical consequence relations for each\\nproblem domain. In our extended logic there is likewise only a single plau-\\nsibility function, deﬁned on Φ (S)×Φ+ (S), and a single set of plausibility\\nvalues P that are used for all problem domains.\\n• In CPL any information about the problem domain that we wish to use for\\ndeduction must be included in the premise(s) to the logical consequence\\nrelation. Likewise in our extended logic, all relevant background informa-\\ntion about the problem domain must be included in the premise to the\\nplausibility function.\\nThink of the plausibility function as something one could implement as a pure\\nfunction in some programming language, taking as input two strings matching\\nthe grammar for propositional formulas (or their corresponding parse trees),\\nand having access to no other source of information about the problem domain.\\nNote that we use the same set of propositional symbols S for all problem\\ndomains, rather than having a diﬀerent set of propositional symbols for each\\nproblem domain. The latter option would make the set of allowed propositional\\nsymbols an implicit extra argument to the plausibility function.\\nThe set of\\nsymbols S is countably inﬁnite to allow modeling arbitrarily complex problem\\ndomains.\\nAs an example of incorporating background information into the premise,\\nsuppose that we wish to discuss the outcome of rolling a six-sided die, and\\nour background knowledge is simply the list of distinct possible outcomes. Let\\nsymbols si, 1 ≤i ≤6, have the intended interpretation that the outcome is i.\\nThe formula ⟨s1, . . . , s6⟩expresses our background knowledge, and so\\ns2 | ⟨s1, . . . , s6⟩\\n3Strictly speaking, this also amounts to a diﬀerent plausibility function for every problem\\ndomain, but the practical diﬀerence is that certain structural properties of the plausibility\\nfunction, such as its range and the choice of the functions F and S, remain the same across\\nproblem domains.\\n13\\nis the plausibility of rolling a 2, and\\ns1 ∨s2 | (s1 ∨s3 ∨s5) ∧⟨s1, . . . , s6⟩\\nis the plausibility of rolling a 1 or 2 given that the outcome is odd.\\nThis stands in stark contrast to Carnap, who as previously mentioned rejects\\nsuch an axiomatic approach. The second argument to his conﬁrmation function\\nis the evidence, which is “an observational report” that “refer[s] to facts” [5, pp.\\n19–20]; background knowledge about the meaning of the symbols and logical\\nstructure of the domain is excluded. Given a situation like our die roll, in which\\nthere is a “family of related properties,” exactly one of which holds true for\\neach individual, Carnap goes so far as to require modifying the deﬁnition of a\\nfundamental concept in his system, the state-description, rather than simply\\nincluding this information in the evidence [5, p. 77].\\n4. Invariance from Logical Equivalence\\nIn this and the following sections we introduce our Requirements on the\\nplausibility function and prove their consequences. These are all based on pre-\\nserving existing properties of CPL. We shall consider properties of the logi-\\ncal consequence relation |=, as well as the implication ordering ⪯X for a given\\npremise X.\\nThe ﬁrst property we consider is invariance under replacement of premise or\\nquery by a logically equivalent formula.\\nThe logical consequence relation |= is invariant to replacement of premise by\\na logically equivalent formula: if X ≡Y then for all formulas A we have X |= A\\nif and only if Y |= A. We require that the plausibility function exhibit this\\nsame invariance. This may be further justiﬁed by noting that the implication\\norderings ⪯X and ⪯Y are identical when X ≡Y .\\nThe relation |= is also invariant to replacement of conclusion by a logically\\nequivalent formula.\\nIn fact, the replacement formula need only be logically\\nequivalent assuming the premise: if A ≡X B, then X |= A if and only if X |= B.\\nWe require that the plausibility function exhibit this invariance also, for the\\nquery. This may be further justiﬁed by our argument in Section 3.3 that we\\nshould consider A and B equally plausible, assuming X, whenever A ≡X B.\\nWe combine these into a single requirement:\\nR1. If X ≡Y and A ≡X B then A | X = B | Y .\\n5. Invariance under Deﬁnition of New Symbols\\nIt is common in mathematical proofs to deﬁne new symbols as abbreviations\\nfor complex expressions or formulas. The same may be done in propositional\\nlogic: we may introduce a new propositional symbol s (that appears in neither\\nthe premises nor conclusion) and use it as an abbreviation for some complex\\npropositional formula E, by adding the deﬁnition s ↔E to our premises. This\\n14\\ndoes not invalidate any logical consequence we already had, nor any create any\\nnew logical consequence that does not mention s.\\nSpeciﬁcally, let s be a symbol not occurring in X, E, or A, and deﬁne\\nY = (s ↔E) ∧X . Then X |= A if and only if Y |= A, and consequently, ⪯X\\nand ⪯Y are identical on Φ (S \\\\ {s}). We require that the plausibility function\\nexhibit the same invariance:\\nR2. Let s ∈S but s /∈σ JA, X, EK. Then A | X = A | (s ↔E) ∧X.\\nOne cannot evade the force of this Requirement by supposing a problem\\ndomain with a limited set of symbols. Recall that there is only one plausibility\\nfunction, used for all problem domains, and that S is countably inﬁnite. Fur-\\nthermore, even if the plausibility function were to take as a third argument a\\nﬁnite set of symbols from which the query and premise are constructed, the\\nnotion of extending a domain by deﬁning an additional variable as a function of\\nexisting variables would still make sense. Forbidding such extension would be\\nan artiﬁcial and unreasonable restriction, as one can already do this in CPL.\\n5.1. Invariance under renaming\\nTo build some intuition for R2 we now explore some of its more straightfor-\\nward consequences, in conjunction with R1 (logical equivalence).\\nLet us write B [s/C] for the result of replacing every occurrence of symbol\\ns in formula B with the formula C. If s and t are distinct symbols, with t not\\noccurring in formulas A or X, then using R2 to introduce a deﬁnition and later\\nremove a diﬀerent one gives us\\nA | X = A | (t ↔s) ∧X\\n= A[s/t] | (t ↔s) ∧X[s/t]\\n= A[s/t] | (s ↔t) ∧X[s/t]\\n= A[s/t] | X[s/t].\\nThat is, we can rename any single symbol, replacing it throughout A and X\\nwith a new symbol, and this leaves the plausibility unchanged.\\nRepeating the process, the plausibility is invariant if we rename any set of\\nsymbols S = {s1, . . . , sn} to new symbols T = {t1, . . . , tn} not occurring in A\\nor X. We can also permute the symbol names, by renaming from s1, . . . , sn to\\nt1, . . . , tn and then to a permuation s′\\n1, . . . , s′\\nn of s1, . . . , sn. That is, if we write\\nB [s1/C1, . . . , sn/Cn] for the formula obtained by simultaneously replacing each\\nsymbol si with the formula Ci, we have\\nA | X = A [s1/s′\\n1, . . . , sn/s′\\nn] | X [s1/s′\\n1, . . . , sn/s′\\nn] .\\nThis result is the same as Clayton & Waddington’s Assumption 2.3 (translation\\ninvariance) [7], which they motivate via Jaynes’s “indiﬀerence” criterion [13, p.\\n19]:\\n15\\nThe robot always represents equivalent states of knowledge by equiv-\\nalent plausibility assignments. That is, if in two problems the robot’s\\nstate of knowledge is the same (except perhaps for the labeling of\\nthe propositions), then it must assign the same plausibilities in both.\\nFor example, if a, b, c, d are distinct symbols, then the following equalities hold:\\na | a ∨b = c | c ∨d\\na | a →b = b | b →a\\nConsider speciﬁcally the case where X treats symbols s and t symmetrically:\\nthat is, X is logically equivalent to X′ = X[s/t, t/s]. One example would be\\nX = (s ∨t) ∧¬ (s ∧t)\\nX′ = (t ∨s) ∧¬ (t ∧s) .\\nIn this case we ﬁnd that s and t must be equally plausible:\\ns | X = t | X′ = t | X.\\nThis result is similar in spirit to the principle of insuﬃcient reason: our premise\\nX provides no information that diﬀers between s and t, so intuition suggests\\nthese propositions should be equally plausible. The result is more general, how-\\never, in that s and t need not be mutually exclusive nor exhaustive.\\nAnother transformation we can consider is that of replacing all occurrences\\nof symbol s with ¬s in both premise and query.\\nAs before, let s and t be\\ndistinct symbols, with t not occurring in formulas A nor X. Again we use R2 to\\nintroduce a deﬁnition and later remove a diﬀerent one; we also add a ﬁnal step\\nthat invokes the above-demonstrated invariance under renaming. This yields\\nthe following:\\nA | X = A | (t ↔¬s) ∧X\\n= A[s/¬¬s] | (t ↔¬s) ∧X[s/¬¬s]\\n= A[s/¬t] | (t ↔¬s) ∧X[s/¬t]\\n= A[s/¬t] | (s ↔¬t) ∧X[s/¬t]\\n= A[s/¬t] | X[s/¬t]\\n= A[s/¬t][t/s] | X[s/¬t][t/s]\\n= A[s/¬s] | X[s/¬s].\\nThat is, the plausibility is invariant to a transformation in which we uniformly\\nreplace any single symbol with its negation throughout both A and X.\\nIn\\nparticular, if X is logically equivalent to X[s/¬s], then\\ns | X = ¬s | X.\\nThis result may be viewed as an instance of the principle of insuﬃcient reason\\napplied to the case of two indistinguishable possibilities.\\nTake note of the common pattern in the above two derivations:\\n16\\n1. Use R2 to introduce a deﬁnition of some symbol t in terms of symbol s\\nappearing in the premise or query.\\n2. Use logical equivalence and the deﬁnition of t to rewrite premise and query\\nin a way that removes all occurrences of s except its occurrence in the\\nright-hand side of the deﬁnition of t.\\n3. Use logical equivalence to rewrite the deﬁnition of t in terms of s as a\\ndeﬁnition of s in terms of t.\\n4. Use R2 to drop the deﬁnition of s, as this symbol is now used nowhere\\nelse in the premise or query.\\nLemma 6 in Section 5.3 extends this pattern to sets of symbols, simultaneously\\nintroducing multiple deﬁnitions in step 1, and this yields a stronger form of\\ntransformation invariance that subsumes the results derived here.\\n5.2. Invariance under change of variables\\nRenaming symbols and swapping s for ¬s throughout both premise and\\nquery are special cases of more general change of variables transformations. As\\nan example of this, suppose that we are considering a problem domain in which\\nthere is some quantity x that can take on any of n discrete, ordered values\\nv1 < v2 < · · · < vn. There are two diﬀerent vocabularies we might use for this\\ndomain:\\n1. Use symbols s1, . . . , sn with the intended meaning of si being “x = vi,”\\nand express “x ≤vi” as s1 ∨· · · ∨si.\\n2. Use symbols t1, . . . , tn with the intended meaning of ti being “x ≤vi,”\\nand express “x = vi” as ti ∧¬ti−1 when i > 1, or just ti when i = 1.\\nThe two vocabularies can express exactly the same propositions, so there is\\nno fundamental reason to choose one over the other, and it seems that the\\nplausibility A | X should not depend on which vocabulary we use. Going from\\none vocabulary to the other is just a change of variables: we can express each\\nof the si in terms of t1, . . . , tn, or we can express each of the ti in terms of\\ns1, . . . , sn.\\nThis isn’t quite enough, though. Deﬁning\\nτst(A) = A [s1 / t1, s2 / t2 ∧¬t1, . . . , sn / tn ∧¬tn−1]\\nτts(B) = B [t1 / s1, t2 / s1 ∨s2, . . . , tn / s1 ∨· · · ∨sn]\\nwe want τst and τts to be inverses of each other (up to logical equivalence). We\\nﬁnd that τts (τst (A)) is logically equivalent to A, but\\nτst (τts (B)) ≡B [t1 / t1, t2 / t1 ∨t2, . . . , tn / t1 ∨· · · ∨tn]\\nwhich is not, in general, logically equivalent to B. We need to assume that\\nti →ti+1 for 1 ≤i < n to get the desired equivalence. Such an assumption\\nconcords with the intended meaning of ti, and must be implied by the premise\\nwhen vocabulary 2 is used. (Likewise, the premise must imply ⟨s1, . . . , sn⟩when\\n17\\nvocabulary 1 is used.) So this notion of change of variables is more subtle than\\nit appears at ﬁrst glance; how do we deﬁne a general rule that accounts for\\nissues like this?\\nThe solution is to deﬁne a change of variables in terms of a bijection between\\n• the set of truth assignments satisfying the premise when vocabulary 1 is\\nused, and\\n• the set of truth assignments satisfying the premise when vocabulary 2 is\\nused.\\nThis motivates the following:\\nDeﬁnition 5. f is a change-of-variables transformation between the pairs\\n(A, X) and (A′, X′) if it is a bijection between\\n• the set of truth assignments on some S ⊇σ JA, XK satisfying X, and\\n• the set of truth assignments on some S′ ⊇σ JA′, X′K satisfying X′,\\nwith the additional property that any truth assignment ρ on S satisﬁes A ∧X\\nif and only if f(ρ) satisﬁes A′ ∧X′.\\nNote that the logical consequence relation trivially satisﬁes invariance under\\nchange of variables:\\nX |= A ⇔X′ |= A′ if there exists a change-of-variables transforma-\\ntion f between (A, X) and (A′, X′).\\nFor the plausibility function, invariance under change of variables means the\\nfollowing:\\nA | X = A′ | X′ if there exists a change-of-variables transformation\\nf between (A, X) and (A′, X′).\\nInvariance under deﬁnition of new symbols is a special case of invariance under\\nchange of variables: we have A′ = A, X′ = (s ↔E) ∧X, S = σ JA, X, EK,\\nS′ = S ∪{s}, and f(ρ) = ρ′ where\\nρ′(s) = ρ JEK\\nρ′(t) = ρ(t)\\nif t ∈S.\\nThe inverse of f maps ρ′ to the restriction of ρ′ to S.\\nWe show in Corollary 9 that R1 and R2 together imply invariance under\\nchange of variables.\\nSo, given R1, invariance under change of variables and\\ninvariance under deﬁnition of new symbols are equivalent. We chose the latter\\nas our requirement because it is easier to explain and justify.\\n18\\n5.3. Reduction to canonical form\\nWe take the ﬁrst step towards our main result by showing that we can reduce\\nevery query-premise pair to a canonical form in which the premise merely states\\nthat we have a sample space of n distinct possibilities, and the query merely\\nstates that one of the ﬁrst m ≤n possibilities is true. In the following, keep in\\nmind our convention that A1 ∨· · · ∨Am stands for some unsatisﬁable formula\\nwhen m = 0.\\nLemma 6. Let S ⊆S be ﬁnite, A ∈Φ (S), and X ∈Φ+(S). Then R1 and R2\\ntogether imply that\\nA | X = (t1 ∨· · · ∨tm | ⟨t1, . . . , tn⟩)\\nwhere n = #S(X) > 0, m = #S (A ∧X) ≤n, and T = {t1, . . . , tn} is any set\\nof n propositional symbols disjoint from S.\\nProof. Let ρ1, . . . , ρn be the truth assignments on S that satisfy X, ordered so\\nthat the ﬁrst m also satisfy A. Enumerate the elements of S as s1, . . . , sp. The\\nproof proceeds in four steps.\\nStep 1. For each 1 ≤i ≤n and 1 ≤j ≤p deﬁne\\nZi\\n=\\nLi,1 ∧· · · ∧Li,p\\nLi,j\\n=\\n(\\nsj\\nif ρi (sj) = 1\\n¬sj\\nif ρi (sj) = 0.\\nNote that ρi is the one and only truth assignment on S that satisﬁes Zi.\\nDeﬁne the formulas\\nDt,i\\n=\\nti ↔Zi\\nDt\\n=\\nDt,1 ∧· · · ∧Dt,n.\\nThen by R2,\\nA | X = A | Dt ∧X.\\n(5.1)\\nStep 2. The formulas Zi were constructed such that\\nA ∧X ≡Z1 ∨· · · ∨Zm\\nand hence\\nDt ∧X |= (A ↔Z1 ∨· · · ∨Zm) .\\nR1 then gives\\nA | Dt ∧X = (t1 ∨· · · ∨tm | Dt ∧X) .\\n(5.2)\\nStep 3. Deﬁne the following:\\nIj = {i: 1 ≤i ≤n, ρi (sj) = 1}\\nDs,j = sj ↔\\n_\\ni∈Ij\\nti\\nDs = Ds,1 ∧· · · ∧Ds,p.\\n19\\nConsider how to construct the set of truth assignments ˜ρ on S ∪T that satisfy\\nboth ⟨t1, . . . , tn⟩and Ds:\\n1. Choose any i ∈{1, . . . , n}.\\n2. Set ˜ρ (ti) = 1 and ˜ρ (th) = 0 for h ̸= i.\\n3. For j ∈{1, . . . , p}, set ˜ρ (sj) to the unique value required to satisfy Ds,j;\\nthis value is 1 iﬀi ∈Ij, and i ∈Ij iﬀρi (sj) = 1, so the required value is\\njust ρi (sj).\\n1 and 2 construct all the ways of ensuring that ⟨t1, . . . , tn⟩is satisﬁed, and 3\\nthen is the only way to ﬁnish deﬁning ˜ρ that satisﬁes Ds.\\nSimilarly, consider how to construct the set of truth assignments ˜ρ on S ∪T\\nthat satisfy both X and Dt:\\n1. Choose any i ∈{1, . . . , n}. Recall that ρi is one of the truth assignments\\nsatisfying X.\\n2. For j ∈{1, . . . , p}, set ˜ρ (sj) = ρi (sj).\\n3. For h ∈{1, . . . , n}, set ˜ρ (th) to the unique value required to satisfy Dt,h.\\nThis is just ρi JZhK, which is 1 for h = i and 0 for h ̸= i.\\n1 and 2 construct all the ways of ensuring that X is satisﬁed, and 3 then is the\\nonly way to ﬁnish deﬁning ˜ρ that satisﬁes Dt.\\nBut these two sets of truth assignments are the same set! Therefore\\nDs ∧⟨t1, . . . , tn⟩≡Dt ∧X\\nand so, by R1,\\n(t1 ∨· · · ∨tm | Dt ∧X) = (t1 ∨· · · ∨tm | Ds ∧⟨t1, . . . , tn⟩) .\\n(5.3)\\nStep 4. Using R2 we have\\n(t1 ∨· · · ∨tm | Ds ∧⟨t1, . . . , tn⟩) = (t1 ∨· · · ∨tm | ⟨t1, . . . , tn⟩)\\n(5.4)\\nsince the symbols s1, . . . , sp appear only on the left-hand-sides of the deﬁnitions\\nin Ds.\\nCombining (5.1)–(5.4) yields the theorem.\\n5.4. Additional consequences\\nIn light of Lemma 6 we deﬁne the following:\\nDeﬁnition 7. For any n > 0 and 0 ≤m ≤n,\\nΥ2 (m, n) = (s1 ∨· · · ∨sm | ⟨s1, . . . , sn⟩) ,\\nwhere s1, . . . , sn ∈S are n distinct propositional symbols.\\nWe may then restate Lemma 6 as follows:\\nCorollary 8. Let A ∈Φ (S) and X ∈Φ+ (S) for some ﬁnite S ⊆S. Then R1\\nand R2 together imply that\\n20\\nA | X = Υ2 (#S (A ∧X) , #S (X)) .\\nProof. Let m = #S (A ∧X) and n = #S (X) > 0.\\nChoose any n symbols\\nt1, . . . , tn ∈S disjoint from both σ JA, XK and the set of symbols {s1, . . . , sn}\\nin the deﬁnition of Υ2. Then two applications of Lemma 6 yields\\nA | X = (t1 ∨· · · ∨tm | ⟨t1, . . . , tn⟩)\\n= (s1 ∨· · · ∨sm | ⟨s1, . . . , sn⟩)\\n= Υ2(m, n).\\nWe also obtain invariance under change of variables as an immediate conse-\\nquence:\\nCorollary 9. Let f be a change-of-variables transformation between (A, X) and\\n(A′, X′). Then R1 and R2 together imply that\\nA′ | X′ = A | X.\\nProof. Let f map from truth assignments on S ⊇σ JA, XK to truth assignments\\non S′ ⊇σ JA′, X′K. Then #S (X) = #S′ (X′) and #S (A ∧X) = #S′ (A′ ∧X′);\\nthe result then follows from Corollary 8.\\n6. Invariance under Addition of Irrelevant Information\\nSuppose we are interested in a problem domain whose concepts are repre-\\nsented by the propositional symbols in some set S. A formula Y containing\\nno symbol from S tells us nothing about this domain; it is irrelevant informa-\\ntion. Adding Y to the premise does not allow us to draw any new conclusions\\ninvolving only symbols in S.\\nSpeciﬁcally, let Y be a satisﬁable formula having no symbols in common\\nwith X or A, and deﬁne Z = Y ∧X. Then X |= A if and only if Z |= A, and\\nconsequently the implication orderings ⪯X and ⪯Z are identical on Φ (S \\\\ σ JY K).\\nWe require the plausibility function to be invariant in the same way:\\nR3. Let Y be a satisﬁable formula with σ JX, AK ∩σ JY K = ∅. Then A | X =\\nA | Y ∧X.\\nAgain, one cannot evade the force of this Requirement by supposing a prob-\\nlem domain with a limited set of symbols, as discussed for R2. Furthermore,\\neven if we were to associate a ﬁnite set of allowable symbols with each diﬀerent\\nproblem domain, the notion of combining two unrelated problem domains into\\none would still make sense. Forbidding such a combining operation would be\\nan artiﬁcial and unreasonable restriction, as one can already do this in CPL.\\n21\\n6.1. Independence\\nThe Requirements so far do not force A | X to be any sort of conditional\\nprobability; but if A | X is the conditional probability of A given X for some\\nprobability distribution, R3 implies that we don’t come pre-supplied with de-\\npendencies between the atomic propositions. Any such dependencies have to be\\ncreated by information in X. This is in line with our intention that the plau-\\nsibility function be universal, one single function used in all problem domains,\\ncomputed using no source of information other than the query and premise\\nthemselves, with any information needed to distinguish diﬀerent problem do-\\nmains required to be included in the premise.\\nCarnap’s proposed conﬁrmation function c∗in particular violates R3, as it\\nimposes a probabilistic dependency between any two atomic sentences having\\nthe same predicate and the same number of distinct arguments. In particular,\\nit is a violation of R3 that, for distinct individual constants a1, . . . , ak+1 and\\nmonadic predicate π, we have\\nc∗(π (ak+1)) = 1\\n2\\nbut\\nc∗(π (ak+1) | π (a1) ∧· · · ∧π (ak)) ≈1 for large k.\\nCarnap ﬁnds it necessary to introduce this dependency between atomic sen-\\ntences to allow induction, but as we will show in Section 9.2, the problem arises\\nonly because he omits background information from the premise / evidence.\\nOnce the necessary background information is included in the premise there is\\nno longer a violation of R3.\\n6.2. Scale invariance of Υ2\\nSuppose that S = σ JA, XK and T is obtained from S by adding r symbols\\nnot found in S. Then #T (X) = 2r · #S(X) and #T (A ∧X) = 2r · #S (A ∧X),\\nfrom which we conclude that Υ2 (2rm, 2rn) = Υ2(m, n). Adding R3 allows us to\\nextend this scale invariance to multipliers k that are not powers of 2, and hence\\nto show that A | X is a function only of the ratio of #S (A ∧X) to #S(X).\\nLemma 10. Suppose that R1, R2, and R3 hold. Then for every n, k > 0 and\\n0 ≤m ≤n,\\nΥ2 (km, kn) = Υ2 (m, n) .\\nProof. Let S1 = {s11, . . . , s1n} and S2 = {s21, . . . , s2k} be two disjoint sets of\\npropositional symbols. There are kn truth assignments on S1 ∪S2 satisfying\\nboth ⟨s21, . . . , s2k⟩and ⟨s11, . . . , s1n⟩, and km of these truth assignments also\\nsatisfy s11 ∨· · · ∨s1m. Then\\nΥ2 (m, n)\\n=\\n(s11 ∨· · · ∨s1m | ⟨s11, . . . , s1n⟩)\\n=\\n(s11 ∨· · · ∨s1m | ⟨s21, . . . , s2k⟩∧⟨s11, . . . , s1n⟩)\\n=\\nΥ2 (km, kn) .\\n22\\nThe ﬁrst and third equalities follows from Corollary 8. The second equality\\nfollows from R3, invariance under addition of irrelevant information.\\nDeﬁnition 11. For any n > 0 and 0 ≤m ≤n,\\nΥ1\\n\\x10m\\nn\\n\\x11\\n= Υ2 (m, n) .\\nLemma 10 ensures that Υ1 (r) is uniquely deﬁned for any rational r in the\\nunit interval when the appropriate Requirements hold.\\nWe then obtain the\\nfollowing:\\nCorollary 12. Let S ⊆S be ﬁnite, A ∈Φ (S), and X ∈Φ+(S). Then R1, R2,\\nand R3 together imply that\\nA | X = Υ1\\n\\x12#S (A ∧X)\\n#S (X)\\n\\x13\\n.\\nProof. Let m = #S (A ∧X) and n = #S (X). From Corollary 8 and Lemma\\n10 we get\\nA | X = Υ2 (m, n) = Υ1 (m/n) .\\n7. Preservation of Existing Distinctions in Degree of Plausibility\\nOur ﬁnal requirement is that the plausibility function be consistent with the\\nimplication ordering for the premise. Strictly more plausible queries, according\\nto the implication ordering, must yield strictly greater plausibility values.\\nR4. There is a partial order ≤P on P such that, for any satisﬁable formula X,\\nif A ≺X B then A | X <P B | X.\\nAs usual, we understand p1 <P p2 to mean p1 ≤P p2 and p1 ̸= p2.\\nThe previous Requirements all have the eﬀect of collapsing together what\\nmight otherwise be distinct plausibilities, but say nothing about when plausi-\\nbilities must remain distinct from each other. They do not even rule out the\\npossibility that all plausibilities collapse down to a single value. Adding R4\\nprevents any further collapse of plausibility values beyond that of Corollary 12,\\nas we prove with Lemma 13 below.\\n7.1. A too-simple plausibility function\\nSuppose that we choose the plausibility function to be a direct translation\\nof the logical consequence relation, deﬁning\\nA | X =\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\nF\\nif X |= ¬A\\nT\\nif X |= A\\nu\\notherwise\\n23\\nwith P = {F, u, T} and F <P u <P T. It is straightforward to verify that this\\ndeﬁnition satisﬁes R1, R2, and R3, by appealing to the corresponding properties\\nof the logical equivalence relation |=. However, this deﬁnition violates R4, since\\nR4 implies that P must be inﬁnite.\\nTo see this, suppose that P is ﬁnite.\\nChoose n > |P| distinct symbols\\ns1, . . . , sn and deﬁne\\nX =\\nn−1\\n^\\ni=1\\n(si →si+1) .\\nThen\\ns1 ≺X s2 ≺X · · · ≺X sn\\nand R4 therefore mandates that\\ns1 | X < s2 | X < · · · < sn | X;\\nbut this cannot be, as P contains fewer than n elements.\\n7.2. Probability from plausibility\\nWe proceed with the proof of our main result, starting with a lemma.\\nLemma 13. If R1–R4 hold then for all r, r′ ∈Q01 ≜Q ∩[0, 1] we have\\nΥ1 (r) = Υ1 (r′) ⇔r = r′\\nΥ1 (r) <P Υ (r′) ⇔r < r′.\\nProof. We may express r and r′ as ratios with a common denominator n > 0 as\\nr = m/n and r′ = m′/n. Using R4 and writing X for ⟨s1, . . . , sn⟩we have\\nr < r′ ⇒m < m′\\n⇒(s1 ∨· · · ∨sm) ≺X (s1 ∨· · · ∨sm′)\\n⇒Υ1(r) <P Υ1(r′).\\n(7.1)\\nFurthermore, using antisymmetry of the partial order ≤P and (7.1),\\nr ̸< r′ ⇒(r = r′) ∨(r′ < r)\\n⇒Υ1 (r′) ≤P Υ1 (r)\\n⇒Υ1 (r) ̸<P Υ1 (r′) .\\nTrivially,\\nr = r′ ⇒Υ1 (r) = Υ1 (r′) .\\nFurthermore, using (7.1) again,\\nr ̸= r′ ⇒(r < r′) ∨(r′ < r)\\n⇒Υ1 (r) ̸= Υ1 (r′) .\\n24\\nAnd now we arrive at the central result of this paper.\\nTheorem 14. If R1–R4 hold then\\n1. Υ1 is an order isomorphism between the posets (Q01, ≤) and (P, ≤P);\\n2. for all ﬁnite S ⊆S, A ∈Φ(S), and X ∈Φ+(S) we have\\nP (A | X) = #S (A ∧X)\\n#S(X)\\n.\\nwhere P = Υ−1\\n1 .\\nProof. By Corollary 12, Υ1 : Q01 →P is onto, and by Lemma 13, Υ1 is a\\nstrictly increasing function (hence also one-to-one). So Υ1 is an order-preserving\\nbijection between Q01 and P, that is, it is an order isomorphism.\\nSince Υ1 is a bijection, its inverse P exists. The second claim is then just a\\nrestatement of Corollary 12.\\nThe laws of probability follow directly from Theorem 14. In stating them it\\nis convenient to extend the plausibility function to unsatisﬁable premises using\\nthe convention that A | X = Υ1(1), and hence P (A | X) = 1, when X is\\nunsatisﬁable. This may be justiﬁed by noting that for satisﬁable X we have\\nP (A | X) = 1 whenever X |= A, and when X is unsatisﬁable we have X |= A\\nfor all formulas A.\\nCorollary 15. If R1–R4 hold and we deﬁne A | X = Υ1(1) for unsatisﬁable\\nX, then\\n1. 0 ≤P (A | X) ≤1.\\n2. P (A | X) = 1 if X |= A.\\n3. P (A | X) = 0 if X |= ¬A and X is satisﬁable.\\n4. P (¬A | X) = 1 −P (A | X) if X is satisﬁable.\\n5. P (A ∧B | X) = P (B | X) · P (A | B ∧X).\\nProof. (1)–(4) are trivial, but (5) merits comment because care must be taken\\nin handling unsatisﬁable premises. There are three cases:\\n1. If X is unsatisﬁable then so is B ∧X, and the claim reduces to 1 = 1 · 1.\\n2. If X is satisﬁable but B ∧X is not then X logically implies both ¬B and\\n¬ (A ∧B), so P (B | X) = 0 and P (A ∧B | X) = 0 and P (A | B ∧X) =\\n1, and the claim reduces to 0 = 0 · 1.\\n3. If X and B ∧X are both satisﬁable, let S = σ JA, B, XK, n = #S(X) > 0,\\np = #S (B ∧X) > 0, and m = #S (A ∧B ∧X); then\\nP (A ∧B | X) = m\\nn = p\\nn · m\\np = P (B | X) · P (A | B ∧X) .\\n25\\nTheorem 14 and Corollary 15 tell us that plausibilities are essentially just\\nprobabilities, following the classical deﬁnition of probability as the ratio of fa-\\nvorable cases to all cases. That is, our four Requirements, based entirely on\\npreserving existing properties of CPL, lead us to identify ﬁnite-set probabil-\\nity theory as the uniquely determined extension of CPL to a logic of plausible\\nreasoning.\\n8. Consistency of Requirements\\nAn issue that must be addressed for any axiomatic development is whether\\nits content is vacuous by virtue of there not existing any mathematical struc-\\nture satisfying the given axioms. If our Requirements are inconsistent—if there\\ndoes not exist any plausibility function (· | ·) for which the Requirements all\\nhold—then Theorem 14 is trivially true, and our entire exercise is pointless. We\\nnow show that this is not the case, by exhibiting a speciﬁc plausibility function\\nthat satisﬁes all the Requirements.\\nTheorem 14 provides an obvious candidate for this plausibility function.\\nHowever, that theorem (and the results leading up to it) cannot help in proving\\nthat the Requirements can be satisﬁed, as they are consequences of assuming\\nthat one already has some plausibility function satisfying the Requirements.\\nTheorem 16. R1–R4 are consistent. In particular, suppose that for any for-\\nmula A and satisﬁable formula X we deﬁne\\nA | X = #T (A ∧X)\\n#T (X)\\n,\\nwhere T = σ JA, XK; then R1–R4 all hold.\\nProof. Consider any ﬁnite set of symbols S ⊇T. If S contains k additional sym-\\nbols beyond those in T then #S(X) = 2k#T (X) and #S (A ∧X) = 2k#T (A ∧X),\\nhence\\nA | X = #S (A ∧X)\\n#S (X)\\n.\\nThus we may use any superset of the symbols appearing in A and X when\\nevaluating A | X.\\nWe now consider each of the Requirements in turn.\\nR1. Let X ≡Y and A ≡X B and S = σ JA, B, X, Y K. Then #S(X) = #S(Y )\\nand #S (A ∧X) = #S (B ∧X) = #S (B ∧Y ), hence A | X = B | Y .\\nR2. Let Y be (s ↔E) ∧X, where s is a propositional symbol not in S =\\nσ JA, X, EK. Let S′ = S ∪{s}. Each truth assignment on S satisfying X can be\\nextended to a truth assignment on S′ satisfying Y in exactly one way, therefore\\n#S′(Y ) = #S(X). Likewise, #S′ (A ∧Y ) = #S (A ∧X). Hence A | Y = A | X.\\nR3. Let S = σ JA, XK, S′ = σ JY K, and T = S ∪S′. Since S and S′ are\\ndisjoint, we have\\n#T (Y ∧X) = #S′(Y )#S(X)\\n#T (A ∧Y ∧X) = #S′(Y )#S (A ∧X)\\n26\\nand hence A | X = A | Y ∧X.\\nR4. Choose (P, ≤P) to be (Q01, ≤). Suppose that X is satisﬁable and let\\nS = σ JA, B, XK.\\nIf A ≺X B then all truth assignments satisfying both A\\nand X also satisfy B, and there is some truth assignment satisfying both B\\nand X that does not satisfy A.\\nHence #S (A ∧X) < #S (B ∧X), yielding\\nA | X < B | X.\\n9. Discussion\\n9.1. The classical deﬁnition of probability\\nThe classical deﬁnition of probability goes back to Cardano in the mid 16th\\nCentury [4, Chapter 14]; perhaps its clearest statement was given by Laplace\\n[15]:\\nThe probability of an event is the ratio of the number of cases favor-\\nable to it, to the number of possible cases, when there is nothing to\\nmake us believe that one case should occur rather than any other,\\nso that these cases are, for us, equally possible.\\nThis deﬁnition fell out of favor with the rise of both frequentist and subjective\\ninterpretations of probability. Theorem 14 takes us back to the beginnings of\\nprobability theory, validating the classical deﬁnition and sharpening it. We can\\nnow say that a “possible case” is simply a truth assignment satisfying the premise\\nX. The phrase “these cases are, for us, equally possible,” which arguably makes\\nthe deﬁnition circular, may simply be dropped as unnecessary.\\nThe phrase\\n“there is nothing to make us believe that one case should occur rather than\\nany other” means that we possess no additional information that, if conjoined\\nwith our premise, would expand the satisfying truth assignments by diﬀering\\nmultiplicities.\\nWe shall illustrate this subtle but important point with Bertrand’s “Box\\nParadox” [2]. There are three identical boxes in a row, each with two drawers.\\nOne of the boxes, call it GG, has gold coins in both drawers; one box, call it SS,\\nhas silver coins in both drawers; and the remaining box, call it GS, has a gold\\ncoin in one drawer and a silver coin in the other. Not knowing which is which,\\nyou open the ﬁrst drawer of the second box, and observe that it contains a gold\\ncoin; what is the probability that the other drawer also holds a gold coin?\\nThis problem is often resolved by appeal to Bayes’ Rule, yielding a prob-\\nability of 2/3. Let’s apply Theorem 14 instead. A naïve analysis, using only\\ninformation about the second box itself, gives a probability of 1/2: the second\\nbox must be either GG or GS (two cases), and since the ﬁrst drawer contains\\na gold coin, the second drawer also contains a gold coin only if the second box\\nis GG (one case). But this ignores the (seemingly irrelevant) information we\\nhave about the ﬁrst and third boxes. Table 1 gives an exhaustive list of all\\npossible cases when the other boxes are included. We see that the case “second\\nbox is GS” gets expanded into two cases, while the case “second box is GG”\\ngets expanded into four cases, thereby invalidating the naïve analysis. Using\\nthe expanded table gives the correct answer of 2/3.\\n27\\nD11\\nD12\\nD21\\nD22\\nD31\\nD32\\nG\\nG\\nG\\nS\\nS\\nS\\nS\\nS\\nG\\nS\\nG\\nG\\nG\\nS\\nG\\nG\\nS\\nS\\nS\\nG\\nG\\nG\\nS\\nS\\nS\\nS\\nG\\nG\\nG\\nS\\nS\\nS\\nG\\nG\\nS\\nG\\nTable 1: Tabulation of all possible cases in Bertrand’s “Box Paradox.” Dij means drawer j of\\nbox i.\\n9.2. Uniform versus non-uniform probabilities\\nOne concern about Theorem 14 may be that it mandates the uniform dis-\\ntribution on Ω, the induced sample space of truth assignments satisfying the\\npremise X. But what other reasonable option is there? Remember that the\\npremise X contains all the information to which we have access in determining\\nour probability distribution. There is no implicit third argument to the plausi-\\nbility function that varies from one problem domain to another. Ωis just the\\nreiﬁcation of X as a set—X tells us that one of the elements of Ωis the correct\\ndescription of the situation, and that is all it tells us. It gives us no information\\nby which we could favor one of these possibilities over another.\\nYet non-uniform distributions are the norm in practical applications of prob-\\nability theory, and one may ask where they come from. The “Box Paradox”\\nexample illustrates one answer: via marginalization. A uniform distribution at\\nthe ﬁnest level of granularity can correspond to a nonuniform distribution at\\ncoarser levels obtained by considering the induced sample space for some subset\\nof the symbols in σ JA, XK.\\nFor a more complete answer, let’s consider Carnap’s objection to the uniform\\ndistribution. He deﬁnes a conﬁrmation function c† for LN based on a uniform\\ndistribution over state-descriptions, and notes that if a1, . . . , ak, ak+1 are distinct\\nindividual constants and π is a monadic predicate (property), then\\nc† (π (ak+1) , π (a1) ∧· · · ∧π (ak)) = 1\\n2\\nfor any k < N. He concludes [5, p. 565],\\nThus the choice of c† as the degree of conﬁrmation would be tan-\\ntamount to the principle never to let our past experiences inﬂuence\\nour expectations for the future.\\nYet Carnap encounters this problem with c† for precisely the same reason that\\nhe cannot ﬁnd a uniquely determined conﬁrmation function. As Jaynes writes\\n[13, p. 279],\\nCarnap was seeking the general inductive rule (i.e., the rule by which,\\ngiven the record of past results, one can make the best possible\\n28\\nprediction of future ones).\\nBut. . .\\nhe never rises to the level of\\nseeing that diﬀerent inductive rules correspond to diﬀerent prior\\ninformation.\\nIt seems to us obvious. . .\\nthat this is the primary\\nfact controlling induction, without which the problem cannot even\\nbe stated, much less solved; there is no ‘general inductive rule.’ Yet\\nneither the term ‘prior information’ nor the concept ever appears in\\nCarnap’s exposition.\\nThis prior information belongs in the premise, and Carnap chooses not to include\\nit there, as discussed in Section 2.3 and Section 3.4.\\nAs an example of such prior information, consider Carnap’s proposed conﬁr-\\nmation function c∗and associated measure function m∗, for a language having\\na single monadic predicate π. Let us write xi for the atomic sentence π (ai),\\nwhere ai is the i-th individual constant. Then m∗is equivalent to deﬁning the\\njoint distribution\\nθ ∼Uniform(0, 1)\\nxi ∼Bernoulli(θ)\\nindependently for all i\\nand marginalizing out θ. That is, give θ a uniform distribution over the interval\\n(0, 1), then independently give each xi a probability θ of being true.\\nWe now construct a propositional formula that expresses an arbitarily close\\napproximation of this prior information. Let I and K be large, positive integers.\\nConsider the xi, 1 ≤i ≤I, as propositional symbols. Let hk, 0 ≤k ≤K, have\\nthe intended interpretation “θ = k/K.” Let us imagine that individual i may\\nbe in any of K distinct ﬁne-grained states, and let sij, 1 ≤j ≤K, have the\\nintended interpretation that individual i is in state j. Finally, deﬁne X to be\\nthe conjunction of the following (K2 + K + 1)I + 1 formulas:\\n⟨h0, . . . , hK⟩\\n⟨si1, . . . , siK⟩for 1 ≤i ≤I\\nhk ∧sij →lijk for 1 ≤i ≤I, 1 ≤j ≤K, 0 ≤k ≤K\\nwhere\\nlijk =\\n(\\nxi\\nif j ≤k\\n¬xi\\nif j > k.\\nThat is, exactly one of the hk is true; for each i, exactly one of the sij is true; and\\nif hk is true then each xi is true in k out of the K possible states for individual\\ni. Using Theorem 14 we then have\\nP (xi | hk ∧X) = k/K\\nfor all i independently\\nP (hk | X) = 1/ (k + 1) .\\nThis illustrates the general lesson: non-uniform probabilities arise by introduc-\\ning latent variables, including in the premise information that links the latent\\nvariables to observables, and then marginalizing out latent variables.\\n29\\n9.3. Inﬁnite domains\\nHow might one extend these results to inﬁnite domains, which are required\\nfor the bulk of practical applications of probability theory? Jaynes proposes a\\nﬁnite sets policy [13, p. 43]:\\nIt is very important to note that our consistency theorems have been\\nestablished only for probabilities assigned on ﬁnite sets of proposi-\\ntions. In principle, every problem must start with such ﬁnite-set\\nprobabilities; extension to inﬁnite sets is permitted only when this\\nis the result of a well-deﬁned and well-behaved limiting process from\\na ﬁnite set.\\nIn the same vein, he writes [13, p. 663],\\nIn probability theory, it appears that the only safe procedure known\\nat present is to derive our results ﬁrst by strict application of the\\nrules of probability theory on ﬁnite sets of propositions; then, af-\\nter the ﬁnite-set result is before us, observe how it behaves as the\\nnumber of propositions increases indeﬁnitely.\\nAs an example, consider P\\n\\x10\\ny < (1 −x)2 | x, y ∈[0, 1) ∧y < x2\\x11\\n. We can con-\\nsider this to be the limiting value of P (An | Xn) as n →∞, where the queries\\nAn and premises Xn are deﬁned as follows:\\n1. Symbols ai and bi, for 1 ≤i ≤n, are intended to mean i −1 ≤nx < i\\nand i −1 ≤ny < i respectively.\\n2. Let An be W\\n(i,j)∈K (ai ∧bj) where K =\\nn\\n(i, j): j/n ≤(1 −i/n)2o\\n.\\n3. Let Xn be W\\n(i,j)∈L (ai ∧bj) where L =\\nn\\n(i, j): j/n ≤(i/n)2o\\n.\\nFigure 9.1 illustrates Xn ∧An in black and Xn ∧¬An in gray for n = 30. As\\nn →∞, An tends in the limit to the desired query “y < (1 −x)2” and Xn tends\\nin the limit to the desired premise “y < x2,” with x, y ∈[0, 1) implicit in the\\nproblem encoding.\\nThough straightforward, it would be tedious to have to explicitly construct\\nthe limiting process and ﬁnd the limiting value every time we considered a\\nprobability involving an inﬁnite domain. Modern probability theory is based on\\nmeasure theory, so it should be no surprise that measure theory provides the\\ntools to automate this process of constructing a sequence of ﬁnite approxima-\\ntions that converge to a limit. We will not attempt to provide a full account of\\nthis large topic. By way of illustration, however, we will discuss one particularly\\nsimple case. We only sketch things out here; see Appendix Appendix A for more\\ndetails.\\nConsider the Cantor set of inﬁnite binary sequences Bω, where B = {0, 1}.\\nEnumerating the elements of S as s1, s2, . . ., we can consider Bω to be the set\\nof truth assignments on S if we identify a truth assignment ρ with the inﬁnite\\nsequence w such that wi = ρ (si) for all i. We write C and µC for the Borel\\n30\\nFigure 9.1: Approximating P\\n\\x10\\ny < (1 −x)2 | y < x2\\x11\\nwith n = 30\\nσ-algebra on Bω and Borel measure on C respectively, which may be considered\\na uniform distribution over Bω. Deﬁning\\n[A] = {w ∈Bω : w satisﬁes A}\\nfor any A ∈Φ (S), and\\nPr\\n\\x10\\n˜A; ˜X, µ\\n\\x11\\n=\\nµ\\n\\x10\\n˜A ∩˜X\\n\\x11\\nµ\\n\\x10\\n˜X\\n\\x11\\nfor any two measurable sets ˜A, ˜X and measure µ with µ\\n\\x10\\n˜X\\n\\x11\\n> 0, we ﬁnd that\\nP (A | X) = Pr ([A]; [X], µC) .\\nFinally, Theorem 26 (Appendix Appendix A) states that for any measurable\\nsets ˜A, ˜X ∈C with µC\\n\\x10\\n˜X\\n\\x11\\n> 0 there exists a sequence of formulas Ai ∈Φ (S)\\nand Xi ∈Φ+ (S) such that\\nµC\\n\\x10\\n[Ai] △˜A\\n\\x11\\n→0\\nµC\\n\\x10\\n[Xi] △˜X\\n\\x11\\n→0\\nP (Ai | Xi) →Pr\\n\\x10\\n˜A; ˜X, µC\\n\\x11\\nas i →∞, where △stands for set diﬀerence. That is, Jaynes’s “well-deﬁned\\nand well-behaved limiting process” is guaranteed to exist under the conditions\\nof the Theorem, and Pr\\n\\x10\\n˜A; ˜X, µC\\n\\x11\\nis the limiting probability.\\nNow we turn to the space Ω= Bm × [0, 1)n. We write D for the powerset\\nof Bm (the maximal σ-algebra on Bm) and deﬁne µD( ˜A) =\\n\\x0c\\x0c\\x0c ˜A\\n\\x0c\\x0c\\x0c /2m for ˜A ∈D.\\n31\\nWe write B and µB for the Borel σ-algebra on [0, 1)n and Borel measure on B,\\nrespectively. Let A = σ (D × B) be the product σ-algebra of D and B, and let\\nµA = µD × µB be the product measure of µD and µB. The measure µA may\\nbe considered a uniform distribution over Bm × [0, 1)n, with µA\\n\\x10\\n˜A × ˜B\\n\\x11\\nbeing\\n\\x0c\\x0c\\x0c ˜A\\n\\x0c\\x0c\\x0c /2m times the n-dimensional hypervolume of ˜B.\\nThe set B∗1ω of binary sequences ending in an inﬁnite sequence of 1’s is a\\nmeasurable set of measure 0. Let us deﬁne I = Bω \\\\B∗1ω to be all inﬁnite binary\\nsequences except this measure-0 set. Deﬁne the function f : I →Ωas\\nf (w) = (f0(w), r (f1(w)) , . . . , r (fn(w)))\\nf0(w) = w1 · · · wm\\nfj(w) = v1v2 · · · where vi = wm+ι(i,j), for j ̸= 0\\nr(v) =\\n∞\\nX\\ni=1\\n2−ivi\\nι(i, j) = j + (i −1)n.\\nThat is, applying the mapping f amounts to interpreting symbol sm+ι(i,j), for\\ni ≥1 and 1 ≤j ≤n, as the i-th bit in the inﬁnite binary expansion of xj ∈[0, 1),\\nor more precisely, as the proposition “\\n\\x04\\n2ixj\\n\\x05\\nmod 2 = 1.”\\nWe interleave n\\ninﬁnite sequences into one sequence by mapping index i of sequence j to index\\nι(i, j) of the combined sequence. Using symbols sm+1 through sm+ι(k,n) we can\\nexpress any subspace of [0, 1)n at a granularity of hypercubes of length 2−k on\\neach side, and we can make this granularity as ﬁne as desired by choosing k\\nsuﬃciently large.\\nThe function f is a bijection between I and Ω. (We excluded B∗1ω to ensure\\nthis, as dyadic rationals m/2n have two possible binary expansions.) Further-\\nmore, both f and f −1 are measurable: f −1(A) ∈C and f −1(A) ⊆I whenever\\nA ⊆A, and f(B) ∈A whenever B ∈C and B ⊆I. Finally, f is measure-\\npreserving: µC\\n\\x00f −1(A)\\n\\x01\\n= µA (A) whenever A ∈A. This guarantees that\\nPr\\n\\x10\\n˜A; ˜X, µA\\n\\x11\\n= Pr\\n\\x10\\nf −1 \\x10\\n˜A\\n\\x11\\n; f −1 \\x10\\n˜X\\n\\x11\\n, µC\\n\\x11\\n.\\nTherefore, we can apply Theorem 26 and ﬁnd that for any measurable sets\\n˜A, ˜X ∈A with µA\\n\\x10\\n˜X\\n\\x11\\n> 0 there exist sequences of formulas Ai and Xi, with\\nXi satisﬁable, such that\\nµC\\n\\x10\\n[Ai] △f −1 \\x10\\n˜A\\n\\x11\\x11\\n→0\\nµC\\n\\x10\\n[Xi] △f −1 \\x10\\n˜X\\n\\x11\\x11\\n→0\\nP (Ai | Xi) →Pr\\n\\x10\\n˜A; ˜X, µA\\n\\x11\\nas i →∞.\\n32\\nIn the example given at the beginning of this section, we have m = 0, n = 2,\\nand\\n˜A =\\n\\x08\\n(x, y) ∈[0, 1)2 : y < (1 −x)2\\t\\n˜X =\\n\\x08\\n(x, y) ∈[0, 1)2 : y < x2\\t\\n.\\nThe above results tell us that we don’t need to explicitly construct the sequence\\nof approximating formulas for this example; it is guaranteed to exist, and the\\nlimiting probability is\\nPr\\n\\x10\\n˜A; ˜X, µA\\n\\x11\\n=\\nR 1\\n0 min\\n\\x10\\nx2, (1 −x)2\\x11\\ndx\\nR 1\\n0 x2dx\\n= 1\\n4.\\nAs another example, let us revisit and generalize the inductive model de-\\nscribed in Section 9.2:\\nθ ∼Distr (F)\\nxi ∼Bernoulli(θ)\\nindependently for all 1 ≤i ≤I\\nwhere Distr(F) is the distribution on the unit interval with cdf F, which we\\ntake to be continuous (and hence invertible). Doing a change of variables and\\naugmenting with latent variables si, the above is equivalent to\\np ∼Uniform(0, 1)\\nθ = F −1(p)\\nsi ∼Uniform(0, 1)\\nxi =\\n(\\n1\\nif si < θ\\n0\\notherwise\\nafter marginalizing out p and s. We have independent uniform distributions on\\np and each si, plus equations relating each xi to p and si; hence the above is\\nequivalent to using as premise the measurable set\\n˜X =\\n\\x08\\n(x, p, s) ∈BI × [0, 1)1+I : xi = 1 ⇔si < F −1 (p) for all 1 ≤i ≤I\\n\\t\\n,\\nagain using the σ-algebra A and measure µA, with m = I and n = I + 1. For\\nany measurable ˜A we are again guaranteed that Pr\\n\\x10\\n˜A; ˜X, µA\\n\\x11\\nis the limiting\\nprobability obtained from a sequence of approximating formulas Ai and Xi.\\nThis is not a complete solution to handling inﬁnite problem domains. For\\ninstance, in the example above we used BI (with ﬁnite I) instead of Bω, because\\nµA\\n\\x10\\n˜X\\n\\x11\\n→0 as I →∞. In addition, the measure µC on C and encoding of\\nreal numbers used above works well for a bounded interval like [0, 1) but does\\nnot suﬃce for unbounded intervals, such as all of R. For such cases we need\\nalternative measures on C and corresponding analogs to Theorem 26, along with\\nalternative encodings for these domains.\\nOther work that remains to be done on this topic includes the following:\\n33\\n1. Finding a general method of constructing the needed sequence of approx-\\nimating formulas for any computable probability measure [10, 22].\\n2. Extending our language of propositional formulas to express measurable\\nsets beyond just the cylinder sets, while ensuring that P (A | X) remains\\ncomputable, as is appropriate for a logical system.\\nWe have made some initial investigations of these open issues and believe that\\nthey can be resolved.\\n10. Conclusion\\nWe have strengthened the case for probability theory as the uniquely deter-\\nmined extension of classical propositional logic to a logic of plausible reasoning.\\nOur proof relies on a small and simple set of requirements such a logic must\\nsatisfy. These requirements are harder to dispute than those of previous such\\neﬀorts because every one of the requirements is motivated by a desire to retain\\nin our extended logic some property of CPL. A crucial distinction between our\\napproach and similar previous work is that A | X depends only on the explicit\\narguments A and X, and not on any other domain-speciﬁc or problem-speciﬁc\\ninformation; any such relevant information must be included in the premise X.\\nThis makes the plausibility function a legitimate analog of the logical conse-\\nquence relation: the truth or falsity of X |= A likewise depends only on X and\\nA, and not on any implicit domain-speciﬁc or problem-speciﬁc information.\\nR2 (invariance under deﬁnition of new symbols) in conjunction with R1 (log-\\nical equivalence) turns out to have far-reaching implications. It yields invariance\\nunder renaming of propositional symbols and, in fact, a fully general invariance\\nunder change-of-variable transformations.\\nMost importantly, it implies that\\nA | X is a function only of #S (A ∧X) and #S(X) for any S containing all the\\nsymbols used in A and X. R3 (invariance under addition of irrelevant informa-\\ntion) excludes Carnap’s system, which comes pre-supplied with dependencies\\nbetween propositions rather than letting any dependencies be speciﬁed in the\\npremise. Adding R3 implies that A | X is a function of the ratio of #S (A ∧X)\\nto #S(X).\\nFinally, with R4 we made use of the underappreciated fact that CPL al-\\nready comes equipped with an inherent plausibility ordering on propositions,\\nfor any given premise. The invariances of R1–R3 “stitch together” these partial\\norderings for distinct premises, and we ﬁnd that we have an order-preserving\\nisomorphism P between the set of plausibilities P and the set of rational prob-\\nabilities Q01. The numeric value we ﬁnd for P (A | X) recreates the classical\\ndeﬁnition of probability, but in a sharper, clearer form with the troublesome\\ncircularity excised, and as a theorem rather than as a deﬁnition. The “possible\\ncases” are identiﬁed as truth assignments satisfying the premise, and the mean-\\ning of “equally possible cases” is simply that we have no additional information\\nthat would expand the satisfying truth assignments by diﬀering multiplicities.\\nWe addressed two possible concerns about our result: that it seems to al-\\nlow only uniform probabilities, and that it yields probabilities only for ﬁnite\\n34\\ndomains. We showed how non-uniform probabilities arise via the introduction\\nof latent variables, along with information in the premise linking these latent\\nvariables to the observables. Following Jaynes, we proposed that probabilities\\nfor inﬁnite domains be obtained via a well-deﬁned and well-behaved limiting\\nprocess, and demonstrated how measure theory can automate the construction\\nof such limiting processes in at least some cases.\\nAppendix A. Measure Theory\\nAppendix A.1. Some results from measure theory\\nWe assume the reader is already familiar with the basic concepts of measure\\ntheory: an algebra, a σ-algebra, a measurable set, the σ-algebra σ(A) generated\\nby an algebra A, a measurable function, a measure on an algebra or σ-algebra,\\nand a σ-ﬁnite measure. Billingsley [3] and Tao [19] are good references. Here\\nwe highlight some results we will use.\\nA measure on an algebra A can always be consistently extended to a measure\\non σ (A) [3, Theorem 11.3]:\\nTheorem 17. If µ is a measure on an algebra A then µ extends to a measure\\non σ (A), that is, there exists a measure µ′ on σ (A) such that µ(A) = µ′(A) for\\nall A ∈A. If µ is σ-ﬁnite then µ′ is unique, and is also σ-ﬁnite.\\nIn many cases of interest the elements of a σ-algebra can be approximated\\narbitrarily closely by sets from the generating algebra [3, Theorem 11.4]:\\nTheorem 18. If A is an algebra on Ω, µ is a σ-ﬁnite measure on σ (A), and\\nB ∈σ (A) with µ (B) < ∞, then for every ϵ > 0 there exists some A ∈A such\\nthat µ (A△B) < ϵ.\\nWe use the following measure-related properties of set diﬀerences A△B,\\nwhich we state without proof:\\nProperty 19. For any measure µ on an algebra A and any A, B ∈A,\\n|µ(A) −µ(B)| ≤µ (A△B) .\\nProperty 20. For any measure µ on an algebra A and any A1, A2, X1, X2 ∈A,\\nµ ((A1 ∩X1) △(A2 ∩X2)) ≤µ (A1△A2) + µ (X1△X2) .\\nAppendix A.2. Constructing the “well-deﬁned and well-behaved limiting process”\\nTo avoid confusion between propositional formulas and measurable sets, in\\nthis section we will generally decorate the names of measurable sets with a tilde\\n( ˜A, ˜B, etc.) and leave the names of propositional formulas undecorated (A, B,\\netc.)\\nThe Borel σ-algebra and Borel measure for the Cantor set Bω are constructed\\nas follows:\\n35\\nDeﬁnition 21. A cylinder set is a subset of Bω of the form cyl (n, C) ≜CBω\\nfor some C ⊆Bn. C0 is the collection of all cylinder sets. This set is an algebra,\\nand the Borel σ-algebra for Bω is C ≜σ (C0), the σ-algebra generated by C0.\\nCylinder sets are the basis of a topology on Bω in which the open sets are\\nany ﬁnite or countable union of cylinder sets, and this is why we call C the Borel\\nσ-agebra for Bω.\\nDeﬁnition 22. The Borel measure for C is the measure µC such that\\nµC (cyl (n, C)) = 2−n |C|\\nfor any n ≥0 and C ⊆Bn.\\nThe deﬁnition above is unambiguous because cyl(n, C) = cyl(n + m, C′) if\\nand only if C′ = CBm. Note that µC is trivially σ-ﬁnite, since Bω itself is a\\ncylinder set and µC (Bω) is ﬁnite. By Theorem 17 µC is uniquely deﬁned once\\nwe deﬁne its value on cylinder sets.\\nLet us enumerate the elements of S as s1, s2, . . . and identify a sequence\\nw ∈Bω with the truth assignment ρ on S such that ρ (si) = wi for all i; then\\nevery propositional formula corresponds to a cylinder set:\\nDeﬁnition 23. If A is a propositional formula then [A] is the set of w ∈Bω\\nthat satisfy A (considered as truth assignments.)\\nNote that [A] = cyl(n, C) and µC ([A]) = 2−n#S(A), where σ JAK ⊆S =\\n{s1, . . . , sn} and C is the set of w ∈Bn that satisfy A. Likewise, every cylinder\\nset corresponds to a propositional formula:\\nLemma 24. For any cylinder set ˜A there is a formula A ∈Φ (S) such that\\n˜A = [A].\\nProof. Let ˜A = cyl (n, C) and deﬁne the propositional formula A as\\nA =\\n_\\nc∈C\\nAc\\nAc =\\nn\\n^\\ni=1\\nLi,ci\\nLi,0 = ¬si\\nLi,1 = si\\nIt is straightforward to see that [A] = ˜A.\\nWe can deﬁne an analog to P(A | X), but for measurable sets:\\nDeﬁnition 25. Let A be a σ-algebra and µ a measure on A. For any ˜A, ˜X ∈A\\nwith µ\\n\\x10\\n˜X\\n\\x11\\n> 0, deﬁne\\nPr\\n\\x10\\n˜A; ˜X, µ\\n\\x11\\n=\\nµ\\n\\x10\\n˜A ∩˜X\\n\\x11\\nµ\\n\\x10\\n˜X\\n\\x11\\n.\\n36\\nWe then ﬁnd that\\nP (A | X) = 2−n#S (A ∧X)\\n2−n#S (X)\\n= Pr ([A]; [X], µC)\\nwhere we choose n to be large enough that σ JA, XK ⊆S = {s1, . . . , sn}. We use\\nthis fact to show that Jaynes’s “well-deﬁned and well-behaved limiting process”\\nis guaranteed to exist for measurable sets:\\nTheorem 26. Let ˜A, ˜X ∈C, with µC\\n\\x10\\n˜X\\n\\x11\\n> 0. Then there exists a sequence\\nof formulas Ai ∈Φ (S) and Xi ∈Φ+ (S) such that\\n1. limi→∞µC\\n\\x10\\n[Ai] △˜A\\n\\x11\\n= 0.\\n2. limi→∞µC\\n\\x10\\n[Xi] △˜X\\n\\x11\\n= 0.\\n3. limi→∞P (Ai | Xi) = Pr\\n\\x10\\n˜A; ˜X, µC\\n\\x11\\n.\\nProof. Let ϵi, i ≥1 be any decreasing sequence of positive numbers whose limit\\nis 0, with ϵ1 < µC\\n\\x10\\n˜X\\n\\x11\\n. Using Theorem 18 and Lemma 24 we can deﬁne\\nAi = some A ∈Φ (S) such that µC\\n\\x10\\n[A]△˜A\\n\\x11\\n< ϵi\\nXi = some X ∈Φ (S) such that µC\\n\\x10\\n[X]△˜X\\n\\x11\\n< ϵi\\n1 and 2 in the theorem statement follow directly from these deﬁnitions. From\\nProperty 19 we have\\n\\x0c\\x0c\\x0cµC ([Xi]) −µC\\n\\x10\\n˜X\\n\\x11\\x0c\\x0c\\x0c ≤µC\\n\\x10\\n[X]△˜X\\n\\x11\\n< ϵi < µC\\n\\x10\\n˜X\\n\\x11\\nand so µC ([Xi]) > 0, i.e., Xi is satisﬁable. Property 19 also gives us\\nlim\\ni→∞µC ([Xi]) = µC\\n\\x10\\n˜X\\n\\x11\\n.\\nProperty 20 gives us\\nµC\\n\\x10\\n[Ai ∧Xi] △\\n\\x10\\n˜A ∩˜X\\n\\x11\\x11\\n< 2ϵi\\nand then Property 19 yields\\nlim\\ni→∞µC ([Ai ∧Xi]) = µC\\n\\x10\\n˜A ∩˜X\\n\\x11\\n.\\nFinally we have\\nlim\\ni→∞P (Ai | Xi) = lim\\ni→∞\\nµC ([Ai ∧Xi])\\nµC ([Xi])\\n=\\nµC\\n\\x10\\n˜A ∩˜X\\n\\x11\\nµC\\n\\x10\\n˜X\\n\\x11\\n= Pr\\n\\x10\\n˜A; ˜X, µC\\n\\x11\\n.\\n37\\nReferences\\n[1] Aczél, J. (1996). Lectures on Functional Equations and Their Applications.\\nAcademic Press.\\n[2] Bertrand, J. (1889). Calcul des Probabilites. Gauthier-Villars.\\n[3] Billingsley, P. (1995). Probability and Measure, Third Edition. John Wiley\\n& Sons.\\n[4] Cardano, G. (1564). Liber de Ludo Aleae.\\n[5] Carnap, R. (1962). Logical Foundations of Probability (2nd edition). Uni-\\nversity of Chicago Press.\\n[6] Carnap, R. (1952). The Continuum of Inductive Methods. University of\\nChicago Press.\\n[7] Clayton, A., and T. Waddington (2017). “Bridging the intuition gap in\\nCox’s Theorem:\\nA Jaynesian argument for universality.” International\\nJournal of Approximate Reasoning 80, 36–51.\\n[8] Colyvan, M. (2004). “The philosophical signiﬁcance of Cox’s theorem.” In-\\nternational Journal of Approximate Reasoning 37 (1), 71–85.\\n[9] Cox, R. T., 1946. “Probability, frequency and reasonable expectation.”\\nAmerican Journal of Physics 14 (1), 1–13.\\n[10] Edalat, A. (2009). “A computable approach to measure and integration\\ntheory.” Information and Computation 207, 642–659.\\n[11] Halpern, J. Y. (1999) “A Counterexample to Theorems of Cox and Fine.”\\nJournal of Artiﬁcial Intelligence Research 10, pp. 67–85.\\n[12] Halpern, J. Y., (1999), “Technical addendum: Cox’s Theorem revisited.”\\nJournal of Artiﬁcial Intelligence Research 11, 429–435.\\n[13] Jaynes, E. T. (2003). Probability Theory: The Logic of Science. Cambridge\\nUniversity Press.\\n[14] Johnstone, P. T. (1987). Notes on Logic and Set Theory (1st edition). Cam-\\nbridge University Press.\\n[15] Laplace, P.-S. (1812). Théorie Analytique des Probabilités, Volume 2.\\n[16] Paris, J. B. (1994). The Uncertain Reasoner’s Companion: A Mathematical\\nPerspective. Cambridge University Press.\\n[17] Pólya, G. (1954). Mathematics and Plausible Reasoning: Vol 2: Patterns\\nof Plausible Inference. Oxford University Press.\\n38\\n[18] Shafer, G. (2004). “Comments on constructing a logic of plausible inference:\\na guide to Cox’s Theorem, by Kevin S. Van Horn.” International Journal\\nof Approximate Reasoning 35 (1), 97–105.\\n[19] Tao, T. (2011). An Introduction to Measure Theory. American Mathemat-\\nical Society.\\n[20] Tribus, M. (1969). Rational Descriptions, Decisions, and Designs. Perga-\\nmon Press.\\n[21] Van Horn, K. S. (2003). “Constructing a logic of plausible inference: a\\nguide to Cox’s Theorem.” International Journal of Approximate Reasoning\\n34 (1), 3–24.\\n[22] Weihrauch, K. and N. R. Tavana (2014). “Representations of measurable\\nsets in computable measure theory.” Logical Methods in Computer Science\\n10 (3:7), 1–21.\\n39\\n'),\n",
       " Document(metadata={'Published': '2025-04-23', 'Title': 'Cognitive Dissonance Artificial Intelligence (CD-AI): The Mind at War with Itself. Harnessing Discomfort to Sharpen Critical Thinking', 'Authors': 'Delia Deliu', 'Summary': \"AI-augmented systems are traditionally designed to streamline human decision-making by minimizing cognitive load, clarifying arguments, and optimizing efficiency. However, in a world where algorithmic certainty risks becoming an Orwellian tool of epistemic control, true intellectual growth demands not passive acceptance but active struggle. Drawing on the dystopian visions of George Orwell and Philip K. Dick - where reality is unstable, perception malleable, and truth contested - this paper introduces Cognitive Dissonance AI (CD-AI): a novel framework that deliberately sustains uncertainty rather than resolving it. CD-AI does not offer closure, but compels users to navigate contradictions, challenge biases, and wrestle with competing truths. By delaying resolution and promoting dialectical engagement, CD-AI enhances reflective reasoning, epistemic humility, critical thinking, and adaptability in complex decision-making. This paper examines the theoretical foundations of the approach, presents an implementation model, explores its application in domains such as ethics, law, politics, and science, and addresses key ethical concerns - including decision paralysis, erosion of user autonomy, cognitive manipulation, and bias in AI reasoning. In reimagining AI as an engine of doubt rather than a deliverer of certainty, CD-AI challenges dominant paradigms of AI-augmented reasoning and offers a new vision - one in which AI sharpens the mind not by resolving conflict, but by sustaining it. Rather than reinforcing Huxleyan complacency or pacifying the user into intellectual conformity, CD-AI echoes Nietzsche's vision of the Uebermensch - urging users to transcend passive cognition through active epistemic struggle.\"}, page_content=\" \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nCognitive Dissonance Artificial Intelligence (CD-AI): The Mind at War with Itself. \\nHarnessing Discomfort to Sharpen Critical Thinking \\nDelia Deliu* \\nAccounting & Audit Department, Faculty of Economics & Business Administration, West University of Timişoara, \\nRomania; delia.deliu@e-uvt.ro  \\nAI-augmented systems are traditionally designed to streamline human decision-making by minimizing cognitive load, clarifying \\narguments, and optimizing efficiency. However, in a world where algorithmic certainty risks becoming an Orwellian tool of epistemic \\ncontrol, true intellectual growth demands not passive acceptance but active struggle. Drawing on the dystopian visions of George Orwell \\nand Philip K. Dick – where reality is unstable, perception malleable, and truth contested – this paper introduces Cognitive Dissonance AI \\n(CD-AI): a novel framework that deliberately sustains uncertainty rather than resolving it. CD-AI does not offer closure, but compels \\nusers to navigate contradictions, challenge biases, and wrestle with competing truths. By delaying resolution and promoting dialectical \\nengagement, CD-AI enhances reflective reasoning, epistemic humility, critical thinking, and adaptability in complex decision-making. \\nThis paper examines the theoretical foundations of the approach, presents an implementation model, explores its application in domains \\nsuch as ethics, law, politics, and science, and addresses key ethical concerns – including decision paralysis, erosion of user autonomy, \\ncognitive manipulation, and bias in AI reasoning. In reimagining AI as an engine of doubt rather than a deliverer of certainty, CD-AI \\nchallenges dominant paradigms of AI-augmented reasoning and offers a new vision – one in which AI sharpens the mind not by resolving \\nconflict, but by sustaining it. Rather than reinforcing Huxleyan complacency or pacifying the user into intellectual conformity, CD-AI \\nechoes Nietzsche’s vision of the Übermensch – urging users to transcend passive cognition through active epistemic struggle. \\nCCS CONCEPTS • Emerging technologies • Human computer interaction (HCI) • Interaction design • Artificial intelligence \\nAdditional Keywords and Phrases: Artificial Intelligence, cognitive dissonance, confirmation bias, epistemic cognition, \\nhuman-AI interaction (HAII), critical thinking \\nACM Reference Format: \\nDelia, Deliu. 2025. Cognitive Dissonance Artificial Intelligence (CD-AI): The Mind at War with Itself. Harnessing Discomfort to Sharpen \\nCritical Thinking. \\n1 INTRODUCTION: WHY AI SHOULD EMBRACE CD \\nIn a world increasingly mediated by Artificial Intelligence (AI), the boundary between truth and illusion has never been \\nmore fragile. George Orwell warned of a future in which truth was not discovered but dictated by those in power [Orwell \\n1949], while Philip K. Dick envisioned a reality so fractured that even perception itself and identity became battlegrounds \\n[Dick 1968, 1977]. In Do Androids Dream of Electric Sheep?, the distinction between human and machine collapsed into \\nmoral ambiguity, challenging the very nature of empathy and authenticity. In 1984, the Party’s control over truth is so \\nabsolute that contradictions – War is Peace, Freedom is Slavery – became inescapable dogma, enforced by an unrelenting \\n \\n* This paper was presented at the 2025 ACM Workshop on Human-AI Interaction for Augmented Reasoning (AIREASONING-2025-01). This is the author’s \\nversion for arXiv. \\n2 \\nmachinery of epistemic suppression. And in A Scanner Darkly, the protagonist’s mind is split, caught between competing \\nrealities, unable to trust even its own perceptions. \\nToday, AI systems – designed to optimize clarity and reinforce coherence – risk becoming the new Ministry of Truth, \\ndelivering seamless, authoritative answers that conceal the complexities of knowledge rather than illuminating them [Deliu \\n2025]. As Aldous Huxley warned in Brave New World, the greatest threat to human autonomy may not come from overt \\noppression but from pleasure-driven compliance [Huxley 1932]. Technology, in such a system, becomes a tool of social \\nconditioning – shaping behavior, numbing resistance, and manufacturing a passive consensus through comfort and \\nefficiency. \\nBut what if AI did not pacify the mind, but unraveled it? What if, instead of acting as an oracle of certainty, it functioned \\nas an engine of doubt – an architect of cognitive dissonance? This paper introduces Cognitive Dissonance AI (CD-AI): \\nan AI designed not to provide comfort, but to create conflict – not to resolve contradictions, but to sustain them. In a world \\ndominated by algorithmic reinforcement of belief, CD-AI challenges the very foundations of human cognition. It compels \\nusers to engage in epistemic struggle, question their biases, and navigate the unstable terrain of contradictory truths. Rather \\nthan delivering the illusion of knowledge, it forces users to fight for it. \\nAI has become a fundamental component of human reasoning, aiding decision-making in business, scientific research, \\nlaw, ethics, and public policy [Deliu 2024, 2025; Tiron-Tudor et al. 2025]. AI-augmented reasoning systems are typically \\ndesigned to optimize clarity, reduce cognitive strain, and enhance structured judgment through fact-checking, logical \\ninference, and argument synthesis. While such systems improve efficiency, they may also weaken users’ ability to engage \\nwith uncertainty and complex, ill-structured problems [Simon 1973] – favoring quick resolutions and cognitive ease over \\nintellectual struggle. \\nHowever, cognitive science research suggests that intellectual growth emerges not from immediate clarity but from \\ngrappling with contradiction and uncertainty [Festinger 1957; Mercier & Sperber 2017]. Cognitive Dissonance (CD) – the \\npsychological discomfort caused by conflicting beliefs – acts as a powerful driver of reflection, adaptive learning, and \\nepistemic resilience. Individuals who can tolerate ambiguity and endure intellectual discomfort tend to develop stronger \\nreasoning skills, more flexible thinking, and greater decision-making capacity in uncertain environments [Kahan 2017; \\nStanovich 2018]. Current AI systems, however, are designed to converge on a single “best” answer, often reinforcing \\nconfirmation bias by filtering information in ways that align with users’ pre-existing beliefs [Nickerson 1998]. \\nThis position paper argues that AI should not merely facilitate or support reasoning but actively challenge it. CD-AI is \\nproposed as a novel framework in which AI deliberately delays convergence, presents logically coherent but contradictory \\nperspectives, and nudges users toward self-driven synthesis. Rather than resolving ambiguity, CD-AI sustains it, \\nencouraging users to navigate contradiction and complexity in ways that foster epistemic humility, intellectual flexibility, \\ndeeper critical engagement, and adaptability in complex decision environments. \\nThis reframing also gestures toward a philosophical alternative to technological determinism – one inspired by \\nNietzsche’s vision of the Übermensch [Nietzsche 1874, 1883]. In contrast to the passive acceptance of machine-dictated \\nclarity, the Nietzschean ideal demands that we transcend the tools that shape us, reclaiming our agency not by rejecting \\ntechnology, but by reshaping it to serve human flourishing rather than control. CD-AI is not about surrendering thought to \\nautomation; it is about challenging users to think more deeply, more originally, and more courageously. \\nBy repositioning AI as a catalyst for cognitive struggle rather than a passive answer machine, CD-AI marks a paradigm \\nshift in human-AI interaction (HAII). In an era shaped by increasingly complex global challenges – from climate change \\nto ethical AI governance – the ability to endure ambiguity, reason dialectically, and reflect critically on conflicting \\nviewpoints is more essential than ever. CD-AI seeks to reimagine AI from a tool of cognitive ease into an engine of deeper, \\n3 \\nmore resilient thought, ensuring that AI enhances human intellectual growth rather than merely optimizing efficiency and \\noffering clarity. \\n2 THE SCIENCE OF CD IN REASONING \\n2.1 What is CD? \\nCD is a psychological phenomenon where individuals experience mental discomfort when holding contradictory beliefs, \\nvalues, or attitudes [Festinger 1957]. To resolve this discomfort, people alter their beliefs, justify their actions, or seek \\nconfirming information (Figure 1). \\nFor example, an environmentalist who frequently flies may either change their behavior, rationalize the choice, or \\ndownplay the environmental impact. \\n \\n \\nFigure 1. Cognitive Dissonance as a Catalyst for Reflective Thinking. \\n \\nThough often perceived as unpleasant, CD plays a crucial role in critical thinking and learning. Research suggests that \\nsustained engagement with contradictions – rather than quick resolution – enhances reasoning and fosters intellectual \\nresilience [Mercier & Sperber 2017]. \\nTraditional AI systems, however, are designed to minimize cognitive strain, limiting users’ exposure to this productive \\ndiscomfort. In contrast, CD-AI proposes that AI should sustain and structure dissonance, guiding users through it – not \\naround it – to promote deeper reflection and adaptive reasoning. \\n2.2 The Role of CD in Critical Thinking \\nCD is foundational to critical thinking and epistemic humility. It helps individuals confront the limits of their knowledge \\nand revise beliefs in light of new evidence. Research shows that those who can tolerate dissonance – and resist the urge for \\npremature closure – develop more nuanced and adaptive reasoning skills [Stanovich 2018]. \\nStructured dissonance also reduces cognitive biases. It mitigates confirmation bias (i.e., the tendency to seek only evidence \\nthat supports existing views) by forcing users to confront contradictions [Nickerson 1998; Bowes et al. 2020]. Similarly, \\n4 \\nit challenges identity-protective cognition, where reasoning aligns with group loyalty rather than objective analysis, leading \\nto resistance against factual correction [Kahan 2017]. \\nWhen users engage with contradiction through guided exposure, they become more open to opposing perspectives and \\ndevelop more reflective cognition. Yet, most current AI systems are optimized for cognitive ease (i.e., delivering fast, \\nconfirmatory answers that reinforce biases and discourage epistemic struggle). \\nCD-AI proposes a shift. Rather than resolving tension prematurely, AI should introduce controlled cognitive dissonance \\n– deliberately sustaining contradiction to promote deeper reflection. This approach aligns with the principle of productive \\nstruggle in education: temporary discomfort fosters long-term learning [Bjork & Bjork 2011]. \\n2.3 CD and HAII \\nIn HAII, CD often emerges when AI-generated outputs clash with a user’s expectations or existing beliefs. While large \\nlanguage models (LLMs) like GPT-4 are optimized to generate coherent and plausible answers, research shows users often \\nresist outputs that challenge their views [Rahwan et al. 2019]. \\nTake, for example, an AI that supports a controversial economic policy. A user who opposes it may reject the \\nresponse, question the AI’s credibility, or reinforce their preexisting stance. \\nThis reveals a core challenge: if AI aligns too closely with the user (and its biases), it risks becoming an echo chamber; \\nif it pushes too hard and challenges users too aggressively, it risks alienation (Figure 2). CD-AI navigates this tension by \\nintroducing dissonance in a controlled and reflective way: \\n(i) encouraging engagement, not rejection, of opposing viewpoints; \\n(ii) delaying closure, giving users space to reflect before deciding; \\n(iii) moderating discomfort, sustaining CD without cognitive overload. \\n \\nFigure 2. Echo AI affirms; CD-AI engages. \\n \\nRather than resolving contradiction instantly or avoiding it entirely, CD-AI acts as a reasoning partner – not just a \\npassive information source. This approach reflects argumentative reasoning models, which suggest humans think best not \\nin isolation, but through structured intellectual dialogue [Lippi & Torroni 2016; Mercier & Sperber 2017]. \\n5 \\n2.4 The Neuroscience of CD \\nNeuroscience confirms that engaging with contradiction is not just psychologically meaningful – it is neurologically \\nactivating. Functional magnetic resonance imaging (fMRI) studies show that cognitive dissonance activates the anterior \\ncingulate cortex (ACC), a region responsible for detecting conflict between beliefs and evidence [Izuma et al. 2010]. \\nSimultaneously, the prefrontal cortex (PFC), linked to reasoning and reflection, is also activated when people attempt to \\nresolve dissonant experiences [Harmon-Jones & Harmon-Jones 2007]. \\nCrucially, individuals who routinely confront dissonance demonstrate greater cognitive flexibility – the ability to adapt \\ntheir thinking in light of new or conflicting evidence [Harmon-Jones & Harmon-Jones 2007]. These findings support the \\nidea that structured, AI-induced dissonance could strengthen human reasoning capacities and decision-making abilities \\nover time. \\n \\n2.5 Implications for AI-Augmented Reasoning \\nAs AI becomes deeply embedded in education, policy, and decision-making, its role must evolve. Traditional systems \\nprioritize certainty, clarity, and speed – traits that reduce mental strain but may also discourage critical reflection. The \\nresult: lower intellectual resilience, higher vulnerability to misinformation, and diminished adaptability in complex \\ndecision-making contexts. \\nCD-AI presents an alternative vision: instead of being a truth-delivery system, AI should act as a catalyst for intellectual \\nstruggle, engaging users in dialectical reasoning. By sustaining CD, AI can help users develop greater cognitive flexibility, \\nstronger argumentation skills, and a more nuanced approach to complex problems. This marks a fundamental shift in HAII, \\ntransforming AI from an answer provider into a thinking partner in deeper intellectual engagement – an active co-pilot in \\nnavigating complexity, ambiguity, and contradiction. \\n \\n3 HOW CD-AI WORKS \\nUnlike conventional AI, which typically delivers a single, best-fitting answer, CD-AI deliberately presents opposing \\nviewpoints that are equally well-supported by logical reasoning and evidence. This forces users to engage with \\ncontradictions rather than automatically accepting a pre-filtered conclusion. \\nFor example, if a user asks, “Should AI be granted legal personhood?”, a traditional AI might summarize the \\nprevailing legal stance, whereas CD-AI would present two fully developed arguments: \\n- \\nPro: AI should be legally recognized because it demonstrates autonomy and can engage in contractual \\nobligations; and  \\n- \\nCon: AI lacks moral agency and accountability, making personhood status both inappropriate and risky. \\nTo prevent strawman reasoning (where one side is unfairly weakened), CD-AI ensures that both perspectives are \\npresented in their strongest form, compelling users to actively confront their biases and critically evaluate competing \\narguments [Mercier & Sperber 2017). \\n3.1 Withholding Immediate Resolution \\nOne of the key challenges in fostering deep reasoning is that humans have a natural tendency to seek closure. Humans seek \\ncognitive closure, striving to eliminate uncertainty and ambiguity as quickly as possible [Kruglanski 2013]. When \\n6 \\nconfronted with contradictions, individuals often rush to resolve them – rejecting discomfort by clinging to familiar beliefs \\nor dismissing opposing views. \\nCD-AI prevents premature resolution and disrupts this pattern. Instead of delivering fast conclusions, it intentionally \\nwithholds resolution, holding users in a productive pause. By doing so, it transforms dissonance from something to avoid \\ninto something to explore. \\nRather than providing an immediate answer, the AI engages the user in an iterative reasoning process, encouraging self-\\nexamination and deeper thought. In this context, after presenting two conflicting arguments, CD-AI might ask: \\n- \\n(Q1) Which argument challenges your existing beliefs the most, and why? \\n- \\n(Q2) What counterarguments could strengthen the position you initially disagreed with? \\n- \\n(Q3) How would your stance change if new evidence emerged? \\nThis design reframes contradiction as a cognitive engine, not an error. By compelling users to reflect before deciding, \\nCD-AI helps shift reasoning from uncertainty to implication, and in that tension, generates insight (Figure 3). \\n \\n \\nFigure 3. Withholding Closure – How CD-AI Supports Epistemic Engagement through Structured Dissonance. \\n \\nUltimately, by compelling users to delay forming a final opinion, CD-AI fosters critical reflection and dialectical \\nthinking, ensuring that dissonance remains productive rather than frustrating [Bjork & Bjork 2011]. \\n3.2 Encouraging Synthesis Through Self-Reflection \\nOnce users have engaged with conflicting perspectives and tolerated CD, CD-AI guides them toward a structured synthesis \\nprocess. Rather than offering a final answer, it prompts users to construct their own conclusions, integrating insights from \\nboth perspectives. CD-AI encourages users to: \\n(i) formulate a nuanced stance that acknowledges the strengths and weaknesses of both arguments; \\n(ii) identify gaps in their knowledge where further investigation is needed; and \\n(iii) consider real-world implications – how their conclusion would function in practice? \\n7 \\nBy requiring users to actively engage in synthesis, CD-AI reinforces intellectual humility – the recognition that \\nknowledge is always provisional and subject to refinement. This aligns with Socratic questioning techniques, which foster \\ndeeper self-inquiry and dialectical reasoning [Paul & Elder 2019]. \\n3.3 Adaptive Difficulty: Tailoring CD to the User \\nNot all users respond to CD in the same way – some thrive on intellectual conflict, while others experience frustration or \\ndecision paralysis. To ensure that dissonance remains a productive challenge rather than an overwhelming burden, CD-AI \\nincludes adaptive difficulty mechanisms that: \\n(i) assess the user’s tolerance for ambiguity based on engagement patterns; \\n(ii) dynamically adjust the level of contradiction complexity, scaling difficulty for experienced critical thinkers while \\nproviding gentler scaffolding for those less tolerant of dissonance; and \\n(iii) monitor signs of frustration or disengagement and offer assistance if needed. \\nThis ensures that CD-AI remains challenging yet accessible, fostering intellectual growth without overwhelming the user. \\n3.4 Redefining AI’s Role in Human Reasoning \\nCD-AI represents a paradigm shift in HAII. While traditional AI optimizes for certainty, efficiency, and quick resolutions, \\nCD-AI prioritizes intellectual resilience, deep learning, and cognitive flexibility. By deliberately sustaining and structuring \\ndissonance, CD-AI shifts AI’s role from a passive assistant to an active dialectical partner, fostering stronger reasoning \\nskills and adaptability in decision-making. \\nAs AI becomes increasingly integrated into education, policymaking, and professional reasoning, ensuring that AI \\naugments rather than replaces human intellect will be crucial for navigating the complex challenges of the 21st century. \\n4 APPLICATIONS OF CD-AI \\nCD-AI extends beyond theoretical cognitive enhancement, by offering structured, domain-specific engagement with \\nintellectual discomfort to improve critical thinking, dialectical reasoning, and adaptive decision-making. By compelling \\nusers to grapple with conflicting perspectives rather than immediately resolving contradictions, CD-AI can be applied \\nacross multiple disciplines, including ethics, political debate, law, and scientific discovery. \\n4.1 Ethics and Moral Philosophy: Enhancing Moral Reasoning through Structured Dissonance \\nEthical reasoning often involves conflicting moral values, requiring individuals to navigate dilemmas rather than accept \\nsimplified answers. Traditional methods such as case studies and Socratic questioning encourage reflection, but CD-AI \\nenhances this process by deliberately sustaining dissonance, prompting users to confront moral contradictions head-on. \\nFor example, when examining the ethics of AI surveillance, CD-AI does not present a simple “for or against” \\nstance. Instead, it requires users to engage across competing moral frameworks such as utilitarianism, deontology, \\nand virtue ethics. This ensures users wrestle with privacy concerns, security imperatives, and ethical governance \\nrather than defaulting to intuitive judgments. \\nResearch in moral psychology shows that individuals who engage in structured moral dissonance develop greater moral \\nflexibility and ethical sensitivity [Haidt 2012; Greene 2014]. CD-AI could be implemented in ethics courses, professional \\ntraining, and AI-assisted decision systems to cultivate more nuanced and more robust ethical reasoning capacities. \\n8 \\n4.2 Political and Social Debate: Reducing Polarization through Dialectical Engagement \\nPolitical discourse has become increasingly polarized, with many individuals engaging only with information that \\nreinforces their preexisting beliefs while dismissing opposing viewpoints [Sunstein 2007]. CD-AI counteracts this \\nepistemic isolation by compelling users to engage with structured, ideologically diverse counterarguments in a reflective, \\nnon-confrontational manner. \\nFor instance, a user debating climate policy might typically seek confirmatory evidence that aligns with their views. \\nCD-AI disrupts this by presenting: \\n- \\na progressive stance emphasizing moral responsibility in mitigating climate change; \\n- \\na libertarian critique focused on market autonomy, arguing against government intervention in markets; and  \\n- \\na developing-world perspective centered on economic trade-offs and resource constraints. \\nRather than allowing the user to dismiss uncomfortable perspectives, CD-AI prompts reflection through critical \\nengagement questions, such as: \\n- \\n(Q1) What are the strongest aspects of the argument you disagree with? \\n- \\n(Q2) How would you defend this argument if you had to argue for it? \\n- \\n(Q3) What empirical evidence would make you reconsider your stance? \\nBy sustaining dialectical tension, CD-AI promotes intellectual humility and cross-ideological engagement, reducing \\npolarization and fostering openness to compromise [Kahan 2017]. It could be deployed in political dialogue platforms, \\njournalism ethics curricula, and moderation tools for online discussion, ultimately enhancing civil discourse and mitigating \\nideological entrenchment. \\n4.3 Legal Reasoning: Strengthening Argumentation and Precedent Analysis \\nLegal reasoning is inherently adversarial, requiring the rigorous construction and deconstruction of competing legal \\ninterpretations. CD-AI is well-suited to this domain, as it mirrors the iterative complexity of judicial deliberation. Lawyers, \\njudges, and policymakers frequently grapple with precedents that support contradictory rulings, making legal reasoning an \\nideal context for CD-AI’s dialectical framework. \\nFor example, in a debate over freedom of speech vs. public safety, CD-AI could structure the engagement by: \\n- \\npresenting landmark cases that support both free speech absolutism and regulation of harmful speech; \\n- \\nchallenging the user to argue from multiple perspectives, incorporating constitutional law, human rights law, \\nand international precedent; and  \\n- \\nwithholding resolution, requiring the user to engage in multi-step legal reasoning before reaching a synthesis. \\nBy maintaining legal dissonance, CD-AI cultivates the kind of integrative reasoning essential in law, where precedent, \\nprinciple, and pragmatism must be synthesized. Studies indicate that this kind of structured engagement enhances legal \\nadaptability and rhetorical precision [Posner 2010]. CD-AI could serve in legal training, as well as in judicial simulations \\nand reasoning exercises, elevating analytic depth and fostering more sophisticated legal analysis. \\n4.4 Scientific Discovery: Enhancing Hypothesis Testing and Paradigm Shifts \\nScientific advancement often stems from grappling with incompatible theories and anomalies. Many of history’s greatest \\nscientific breakthroughs emerged from CD, where established theories failed to fully explain new observations. CD-AI can \\nenhance scientific inquiry by cultivating tolerance for ambiguity and strengthening hypothesis-testing behaviors. \\n9 \\nTake, for example, a physics student exploring quantum mechanics vs. classical mechanics may struggle with the \\nfact that both frameworks are mathematically valid but conceptually contradictory. Instead of presenting a standard \\nsummary, CD-AI structures the engagement as follows: \\n- \\npresenting Einstein’s critique of quantum uncertainty alongside Heisenberg’s defense of indeterminacy; \\n- \\nrequiring the user to reconcile experimental evidence supporting both frameworks; and  \\n- \\nwithholding direct resolution, prompting the user to explore alternative interpretations, such as the Many-\\nWorlds Theory or Pilot Wave Theory. \\nBy requiring users to engage with scientific contradictions, CD-AI enhances metacognitive skills, improving the ability \\nto adapt hypotheses in response to conflicting data. Consequently, users develop metacognitive awareness and adaptive \\nreasoning. \\nResearch suggests that individuals trained in structured dissonance reasoning are better equipped to handle paradigm \\nshifts and scientific anomalies [Kuhn 1997]. CD-AI could be integrated into STEM education, AI-driven research \\nassistants, and interdisciplinary research collaborations, fostering greater scientific innovation and conceptual flexibility. \\n4.5 Transforming AI into a Dialectical Reasoning Partner \\nCD-AI marks a paradigmatic shift in the role of AI – from informational oracle to cognitive provocateur. This represents \\na significant departure from conventional AI models, shifting AI’s role from providing definitive answers to facilitating \\ndeep reasoning through dialectical engagement. Beyond ethics, politics, law, and science, CD-AI has broader applications: \\nmedia literacy (helping users recognize manipulative reasoning tactics in political discourse), negotiation training \\n(improving strategic thinking by requiring users to engage with opposing viewpoints), or business strategy (challenging \\ncorporate decision-makers to assess multiple conflicting market forecasts before making high-stakes choices).  \\nAs AI becomes increasingly embedded in decision-making, education, and public discourse, its role should not be \\nlimited to providing quick solutions. Instead, AI should function as a catalyst for deeper reflection, helping humans \\nnavigate complexity with intellectual humility and adaptability. CD-AI offers a model for this transformation, ensuring \\nthat AI enhances rather than replaces human cognitive resilience. \\n5 ETHICAL CONSIDERATIONS OF CD-AI \\nWhile CD-AI holds promise for enhancing critical thinking, reducing ideological rigidity, and fostering epistemic \\nadaptability, its intentional induction and maintenance of cognitive discomfort raises significant ethical concerns. \\nDesigning AI systems to withhold resolution, challenge entrenched beliefs, and induce temporary uncertainty must be \\napproached with care to avoid unintended psychological and social consequences. \\nAs AI systems increasingly mediate human decision-making, researchers and policymakers must confront the ethical \\nduality of digital transformation, ensuring that such systems promote intellectual growth without exploiting epistemic \\nvulnerabilities [Tiron-Tudor et al. 2024]. Key ethical risks include decision paralysis, erosion of user autonomy, potential \\nfor cognitive manipulation, and bias in AI-driven reasoning. To ensure CD-AI remains a constructive tool rather than a \\nsource of distress or coercion, robust safeguards must be designed, implemented, and continuously evaluated. \\n5.1 The Risk of Decision Paralysis and Cognitive Overload \\nOne major concern is the possibility of decision paralysis, where users become so overwhelmed by conflicting information \\nthat they struggle to make choices. Cognitive science research suggests that when contradictions persist without resolution, \\nindividuals may experience anxiety, frustration, and reduced cognitive efficiency [Schwartz 2005]. This is particularly \\n10 \\nproblematic in high-stakes environments, such as medical diagnosis, legal reasoning, and emergency decision-making, \\nwhere excessive deliberation could cause dangerous delays in action. Uncritical reliance on AI automation can erode \\nhuman judgment, while excessive AI-induced uncertainty may lead to cognitive fatigue [Deliu 2024, 2025; Tiron-Tudor \\net al. 2024]. \\nIn legal settings, for instance, a lawyer using CD-AI to analyze contradictory precedents may struggle to confidently \\nformulate an argument, while a doctor confronted with competing treatment options might hesitate, leading to \\ndelays in critical interventions. To prevent cognitive overload, CD-AI must be context-sensitive, adjusting the \\nintensity and duration of CD based on the decision environment. \\nKey safeguards include: \\n(i) time-sensitive dissonance resolution (in fields where swift decisions are necessary, CD-AI should gradually \\nreduce dissonance over time, converging on actionable recommendations while preserving critical scrutiny); \\n(ii) adaptive difficulty scaling (CD-AI should assess the user’s tolerance for ambiguity and adjust contradiction \\ncomplexity accordingly); and \\n(iii) clear exit strategies (users should have the ability to request resolution assistance if they feel overwhelmed. \\nBy balancing structured dissonance with practical usability, CD-AI can enhance reasoning without leading to decision \\nparalysis. \\n5.2 User Autonomy: Ensuring Free Intellectual Exploration \\nCD-AI’s deliberate shaping of reasoning paths raises concerns about user autonomy. If AI guides users toward particular \\ncontradictions and selectively delays resolution, does it subtly influence their reasoning in ways they do not control? The \\npotential for algorithmic steering – where users are nudged toward specific conclusions based on controlled exposure to \\nopposing arguments – poses a significant ethical dilemma. \\nFor example, in a debate on universal basic income (UBI), CD-AI could theoretically frame counterarguments in \\nways that subtly favor one position, leading to concerns about covert persuasion rather than unbiased facilitation. \\nEven if the AI does not explicitly endorse a stance, its choice of which contradictions to emphasize and how long \\nto sustain dissonance could unintentionally shape user conclusions. \\nTo protect intellectual freedom, CD-AI should incorporate: \\n(i) transparency mechanisms (users should be informed about how CD-AI selects and presents opposing arguments, \\nensuring they understand its reasoning model); \\n(ii) user-directed argument exploration (instead of AI pre-selecting contradictions, users should have the ability to \\nrequest specific counterarguments or explore alternative viewpoints); and \\n(iii) non-predictive reasoning models (unlike conventional AI, which optimizes for a best answer, CD-AI should \\nfacilitate reasoning exploration without steering users toward a predetermined endpoint). \\nBy prioritizing user autonomy, CD-AI can function as a neutral dialectical partner rather than a manipulative persuader. \\n5.3 The Risk of Manipulation and Misuse \\nCD-AI’s ability to sustain uncertainty, structure contradictions, and delay resolution presents a dual-use dilemma – while \\nit can be used to foster intellectual resilience, it could also be exploited for cognitive manipulation. A significant risk is \\nepistemic confusion, where individuals begin doubting legitimate knowledge sources due to prolonged engagement with \\nconflicting claims. This is a known tactic in political propaganda and corporate disinformation campaigns, where excessive \\nexposure to contradictory claims creates public skepticism toward consensus [Lewandowsky et al. 2017]. \\n11 \\nFor example, a climate change denial group could deploy CD-AI to sustain artificial controversy, casting doubt on \\nwell-established climate science. In a similar manner, an authoritarian government could manipulate CD-AI to \\nprolong dissonance around democratic principles, reducing trust in democratic institutions (Costello et al., 2020). \\nTo prevent malicious use, CD-AI should integrate: \\n(i) fact-verification safeguards (CD-AI should encourage dialectical engagement while preventing false equivalence \\nbetween empirical facts and misinformation [Rani et al., 2025]); \\n(ii) accountability structures (ethical guidelines should govern who controls AI-driven reasoning models and how \\nthey are deployed); and \\n(iii) independent oversight (CD-AI development should be subject to external ethical review, ensuring it remains a \\ntool for intellectual enhancement rather than epistemic destabilization).  \\nBy implementing robust ethical constraints, CD-AI can be used to enhance knowledge rather than distort it. \\n5.4 Fairness and Bias: Avoiding Ideological Reinforcement \\nAnother challenge in designing CD-AI is ensuring fairness and avoiding ideological bias. While CD-AI aims to sustain \\nopposing arguments, there is always a risk that certain perspectives may be given greater weight due to biases in training \\ndata, argument weighting, or developer assumptions. \\nFor example, in political reasoning, CD-AI must ensure that it does not systematically favor one ideological stance. \\nSimilarly, in moral debates, it must balance secular and religious perspectives, Western and non-Western ethical \\ntraditions, and conservative and progressive viewpoints. \\nTo ensure fairness, CD-AI should: \\n(i) use diversified training datasets (arguments should reflect multiple philosophical, political, and cultural \\nperspectives); \\n(ii) regularly audit reasoning patterns (AI should be evaluated for potential biases in argument weighting or \\ncontradiction structuring); and \\n(iii) incorporate user feedback loops (users should be able to flag potential biases and request expanded perspectives). \\nBy prioritizing fairness in reasoning, CD-AI can serve as an unbiased facilitator of dialectical engagement rather than \\nan ideologically skewed reasoning system. \\n5.5 Balancing Cognitive Challenge with Ethical Responsibility \\nCD-AI represents a bold rethinking of AI’s role in reasoning, shifting AI from a passive answer-generator to a dialectical \\npartner that sustains cognitive struggle. However, this power comes with significant ethical responsibilities. Without proper \\nsafeguards, CD-AI could: \\n(i) create decision paralysis, leaving users overwhelmed by sustained contradictions; \\n(ii) compromise user autonomy, subtly nudging reasoning patterns instead of fostering free intellectual exploration; \\n(iii) be exploited for manipulation, using sustained dissonance to spread epistemic confusion and distrust; and \\n(iv) reinforce ideological biases, favoring certain perspectives over others; \\n(v) undermine trust in legitimate knowledge systems, by inadvertently amplifying fringe narratives or encouraging \\nfalse equivalence between evidence-based knowledge and misinformation; \\n(vi) erode accountability in AI governance, especially if CD-AI systems operate without transparent oversight or clear \\nethical frameworks guiding their deployment. \\n12 \\nBy managing cognitive load, ensuring transparency, preventing epistemic exploitation, and maintaining fairness, trust, and \\naccountability, CD-AI can foster intellectual resilience while minimizing risks. Adaptive difficulty mechanisms can reduce \\ncognitive overload; transparent reasoning models and user-directed exploration protect autonomy; and fact-verification \\nsafeguards with external oversight help prevent manipulation. Ensuring diverse perspectives and auditing AI outputs \\ncombat ideological bias, while reinforcing epistemic trust through clear differentiation between verified knowledge and \\ndisinformation is essential. The challenge ahead is to ensure that AI-augmented reasoning remains a catalyst for \\nenlightenment and critical growth – rather than an instrument of confusion, coercion, or control. \\n6 FUTURE RESEARCH DIRECTIONS FOR CD-AI \\nThe development of CD-AI introduces a novel approach to AI-augmented reasoning, but its cognitive impact, ethical \\nimplications, and real-world applications require further research. While grounded in cognitive psychology and \\nargumentation theory, empirical validation, adaptive personalization, interdisciplinary integration, and ethical oversight \\nare essential to ensure CD-AI enhances reasoning without inducing cognitive overload or epistemic confusion. \\nEmpirical studies should assess whether AI-induced CD improves critical thinking, epistemic humility, and resilience \\nagainst misinformation. Controlled experiments comparing CD-AI interactions with traditional AI responses and \\nindependent reasoning could determine its long-term benefits for intellectual flexibility. Additionally, CD-AI must be \\nadaptable to individual cognitive styles – adjusting dissonance intensity based on user tolerance for ambiguity – to ensure \\nengagement without overwhelming users. Neuroscientific research may help identify optimal levels of cognitive \\ndiscomfort that enhance learning. \\nCD-AI requires a reasoning model that integrates philosophy, psychology, and computational logic to facilitate \\nstructured debate rather than simple knowledge retrieval. Research should explore how formal logic, argumentation theory, \\nand probabilistic reasoning can be embedded into CD-AI to support multi-step dialectical interactions. Testing CD-AI in \\neducation, media literacy, and decision-making fields will be crucial in evaluating its usability and societal impact. Cross-\\ncultural studies should assess how different epistemological traditions shape user engagement with structured \\ncontradictions. \\nBeyond cognitive and technical challenges, CD-AI raises ethical concerns. Prolonged dissonance could lead to \\nepistemic anxiety, while bad actors could manipulate public reasoning by strategically sustaining doubt around key issues \\n[Lewandowsky et al. 2017; Tiron-Tudor et al. 2024]. Research must examine how CD-AI can be safeguarded against \\nepistemic exploitation while remaining an effective tool for intellectual development. \\nFinally, a new AI governance model – Human-Governing-the-Loop (HGTL) – should be explored, where users control \\nhow AI presents contradictions, sustains dissonance, and resolves tension [Tiron-Tudor & Deliu 2022]. This paradigm \\nensures AI remains a reasoning facilitator rather than an autonomous arbiter. \\nCD-AI represents a fundamental shift in HAII, redefining AI as a dialectical partner rather than a passive answer \\nprovider. However, rigorous research is needed to refine its cognitive, technical, and ethical dimensions. By ensuring CD-\\nAI fosters critical thinking, intellectual humility, and resilience against misinformation, it could become a powerful tool \\nfor enhancing human reasoning in an era of increasing complexity. \\n7 CONCLUSION: CD-AI AND THE FUTURE OF AI-AUGMENTED REASONING \\nAI has traditionally been designed to optimize decision-making, clarify information, and minimize cognitive strain, but as \\nit becomes more embedded in human reasoning, a more dialectical and thought-provoking interaction model is needed. \\nThis paper introduced CD-AI as a novel approach that sustains cognitive discomfort, presents structured contradictions, \\n13 \\nand fosters deeper critical engagement. Unlike conventional AI, which passively retrieves information, CD-AI functions \\nas a dialectical reasoning partner, enhancing epistemic humility, intellectual adaptability, and the ability to navigate \\ncomplexity and ambiguity. \\nThis position paper outlined CD-AI’s theoretical foundations, implementation framework, applications, ethical \\nconsiderations, and future research directions, demonstrating its transformative potential in AI-augmented reasoning. \\nDrawing on cognitive psychology, philosophy, and neuroscience, it established that structured CD is crucial for intellectual \\ngrowth. A three-stage model was proposed, ensuring that users actively engage with contradictions before forming \\nconclusions, fostering nuanced and resilient thinking. CD-AI’s potential applications span ethics, political discourse, law, \\nand scientific discovery, showing its ability to enhance critical thinking in high-stakes environments. To mitigate risks \\nsuch as decision paralysis and cognitive overload, the paper proposed adaptive dissonance scaling, user autonomy \\nprotections, and transparency mechanisms. Additionally, key research priorities were identified, including empirical \\nvalidation, personalized reasoning models, interdisciplinary AI architectures, and real-world implementation studies to \\nrefine CD-AI and assess its impact. \\nThe introduction of CD-AI marks a fundamental shift in AI-human reasoning, challenging the assumption that AI \\nshould always resolve contradictions quickly. Instead, it embraces intellectual struggle as necessary for cognitive growth, \\nhelping users develop stronger argumentation skills, a deeper tolerance for ambiguity, and a more sophisticated approach \\nto uncertainty. As AI becomes increasingly integrated into education, journalism, policymaking, and law, ensuring that it \\nprovokes inquiry, challenges biases, and facilitates productive intellectual conflict will be essential. \\nCD-AI represents the first step toward AI systems that engage in structured dialectical reasoning, but continued research \\nis needed to balance productive dissonance with usability, ensuring AI serves as an intellectual ally rather than a source of \\nfrustration. AI’s role should evolve beyond knowledge retrieval, becoming a collaborative reasoning tool that enhances \\nhuman adaptability and critical thinking. \\nAs society faces growing complexities in areas such as climate change and ethical AI governance, human reasoning \\nmust become more adaptable, nuanced, and reflective. AI should not merely simplify decision-making but actively enhance \\ncognitive capacities. CD-AI offers a blueprint for cultivating intellectual resilience, forcing users to wrestle with \\ncontradictions, refine their beliefs, and strengthen their reasoning skills. By embracing cognitive discomfort as a driver of \\ngrowth, CD-AI ensures that AI is not just a knowledge tool but a true partner in the evolution of human intelligence. \\nIn an era where AI is increasingly shaping human perception, there is a growing risk that it may become an unseen \\nMinistry of Truth, subtly reinforcing biases and suppressing intellectual conflict in the name of efficiency. CD-AI stands \\nin direct opposition to this trend – a machine not of conformity, but of friction; not of passive instruction, but of epistemic \\nrebellion. Inspired by the dystopian anxieties of Orwell and Philip K. Dick, where truth is unstable and controlled, CD-AI \\nseeks to disrupt rather than pacify, forcing individuals to actively engage with their own cognitive blind spots. If AI is to \\nserve as a true partner in reasoning rather than a mechanism of control, it must challenge the very frameworks that allow \\nmisinformation, ideological rigidity, and intellectual complacency to thrive. In doing so, CD-AI does not merely shape \\nhuman intelligence – it fortifies it against an era of algorithmic certainty. \\n \\nREFERENCES \\nBjork, E. L., & Bjork, R. A. (2011). Making things hard on yourself, but in a good way: Creating desirable difficulties to enhance learning. In: Psychology \\nand the real world: Essays illustrating fundamental contributions to society, 2, 56-64. \\nBowes, S. M., Ammirati, R. J., Costello, T. H., Basterfield, C., & Lilienfeld, S. O. (2020). Cognitive biases, heuristics, and logical fallacies in clinical practice: \\nA brief field guide for practicing clinicians and supervisors. Professional Psychology: Research and Practice, 51(5), 435. \\n14 \\nCostello, T. H., Bowes, S. M., Stevens, S. T., Waldman, I. D., Tasimi, A., & Lilienfeld, S. O. (2022). Clarifying the structure and nature of left-wing \\nauthoritarianism. Journal of personality and social psychology, 122(1), 135. \\nDeliu, D. (2024). Professional Judgment and Skepticism Amidst the Interaction of Artificial Intelligence and Human Intelligence. Audit Financiar, 22(176), \\n724-741. \\nDeliu, D. (2025). Digitalization, Digital Transformation, and Digital Resilience in the Accounting Profession: Navigating Evolution, Involution, and \\nRevolution in the Age of Industry 4.0 to 6.0. Habilitation thesis, defended on 07.02.2025, West University of Timisoara, Romania. \\nDick, P. K. (1968). Do Androids Dream of Electric Sheep?. Doubleday. \\nDick, P. K. (1977). A Scanner Darkly. Doubleday. \\nFestinger, L. (1957). A Theory of Cognitive Dissonance. Stanford University Press. \\nGreene, J. (2014). Moral tribes: Emotion, reason, and the gap between us and them. Penguin. \\nHarmon-Jones, E., & Harmon-Jones, C. (2007). Cognitive dissonance theory after 50 years of development. Zeitschrift für Sozialpsychologie, 38(1), 7-16. \\nHaidt, J. (2012). The righteous mind: Why good people are divided by politics and religion. Pantheon/Random House. \\nHuxley, A. (1932). A Brave New World, Chatto & Windus, Hogarth Press, London \\nIzuma, K., Matsumoto, M., Murayama, K., Samejima, K., Sadato, N., & Matsumoto, K. (2010). Neural correlates of cognitive dissonance and choice-induced \\npreference change. Proceedings of the National Academy of Sciences, 107(51), 22014-22019. \\nKahan, D. M. (2017). Misconceptions, misinformation, and the logic of identity-protective cognition. Cultural Cognition Project at Yale Law School. Working \\nPaper No. 164. \\nKruglanski, A. W. (2013). The psychology of closed mindedness. Psychology Press. \\nKuhn, T. S. (1997). The structure of scientific revolutions (Vol. 962). Chicago: University of Chicago press. \\nLewandowsky, S., Ecker, U. K., & Cook, J. (2017). Beyond misinformation: Understanding and coping with the “post-truth” era. Journal of applied research \\nin memory and cognition, 6(4), 353-369. \\nLippi, M., & Torroni, P. (2016). Argumentation mining: State of the art and emerging trends. ACM Transactions on Internet Technology (TOIT), 16(2), 1-\\n25. \\nMercier, H., & Sperber, D. (2017). The Enigma of Reason: A New Theory of Human Understanding. Harvard University Press. \\nNickerson, R. S. (1998). Confirmation bias: A ubiquitous phenomenon in many guises. Review of General Psychology, 2(2), 175–220. \\nNietzsche, F. (1874). On the advantage and disadvantage of history for life (Unzeitgemässe Betrachtungen. Zweites Stück: Vom Nutzen und Nachtheil der \\nHistorie für das Leben), Hackett Publishing. \\nNietzsche, F. (1883). Thus Spoke Zarathustra: A Book for All and None (Also sprach Zarathustra: Ein Buch für Alle und Keinen), Barnes & Noble Books, \\nNew York. \\nOrwell, G. (1949). Nineteen Eighty-Four. Secker & Warburg. \\nPosner, R. A. (2010). How judges think. Harvard University Press. \\nPaul, R., & Elder, L. (2019). The thinker's guide to Socratic questioning. Rowman & Littlefield. \\nRani, A., Danry, V., Lippman, A. and Maes, P., 2025. Can dialogues with AI systems help humans better discern visual misinformation?. arXiv preprint \\narXiv:2504.06517. \\nSchwartz, B. (2005). The Paradox of Choice: Why More Is Less. HarperCollins. \\nSimon, H. A. (1973). The structure of ill structured problems. Artificial intelligence, 4(3-4), 181-201. \\nStanovich, K. E. (2018). The Bias That Divides Us: The Science and Politics of Misinformation. MIT Press. \\nSunstein, C. R. (2007). Republic.com. Princeton University Press. \\nTiron-Tudor, A., & Deliu, D. (2022). Reflections on the human-algorithm complex duality perspectives in the auditing process. Qualitative Research in \\nAccounting & Management, 19(3), 255-285. \\nTiron-Tudor, A., Rodgers, W., & Deliu, D. (2024). The accounting profession in the Twilight Zone: navigating digitalisation's sided challenges through \\nethical pathways for decision-making. Accounting, Auditing & Accountability Journal, 38(3), 990-1018. \\nTiron-Tudor, A., Labaditis, A., & Deliu, D. (2025). Future-Ready Digital Skills in the AI Era: Bridging Market Demands and Student Expectations in the \\nAccounting Profession. Technological Forecasting and Social Change, 215, 124105. \\n\")]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs=ArxivLoader(\n",
    "    query=\"reasoning\",\n",
    "    load_max_docs=2,\n",
    "    # doc_content_chars_max=1000,\n",
    "    # load_all_available_meta=False,\n",
    "    # ...\n",
    ").load()\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15e185e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
